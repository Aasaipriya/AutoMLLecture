
\input{../latex_main/main.tex}


\newcommand{\lz}{\vspace{0.5cm}}
\newcommand{\thetab}{\bm{\weights}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\Xmat}{\mathbf{X}}
\newcommand{\ydat}{\mathbf{y}}
\newcommand{\id}{\boldsymbol{I}}
\newcommand{\Amat}{\mathbf{A}}
\newcommand{\Xspace}{\mathcal{X}}                                           
\newcommand{\Yspace}{\mathcal{Y}}
\newcommand{\ls}{\ell}
\newcommand{\natnum}{\mathbb{N}}
\newcommand{\intnum}{\mathbb{Z}}

\usepackage{fontawesome}
\usepackage{dirtytalk}
\usepackage{csquotes}


\title[AutoML: GPs]{AutoML: Gaussian Processes} % week title
\subtitle{Covariance Functions for GPs} % video title
\author[Marius Lindauer]{\underline{Bernd Bischl} \and Frank Hutter \and Lars Kotthoff\newline \and Marius Lindauer \and Joaquin Vanschoren}
\institute{}
\date{}



% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}
	
	\maketitle
	

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c,allowframebreaks]{Covariance function of a GP}

The marginalization property of the Gaussian process implies that for any set of input values, the corresponding vector of function values is Gaussian:

  $$
    \bm{f} = \left[f\left(\xI{1}\right),\dots, f\left(\xI{n}\right)\right] \sim \mathcal{N}\left(\bm{m}, \bm{K}\right).
  $$ 


\begin{itemize}

  \item The covariance matrix $\bm{K}$ is constructed according to the chosen inputs $\left\{\xI{1},\dots,\xI{n}\right\}$.
  \item Each entry $\bm{K}_{ij}$ is computed by $k\left(\xI{i}, \xI{j}\right)$.
  \item Technically, to be a valid covariance matrix, $\bm{K}$ needs to be positive semi-definite for \textbf{every} choice of inputs $\left\{\xI{1},\dots,\xI{n}\right\}$.
  \item A function $k(\cdot,\cdot)$ that satisfies this condition is called \textbf{positive definite}.
  
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak

\begin{itemize}
\item Recall that the purpose of the covariance function is to control to which degree the following condition is fulfilled:
\end{itemize}
\lz
\begin{displayquote}
If $\xI{i}$ and $\xI{j}$ are close in the $\Xspace$-space, their function values $f(\xI{i})$ and $f(\xI{j})$ should be close in $\Yspace$-space.
\end{displayquote}

\lz
\lz

\begin{itemize}
\item[\faLightbulbO] Closeness of $\xI{i}$ and $\xI{j}$ in the input space $\Xspace$ is measured by $\bm{d} =\xI{i}-\xI{j}$. 

\lz

\item[\faLightbulbO] $\bm{K}_{ij}$ is the covariance of $f(\xI{i})$ and $f(\xI{j})$, and \textbf{stationary} covariance functions are those in which the following holds:$$k(\xI{i}, \xI{j}) = k(\bm{d})$$ 
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c,allowframebreaks]{Covariance function of a GP: Example}


\begin{itemize}
\item Let $f(\x)$ be a GP with $k(\x, \x^\prime) = \exp(-\frac{1}{2}\|\bm{d}\|^2)$ where $\bm{d} = \x - \x^\prime$.

\vspace{.3cm}

\item Consider two points $\xI{1} = 3$ and $\xI{2} = 2.5$. To investigate how correlated their function values are, compute their correlation!
\end{itemize}

\vspace{.3cm}

\begin{figure}
\includegraphics[width=0.3\textwidth]{figure_man/covariance2point/example_covariance_1.png} ~
\includegraphics[width=0.3\textwidth]{figure_man/covariance2point/example_function_1-1.png}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak


\begin{itemize}
  \item Assume that we observe a value of $\yI{1} = - 0.8$. Under the said assumption for the Gaussian process, the value of $\yI{2}$ should be close to $\yI{1}$.
\end{itemize}
\vspace{1cm}

\begin{figure}
\includegraphics[width=0.3\textwidth]{figure_man/covariance2point/example_covariance_1.png} ~
\includegraphics[width=0.3\textwidth]{figure_man/covariance2point/example_function_1-2.png}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak

\begin{itemize}
\item Now, let us take a new point $\xI{3}$ which is not too close to $\xI{1}$.
  
\vspace{.3cm}
\item Their function values should not be so correlated. That is, $\yI{1}$ and $\yI{3}$ are probably far away from each other.
\end{itemize}

\vspace{.3cm}

\begin{figure}
\includegraphics[width=0.3\textwidth]{figure_man/covariance2point/example_covariance_2.png} ~      
\includegraphics[width=0.3\textwidth]{figure_man/covariance2point/example_function_2-1.png}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]{Sampling from a GP: Covariance Function}

Let us draw $10$ functions from a Gaussian process prior with the squared exponential covariance function but with two different values of $\ls$, also called the characteristic length-scale.

$$k(\x, \x^\prime) = \exp\left(-\frac{1}{2\ls^2}\|\x - \x^\prime\|^2\right)$$

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure_man/gp-sqexp-l-1.pdf}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[c]{Covariance Functions}

Three types of properties are commonly used in covariance functions:

\lz

\begin{itemize}
\item $k$ is \textbf{stationary} if its returning values depend on $\bm{d} =\x -\x^\prime$ and is denoted by $k(\bm{d})$.

\item $k$ is \textbf{isotropic} if its returning values depend on $r = \|\x - \x^\prime\|$and is denoted by $k(r)$.

\item $k$ is a \textbf{dot product} if its returning values depend on $\x^T \x^\prime$.
\end{itemize}

\lz
\lz

\begin{itemize}
\item[\faLightbulbO] Isotropy implies stationarity.
\item[\faLightbulbO] Isotropic functions are rotationally invariant.
\item[\faLightbulbO] Stationary functions are translationally invariant:
\vspace{-.3cm}

$$k(\x,\x + \bm{d}) = k(\bm{0},\bm{d})=k(\bm{d})$$
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c,allowframebreaks]{Commonly Used Covariance Functions}


\begin{table}[]
\centering
\begin{tabular}{|c|c|}
\hline
Name & $k(\x, \x^\prime)$\\
\hline
constant & $\variance_0$ \\ [1em]
linear & $\variance_0 + \x^T\x^\prime$ \\ [1em]
polynomial & $(\variance_0 + \x^T\x^\prime)^p$ \\ [1em]
squared exponential & $\exp(- \frac{\|\x - \x^\prime\|^2}{2\ls^2})$ \\ [1em]
Matérn & \begin{footnotesize} $\frac{1}{2^\nu \Gamma(\nu)}\biggl(\frac{\sqrt{2 \nu}}{\ls}\|\x - \x^\prime\|\biggr)^{\nu} K_\nu\biggl(\frac{\sqrt{2 \nu}}{\ls}\|\x - \x^\prime\|\biggr)$\end{footnotesize}  \\ [1em]
exponential & $\exp\left(- \frac{\|\x - \x^\prime\|}{\ls}\right)$ \\ [1em]
\hline
\end{tabular}
\end{table}
\begin{footnotesize}
\centering
$K_\nu(\cdot)$ is the modified Bessel function of the second kind.
\end{footnotesize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak

\begin{itemize}
\item[\faLightbulbO] Some random functions drawn from Gaussian processes with a Squared Exponential Kernel (left), Polynomial Kernel (middle), and a Matérn Kernel (right, $\ls = 1$). 
\item[\faLightbulbO] The choice of the hyperparameter determines the ``wiggliness'' of the function.
\end{itemize}

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure_man/covariance.png}
\end{figure}




\end{frame}
%-----------------------------------------------------------------------

\end{document}
