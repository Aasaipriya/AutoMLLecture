
\input{../latex_main/main.tex}

\newcommand{\lz}{\vspace{0.5cm}}
\newcommand{\thetab}{\bm{\weights}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\Xmat}{\mathbf{X}}
\newcommand{\ydat}{\mathbf{y}}
\newcommand{\id}{\boldsymbol{I}}

\usepackage{pgffor}

\title[AutoML: GPs]{AutoML: Gaussian Processes} % week title
\subtitle{The Bayesian Linear Model} % video title
\author[Marius Lindauer]{\underline{Bernd Bischl} \and Frank Hutter \and Lars Kotthoff\newline \and Marius Lindauer \and Joaquin Vanschoren}
\institute{}
\date{}



% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}
	
	\maketitle
	

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, allowframebreaks]{Review: The Bayesian Linear Model}

Let $\datasettrain = \left\{(\xI{1},\yI{1}), ..., (\xI{n},\yI{n})\right\}$ be a training set of i.i.d. observations from some unknown distribution.
\begin{figure}
  \includegraphics[width=0.4\textwidth]{figure_man/bayes-lm/example.pdf}
\end{figure}

Let $\textbf{y} = (\yI{1}, ..., \yI{n})^\top$ and $\Xmat \in \realnum^{n \times p}$ be the design matrix where the i-th row contains vector $\xI{i}$. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak

The linear regression model is defined as


$$
\yI{i} = f(\xI{i}) + \epsilon^{(i)} = \bm{\weights}^\top \xI{i} + \epsilon^{(i)} \text{, for all } i \in \{1,\dots,n\}.
$$

\lz 

\lz 

The observed values $\yI{i}$ differ from the function values $f(\xI{i})$ by some additive noise, which is assumed to be i.i.d. Gaussian 
$$\epsilon^{(i)} \sim \normaldist (0, \variance).$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak
Let us assume we have \textbf{prior beliefs} about the parameter $\thetab$ that are represented in a prior distribution $\thetab \sim \normaldist (\zero, \tau^2 \id_p).$

\lz 

Whenever data points are observed, we update the parameters' prior distribution according to Bayes' rule 

$$
\underbrace{p(\thetab \mid \Xmat, \ydat)}_{\text{posterior}} = \frac{\overbrace{p(\ydat \mid \Xmat, \thetab)}^{\text{likelihood}}\overbrace{q(\thetab)}^{\text{prior}}}{\underbrace{p(\ydat\mid\Xmat)}_{\text{marginal}}}.
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak

The posterior distribution of the parameter $\thetab$ is again normal distributed (the Gaussian family is self-conjugate): 

$$
\thetab \mid \Xmat, \ydat \sim \normaldist(\sigma^{-2}\bm{A}^{-1}\Xmat^\top\ydat, \bm{A}^{-1})\text{, where }\bm{A}:= \sigma^{-2}\Xmat^\top\Xmat + \frac{1}{\tau^2} \id_p.
$$


\lz 

\begin{footnotesize}
\textbf{Note:} If the posterior distributions $p(\thetab\mid\Xmat, \ydat)$ are in the same probability distribution family as the prior $q(\thetab)$, the prior and posterior are then called \textbf{conjugate distributions}, and the prior is  called a \textbf{conjugate prior} for the likelihood function $p(\ydat\mid\Xmat, \thetab)$. 
\end{footnotesize}

\lz

\begin{footnotesize}
\textbf{Note:} The Gaussian family is \textbf{self-conjugate} with respect to a Gaussian likelihood function: choosing a Gaussian prior for a Gaussian likelihood ensures that the posterior is also Gaussian.
\end{footnotesize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak

%\begin{figure}
%  \includegraphics[width=0.5\textwidth]{figure_man/bayes-lm/prior-1.pdf}~\includegraphics[width=0.5\textwidth]{figure_man/bayes-lm/prior-2.pdf}
%\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak


%\foreach \x in{5, 10, 20} {
%\begin{figure}
%  \includegraphics[width=0.5\textwidth]{figure_man/bayes-lm/posterior-\x-1.pdf}~  \includegraphics[width=0.5\textwidth]{figure_man/bayes-lm/posterior-\x-2.pdf}
%\end{figure}
%}
%\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak


\begin{footnotesize}
\textbf{Theorem:}\\
\begin{itemize}
  \item For a Gaussian prior on $\thetab \sim \normaldist(\zero, \tau^2 \id_p)$, and
  \item a Gaussian likelihood $y \mid \Xmat, \thetab \sim \normaldist(\Xmat^\top \thetab, \sigma^2 \id_n)$, 
\end{itemize}
the resulting posterior is Gaussian: $\normaldist(\sigma^{-2}\bm{A}^{-1}\Xmat^\top\ydat, \bm{A}^{-1})$, with $\bm{A}:= \sigma^{-2}\Xmat^\top\Xmat + \frac{1}{\tau^2} \id_p$.

\vspace{+.4cm}
\textbf{Proof:}\\
%\vspace{-.4cm}

Plugging in Bayes' rule and multiplying out yields


\begin{eqnarray*}
p(\thetab \mid \Xmat, \ydat) &\propto& p(\ydat \mid \Xmat, \thetab) q(\thetab) \propto \exp\biggl[-\frac{1}{2\sigma^2}(\ydat - \Xmat\thetab)^\top(\ydat - \Xmat\thetab)-\frac{1}{2\tau^2}\thetab^\top\thetab\biggr] \\
&=& \exp\biggl[-\frac{1}{2}\biggl(\underbrace{\sigma^{-2}\ydat^\top\ydat}_{\text{doesn't depend on } \thetab} - 2 \sigma^{-2} \ydat^\top \Xmat \thetab + \sigma^{-2}\thetab^\top \Xmat^\top \Xmat \thetab  + \tau^{-2} \thetab^\top\thetab \biggr)\biggr] %\\
%&\propto& \exp\biggl[-\frac{1}{2}\biggl(\sigma^{-2}\thetab^\top \Xmat^\top \Xmat \thetab  + \tau^{-2} \thetab^\top\thetab  - 2 \sigma^{-2} \ydat^\top \Xmat \thetab \biggr)\biggr] \\
%&=& \exp\biggl[-\frac{1}{2}\thetab^\top\underbrace{\biggl(\sigma^{-2} \Xmat^\top \Xmat + \tau^{-2} \id_p \biggr)}_{:= \Amat} \thetab + \textcolor{red}{\sigma^{-2} \ydat^\top \Xmat \thetab}\biggr]
\end{eqnarray*}


This expression resembles a normal density - except for the term in red!

\end{footnotesize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak











\end{frame}
%-----------------------------------------------------------------------

\end{document}
