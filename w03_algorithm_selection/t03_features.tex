
\input{../latex_main/main.tex}


\title[AutoML: Overview]{AutoML: Algorithm Selection} % week title
\subtitle{Features} % video title
\author[Marius Lindauer]{Bernd Bischl \and Frank Hutter \and \underline{Lars Kotthoff}\newline \and Marius Lindauer \and Joaquin Vanschoren}
\institute{}
\date{}


% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}
	
\maketitle

\begin{frame}{Algorithm Selection}
\begin{center}
\resizebox{.7\textwidth}{!}{%
\tikzset{>=latex}
\begin{tikzpicture}[node distance=1em,inner sep=.5em,n/.style={drop
shadow,fill=white,rounded corners},scale=.5]
\node (p) [align=left] {Portfolio};
\node (a2) [rectangle,align=center,below=of p,draw] {Algorithm 2};
\node (a1) [rectangle,align=center,left=of a2,draw] {Algorithm 1};
\node (a3) [rectangle,align=center,right=of a2,draw] {Algorithm 3};
\begin{pgfonlayer}{background}
\node (pc) [n,fit={(p) (a1) (a2) (a3)},draw] {};
\end{pgfonlayer}

\node (i) [align=left,right=15em of p.east] {Training Instances};
\node (i2) [rectangle,align=center,below=of i,draw] {Instance 2};
\node (i1) [rectangle,align=center,left=of i2,draw] {Instance 1};
\node (i3) [rectangle,align=center,right=of i2,draw] {Instance 3};
\begin{pgfonlayer}{background}
\node (ic) [n,fit={(i) (i1) (i2) (i3)},draw] {};
\end{pgfonlayer}

\node (as) [n,draw,align=left,below=10em of $(p)!0.5!(i)$] {Algorithm Selection};
\node (m) [n,draw,align=left,below=3em of as] {Performance Model};

\node (it) [align=center,right=10em of m] {Instance 4\\Instance 5\\Instance 6\\$\vdots$};

\node (s) [align=center,below=3em of m] {Instance 4: Algorithm 2\\Instance 5: Algorithm 3\\Instance 6: Algorithm 3\\$\vdots$};

\path [->] (pc.south) edge (as);
\path [->] (ic.south) edge node [below right, rectangle, draw = blue, line width = 3pt] {Feature Extraction} (as);
\path [->] ([xshift=-.5em]it.west) edge node [below,align=center, rectangle, draw = blue, line width = 3pt] {Feature\\Extraction} (m.east);
\path [->] (as.south) edge (m.north);
\path [->] (m.south) edge (s.north);
\end{tikzpicture}}
\end{center}
\end{frame}

\begin{frame}{Features}
\begin{itemize}
\item relate properties of datasets or problem instances to algorithm performance
\item relatively cheap to compute -- must be cheaper than running the algorithm
    to see what its performance is
\item often specified by domain expert
\item syntactic -- analyze dataset or instance description
\item probing -- run algorithm for short time
\item dynamic -- monitor changes while algorithm is running
\end{itemize}
\end{frame}

\begin{frame}{Syntactic Features}
\begin{itemize}
    \item for machine learning datasets:
        \begin{itemize}
            \item number of binary/numeric/categorical features
            \item skewness of classes
            \item fraction of missing values
            \item \ldots
        \end{itemize}
    \item for combinatorial problems:
        \begin{itemize}
            \item number of variables, number of clauses/constraints/\ldots
            \item ratios
            \item order of variables/values
            \item connectivity clause/constraints--variable graph or variable graph
            \item \ldots
        \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Probing Features}
\begin{itemize}
    \item for machine learning datasets (also called landmarkers):
        \begin{itemize}
            \item performance of majority class/mean value predictor
            \item decision stump performance
            \item simple rule model performance
            \item \ldots
        \end{itemize}
    \item for combinatorial problems:
        \begin{itemize}
            \item number of nodes/propagations within time limit
            \item estimate of search space size
            \item tightness of problem/constraints
            \item \ldots
        \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Dynamic Features}
\begin{itemize}
    \item for machine learning datasets:
        \begin{itemize}
            \item change in information gain between levels of a tree
            \item convergence speed
            \item performance gain through additional ensemble model
            \item \ldots
        \end{itemize}
    \item for combinatorial problems:
        \begin{itemize}
            \item change of variable domains
            \item number of constraint propagations
            \item number of failures a clause participated in
            \item \ldots
        \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{No Features}
    \begin{itemize}
        \item use deep learning to process dataset or problem instance as-is
        \item no need for expert-designed features
        \item performance not good, no widespread adoption yet
    \end{itemize}
    \lit{Loreggia, Andrea, Yuri Malitsky, Horst Samulowitz, and Vijay Saraswat. ``Deep Learning for Algorithm Portfolios.'' In Thirtieth AAAI Conference on Artificial Intelligence, 1280â€“86. 2016.}
\end{frame}

\begin{frame}{Aside: Algorithm Features}
    \begin{itemize}
        \item can characterize algorithm in addition to datasets and problem
            instances
        \item allows to relate performance to specific aspects of an algorithm
            rather than black boxes
        \item for example size of code base, properties of abstract syntax
            tree\ldots
        \item ongoing work
    \end{itemize}
\end{frame}

\begin{frame}{What Features Do We Need in Practice?}
    \begin{itemize}
        \item trade-off between complex features and complex models
        \item in practice, very simple features (e.g.\ problem size) can perform well
        \item often only few features of a set are needed (e.g.\ 5 out of $>$100)
        \item in the end, whatever works best
    \end{itemize}
\end{frame}

\end{document}
