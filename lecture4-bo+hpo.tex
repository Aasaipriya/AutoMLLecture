\begin{frame}[c]{}

\centering
\huge
Lecture 4:\\
Hyperparameter optimization\\
Bayesian Optimization
\end{frame}
%----------------------------------------------------------------------
\begin{frame}[c]{Where are we? The big picture}

\begin{itemize}
	\item Introduction
	\item[$\to$] Background
	\begin{itemize}
		\item Design spaces in ML
		\item[$\to$] Evaluation and visualization
	\end{itemize}
	\item Hyperparameter optimization (HPO)
	\begin{itemize}
		\item Bayesian optimization
		\item Other black-box techniques
		\item Speeding up HPO with multi-fidelity optimization
	\end{itemize}
	\item Pentecost (Holiday) -- no lecture
	\item Architecture search I + II
	\item Meta-Learning
	\item Learning to learn $\&$ optimize
	\item Beyond AutoML: algorithm configuration and control
	\item Project announcement and closing
\end{itemize}

\end{frame}
%----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{frame}[c]{Learning Goals}

After this lecture, you will be able to \ldots

\begin{itemize}
  \item explain the \alert{challenges in hyperparameter optimization}
  \item efficiently optimize black box functions via \alert{Bayesian Optimization}
  \begin{itemize}
    \item discuss the advantages of different \alert{surrogate models}
    \item explain the idea of \alert{acquisition functions} to trade off exploration and exploitation
  \end{itemize}
\end{itemize}


\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{How to Optimize Black Box Functions?}

\centering
\includegraphics[width=0.7\textwidth]{images/black_box_manual_opt.png}

Only interaction: Query of function at $x$ to obtain $f(x)$

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{How to Optimize Black Box Functions?}

\centering
\includegraphics[width=0.9\textwidth]{images/black_box_aut_opt.png}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Why Black Box Functions for AutoML?}

\begin{itemize}
  \item Internals of algorithms are often not known\\
  (or well understood)
  \item Extreme: Only way of interaction is running the algorithms
\end{itemize}

\pause
\begin{block}{Example: Hyperparameter Optimization (HPO)}
	Let 
	\begin{itemize}
		\item $\lambda$ be the hyperparameters of an ML algorithm $A$ with domain $\Lambda$,
		\item $D_{opt}$ be a training set which is split into $D_{train}$ and $D_{valid}$ 
		\item $\mathcal{L}(A_\conf, \dataset_{train}, \dataset_{valid})$ denote the loss of $A_\lambda$ trained on $D_{train}$ and evaluated on $D_{valid}$.
	\end{itemize}
	The \emph{hyper-parameter optimization (HPO)} problem is to find a hyper-parameter configuration that minimizes this loss:
	\begin{equation}
	\lambda^* \in \argmin_{\lambda \in \Lambda} \mathcal{L}(A_\conf, \dataset_{train}, \dataset_{valid}) \nonumber  
	\end{equation}
\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Challenges in HPO}
\only<1>
{%
	\centering
	
	\vspace*{2.25cm}
	
	What could be challenges in hyperparameter optimization?
	
	\bigskip
	
	\includegraphics[width=0.2\textwidth]{images/hands.png}
}%
\pause
\begin{itemize}
  \item function evaluations are very expensive
  \begin{itemize}
    \item training a single ML-pipeline can require minutes (or even hours) 
    \item[$\leadsto$] exhaustive search is not feasible
  \end{itemize}
  \pause
  \medskip
  \item complex, structured search space
  \begin{itemize}
    \item small continuous parameter spaces already challenging to optimize
    \item typically, we talk about large configuration spaces\newline ($\gg 10$ hyper-parameters)
    \begin{itemize}
      \item many HPO benchmarks only consider a few continuous parameters
    \end{itemize}
    \item mixed parameter types
    \item conditional structures
  \end{itemize}
  \pause
  \medskip
  \item resembles a black-box optimization problem
  \begin{itemize}
    \item Input: hyperparameter configuration
    \item Black box: ML pipeline 
    \item Output: loss
    \item Note: no gradient information available
  \end{itemize}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
\begin{frame}[c]{How to Optimize Black Box Functions?}

\centering
\includegraphics[width=0.9\textwidth]{images/black_box_aut_opt.png}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Grid Search \litw{Bergstra et al. '12}}

\begin{center}
\includegraphics[width=0.28\textwidth]{images/grid_search}%
\includegraphics[width=0.28\textwidth]{images/gs}
\end{center}

\begin{block}{Pros and Cons}

\only<1-2>{\hands What are potential pros and cons of grid search?}
\pause

\begin{columns}
\column{0.4\textwidth}
Pros:
\begin{itemize}
  \item easy to implement
  \item easy to parallelize 
\end{itemize}

\column{0.6\textwidth}
Cons:
\begin{itemize}
  \item discretization of $\pcs$?
  \item does not scale with $\#$hyperparameters
  \item inefficient if not all hyperparameters are important
\end{itemize}


\end{columns}

\end{block}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Random Search \litw{Bergstra et al. '12}}

\begin{center}
\includegraphics[width=0.28\textwidth]{images/random_search}%
\includegraphics[width=0.28\textwidth]{images/rs}
\end{center}

\begin{block}{Pros and Cons}

\only<1-2>{\hands What are potential pros and cons of random search?}
\pause

\begin{columns}
\column{0.4\textwidth}
Pros:
\begin{itemize}
  \item even easier to implement
  \item easy to parallelize 
  \item more evaluations along each parameter
\end{itemize}

\column{0.6\textwidth}
Cons:
\begin{itemize}
  \item does not scale with $\#$hyper-parameters
  \item purely explorative
\end{itemize}


\end{columns}

\end{block}

\end{frame}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Bayesian Optimization \litw{J. Mockus 1977}}

\begin{columns}

\column{0.4\textwidth}

\includegraphics[width=\textwidth]{images/bo_pic1.png}\\
\pause
\includegraphics[width=\textwidth]{images/bo_pic2.png}\\
\pause
\includegraphics[width=\textwidth]{images/bo_pic3.png}
\pause

\column{0.55\textwidth}
General Approach:
\begin{enumerate}
  \item Fit model on collected observations $\langle{}\conf, f(\conf)\rangle{}$
  \pause
  \item use acquisition function $a$ to trade off exploration and exploitation
  \pause
  \item maximize acquisition function: $x^* \in \argmin a(x)$
  \pause
  \item obtain new observation at $x^*$
\end{enumerate}

\pause
Moving pieces:
\begin{itemize}
  	\item Which \alert{model family} to use 
	\item How to use the model to guide optimization
	\begin{itemize}
		\item Determined by $a(x)$\\
		(Which data point should I \emph{acquire} next?) 
	\end{itemize}
\end{itemize}

\end{columns}

\end{frame}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Bayesian Optimization}

\begin{block}{Pros and Cons}

Pros:
\begin{itemize}
  \item sample efficient
  \item can be applied to many black-box functions with expensive function evaluations (not only HPO)
\end{itemize}

Cons:
\begin{itemize}
  \item overhead because of model training in each iteration
  \item hard to efficiently parallelize
  \item (requires good surrogate model)
\end{itemize}

\end{block}

\end{frame}
%-----------------------------------------------------------------------


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c,fragile]{The Role of the Acquisition Function}
\begin{itemize}
  \item Given: a model $\hat{f}:\confs \rightarrow \mathds{R}$ that predicts the quality $\mu(\conf)$ for each configuration $\conf$ and its standard deviation $\sigma(\conf)$ ($\leadsto$ uncertainty)
  \begin{itemize}
  	\item Assume w.l.o.g. that we want to \emph{maximize} $f$
  \end{itemize}
  \medskip
  \pause
  \item Which configuration should we select next? Need to trade off: 
  \begin{itemize}
    \item \alert{Exploitation}\\(sampling where the predicted mean $\mu(\conf)$ is high)
    \item \alert{Exploration}\\(sampling where we're uncertain about f; i.e., $\sigma(\conf)$ is high)
  \end{itemize}
  \medskip
  \pause
  \item Various acquisition functions achieve this trade-off
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c,fragile]{Probability of Improvement}
\begin{itemize}
\vspace*{-0.2cm}
  \item Let $f(\theta^+)$ denote the best (here: max) function value known so far.
\vspace*{-0.2cm}  
  \begin{eqnarray}
\nonumber{}  PI(\conf) & = & P(f(\conf) \ge f(\theta^+))) = \Phi \left( \frac{\mu(\theta) - f(\theta^+)}{\sigma(\theta)} \right)
  \end{eqnarray}
  \item Here, $\Phi$ is the cumulative distribution function of the standard normal distribution. (There are $\mathcal{O}(1)$ lookup tables for this.)
\end{itemize}
\centering
\includegraphics[width=0.55\textwidth]{images/Acquisition-PI.png} 

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c,fragile]{Expected Improvement}

\begin{itemize}
\vspace*{-0.2cm}
  \item Like probability of improvement, but also takes into account the \alert{magnitude} of the improvement.
  \item Define the improvement at a point $\conf$ as:\\
\vspace*{-0.2cm}
  \begin{equation}
  \nonumber{}I(\conf) = \max (f(\conf) - f(\conf^+), 0)
  \end{equation}
  \pause
\vspace*{-0.4cm}  
  \item Then, we can compute the expectation of this improvement across the predictive distribution
  \begin{eqnarray}
\nonumber{} \mathds{E}[I(x)] = \int_{-\infty}^{\infty} \max (f(\conf) - f(\conf^+), 0) \cdot \norm( f(\conf) ; \mu(\conf), \sigma^2(\conf) )  df(\conf) 
  \end{eqnarray}
  \pause
\vspace*{-0.2cm}
  \item This turns out to have a closed form solution:
  \small
  \begin{eqnarray}
\nonumber{} \mathds{E}[I(x)] = (\mu(\conf) - f^+) \Phi\left( \frac{\mu(\conf) - f(\conf^+)}{\sigma(\conf)} \right)  + \sigma(\conf) \phi \left( \frac{\mu(\conf) - f(\conf^+)}{\sigma(\conf)} \right)
  \end{eqnarray}
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c,fragile]{Upper Confidence Bound}
\begin{itemize}
\vspace*{-0.2cm}
  \item UCB$(\conf) = \mu(\conf) + \kappa\sigma(\conf)$, with exploration parameter $\kappa$
\end{itemize}
\vspace*{-0.2cm}  
\centering
\includegraphics[width=0.55\textwidth]{images/Acquisition-UCB.png} 
\vspace*{0.2cm}  
\begin{itemize}
\item Which point would we pick next with UCB and $\kappa = 1$? \hands
\pause
 \item GP-UCB$(\conf) = \mu(\conf) + \sqrt{\beta_t} \sigma(\conf)$, with $\beta_t$ \alert{increasing} over time\\
 \lit{Srinivas et al. 2009}
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c,fragile]{Entropy Search}

TODO

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c,fragile]{Surrograte model Model}

Required features

\begin{itemize}
  \item Mandatory:
  \begin{itemize}
    \item Regression model
	\item Uncertainty estimates
  \end{itemize}
  \pause
  \item Preferable:
  \begin{itemize}
    \item accurate predictions
    \item cheap-to-train
    \item scales with the complexity of the data\\ (number of features and observations)
    \item can handle different types of features\\ (categorical and continuous)
  \end{itemize}
\end{itemize}

\pause
Common choice: Gaussian Processes\\
Our choice: Random Forests

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c,fragile]{Recap Random Forests}


\centering
\includegraphics[width=0.55\textwidth]{images/random_forest_pic}

\begin{itemize}
  \item Train:
  \begin{itemize}
    \item $n$ decision (or regression) trees (potentially with pruning)
    \item subsampled training data for each tree (with bootstrapping)
    \item subsampled feature set for each split
  \end{itemize}
  \item Predict
  \begin{itemize}
    \item Obtain prediction of each tree
    \item Aggregate predictions (voting, average, \ldots)
    \pause
    \item Uncertainty predictions: average across tree predictions 
  \end{itemize}
\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c,fragile]{Advantages and Disadvantages of Random Forests}


\begin{columns}
\column{0.5\textwidth}
Pros:
\begin{itemize}
  \item Cheap to train
  \item Scales well with \#observations
  \begin{itemize}
    \item Worst-case complexity for $T$ tress with $n$ data points of dimensionality $p$: $\mathcal O(T\cdot p \cdot n^2 \log{n})$
  \end{itemize}
  \item training can be parallelized
  \item Can handle continuous and categorical features
  \begin{itemize}
    \item most RF implementations can handle only continuous features
  \end{itemize}
\end{itemize}

\column{0.5\textwidth}
Cons:
\begin{itemize}
  \item Poor uncertainty estimates
  \item No extrapolation
  \begin{itemize}
    \item last seen value for extrapolation (constant)
  \end{itemize}
\end{itemize}

\end{columns}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------
\begin{frame}[c]{Preprocessing Configuration}

\begin{block}{Convert $\conf$ for ML Model}
  \begin{itemize}
    \item continuous and integer parameter can be directly passed
    \begin{itemize}
      \item scaled to $[0,1]$
    \end{itemize}
    \pause
    \item categorical parameters should be encoded if necessary
    \begin{itemize}
      \item e.$\,$g., random forest can handle categorical parameter natively\\
      (Not all implementations are able to do it!) 
      \item use one-hot encoding if necessary:
      \begin{itemize}
        \item add new variable for each possible value $v_i$
        \item set one of these variable to one (depending on the configuration)
      \end{itemize}
    \end{itemize}
    \pause
    \item ordinal parameters can be converted to integers or also one-hot encoded
  \end{itemize}
\end{block}


\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Preprocessing Configuration (cont'd)}

\begin{block}{Conditional Hyperparameters}
  \begin{itemize}
    \item Configuration spaces often have a hierarchical structure
    \item e.$\,$g., hyperparameter A is only active if heuristic H is used
    \pause
    \item[$\leadsto$] List of hyperparameter values has missing entries because of inactive hyperparameters
    \medskip
    \pause
    \item Fixes:
    \begin{itemize}
    	\item Impute missing data, e.$\,$g., by default setting
    	\item Mark these inactive hyperparameters and let the model deal with it\\
    		  e.$\,$g., a random forest could only split on active parameters
    \end{itemize}
  \end{itemize}
\end{block}

\end{frame}
%-----------------------------------------------------------------------
{\setbeamertemplate{logo}{}
\begin{frame}[c,fragile]{Important Design Dimensions in BO}

\only<1>{
\begin{block}{Transformation of y-values}
	\begin{minipage}{0.6\textwidth}
		\begin{figure}[H]
			\centering
			\includegraphics[height=.6\textheight]{images/borf_boxplot_y}
		\end{figure}
	\end{minipage} \hfill
	\begin{minipage}{0.39\textwidth}
		\begin{itemize}
			\item log-transformed values to fit the model and the acquisition function improves performance
			\item less emphasize on large outlier values
			\item focus more on small improvements and less on exploration in unexplored spaces
		\end{itemize}
	\end{minipage}
\end{block}
}

\only<2>{
	\begin{block}{Interleaving Random Points}
		\begin{minipage}{0.6\textwidth}
			\begin{figure}[H]
				\centering
				\includegraphics[height=.6\textheight]{images/borf_boxplot_r}
			\end{figure}
		\end{minipage} \hfill
		\begin{minipage}{0.39\textwidth}
			\begin{itemize}
				\item RFs don't extrapolate well
				\item Interpolation between two observations with similar function values leads to
				constant uncertainty estimates
				\item BO with RFs can easily get stuck in local optima
				\item interleave randomly sampled points to escape local optima
			\end{itemize}
		\end{minipage}
	\end{block}
}

\only<3>{
	\begin{block}{Initial Design}
		\begin{minipage}{0.6\textwidth}
			\begin{figure}[H]
				\centering
				\includegraphics[height=.6\textheight]{images/borf_boxplot_init}
			\end{figure}
		\end{minipage} \hfill
		\begin{minipage}{0.39\textwidth}
			\begin{itemize}
				\item Alternative to exploration via randomly sampled points
				\item explore the space before the actual BO takes place
				\item Pro: will improve the EPM in early iterations
				\item Con: will invest a considerable number of function evaluations without taking the already gathered knowledge into account
			\end{itemize}
		\end{minipage}
	\end{block}
}
\end{frame}
}
%----------------------------------------------------------------------
\begin{frame}[c]{Learning Goals}

After this lecture, you are able to \ldots

\begin{itemize}
	\item explain the \alert{challenges in hyperparameter optimization}
	\item efficiently optimize black box functions via \alert{Bayesian Optimization}
	\begin{itemize}
		\item discuss the advantages of different \alert{surrogate models}
		\item explain the idea of \alert{acquisition functions} to trade off exploration and exploitation
	\end{itemize}
	\item define \alert{configuration spaces}
	\item understand \alert{grey-boxes} for hyperparameter optimization (HPO)
\end{itemize}
\end{frame}
%-----------------------------------------------------------------------