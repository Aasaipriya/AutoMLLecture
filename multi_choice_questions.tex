\documentclass{exam}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\usepackage{array,multirow,graphicx}

%-------------------------------------------------------------
\title{Collection of AutoML multiple Choice Questions}

\begin{document}
\maketitle
%-------------------------------------------------------------------
\section{Big Picture}
%Here, the questions begin
All questions require a single choice answer.
\begin{questions}

\question AutoML can help you to ...
\begin{choices}
 \choice replace the job of someone by a machine.
 \choice enables efficient interdisciplinary projects.
 \choice increase the time for finishing ML projects.
 \choice apply ML without any knowledge about the risks of ML.
\end{choices}

\question AutoML is a hard problem because ... (among other reasons).
\begin{choices}
 \choice every function evaluation is super cheap
 \choice we only have to tune a single hyperparameter
 \choice we need the same ML pipeline for each dataset
 \choice the search space can quite complex and large
\end{choices}

\question The CASH problem stands for ...
\begin{choices}
 \choice Combined Algorithm Solving and Hashing.
 \choice Combined Algorithm Selection and Hyperparameter Optimization.
 \choice Combined Automated Selection of Heuristics.
 \choice Combined Automated Selection of Hyperparameters.
\end{choices}

\question  To apply AutoML to deep learning, we ...
\begin{choices}
 \choice want to ultimately optimize in the joint space of architectures and hyperparameters.
 \choice first optimize the architectures and afterwards the hyperparameters.
 \choice irst optimize the hyperparameters and afterwards the architecture.
\end{choices}

\question In dynamic algorithm configuration, we ...
\begin{choices}
 \choice select a configuration specifically to each instance at hand.
 \choice predict which configuration an algorithm should use.
 \choice search for fixed dynamic schedule of configurations.
 \choice learn a policy that predicts a configuration for a given algorithm state.
\end{choices}

\question To mitigate the risks of AutoML, we ...
\begin{choices}
 \choice will build systems that will avoid all of these risks automatically.
 \choice will blame ML and not AutoML.
 \choice teach others about potential risks of ML and AutoML.
 \choice will never publish our code as open-source.
\end{choices}

\pagebreak
%-------------------------------------------------------------------------------
\section{Evaluation}
Kahoot quiz from the video consulting hour: \url{https://create.kahoot.it/share/automlss20-w2/9239e560-d2d6-4063-91d3-1be13047ae41}
\question Training Machine Learning Models is ...
\begin{choices}
    \choice trivially easy.
    \choice fundamentally an optimization problem. % Correct
    \choice always cheap.
\end{choices}

\question Performance estimation of a model = performance estimation of an algorithm
\begin{choices}
    \choice True
    \choice False % Correct
\end{choices}

\question Reason for bad learning curves include ...
\begin{choices}
    \choice underfitting % Correct
    \choice overfitting % Correct
    \choice optimal hyperparameter settings
    \choice high bias   % Correct
\end{choices}

\question Choose all that apply:
\begin{choices}
    \choice underfitting $\mapsto$ high bias in model % Correct
    \choice underfitting $\mapsto$ high variance in model
    \choice overfitting $\mapsto$ high bias in model
    \choice overfitting $\mapsto$ high variance in model % Correct
\end{choices}

\question In a statistical hypothesis test with $\alpha = 0.05$ and $p < \alpha$ we
\begin{choices}
    \choice Reject $H_0$ % Correct
    \choice Accept $H_0$
    \choice We cannot draw a conclusion.
\end{choices}

\question In a statistical hypothesis test with $\alpha = 0.05$ and $p > \alpha$ we
\begin{choices}
    \choice Reject $H_0$
    \choice Accept $H_0$
    \choice We cannot draw a conclusion. % Correct
\end{choices}

\question Is it permissible to first look at your $p$ value before choosing your $\alpha$?
\begin{choices}
    \choice Yes
    \choice No % Correct
\end{choices}

\question A parametric statistical hypothesis test makes distributional assumptions.
\begin{choices}
    \choice True  % Correct
    \choice False
\end{choices}

\question Non-parametric test are (typically) more powerful compared to parametric tests (if applied correctly).
\begin{choices}
    \choice True
    \choice False % Correct
\end{choices}

\question We need multi-test-correction because otherwise ...
\begin{choices}
    \choice the probability of one test being wrong increases with the number of tests % Correct
    \choice the probability of one test being wrong decreases with the number of tests
    \choice it will increase the chance of rejecting $H_0$.
    \choice otherwise the number of required samples gets too large.
\end{choices}

\question An AutoML approach uses for its internal optimization
\begin{choices}
    \choice the test set to estimate the generalization error.
    \choice the train set to avoid peaking at the test set.
    \choice the validation set to estimate the generalization error. % Correct
    \choice nested re-sampling to get a more reliable generalization estimate. % Correct
\end{choices}

\question For two algorithms X and Y we list their classification performance in the following table, with $0$ indicating a wrong classification and $1$ indicating correct classification.
    \begin{center}
      \begin{tabular}{cc|cc}
          & & \multicolumn{2}{c}{X} \\
          & & $0$ & $1$ \\
          \hline
          \multirow{2}{*}{Y} & $0$ & 30 & 90 \\
          & $1$ & 75 & 5 \\
      \end{tabular}
    \end{center}
    Use the McNemar Test where the \texttt{$H_0$: both models have the same performance} and \texttt{$H_1$: performances of the models are not equal}  with $\alpha = 0.05$\footnote{$\chi^2_1 = 3.841$}
    % Chi^2 = 1.188
\begin{choices}
    \choice Reject $H_0$
    \choice Accept $H_0$
    \choice We cannot draw a conclusion. % Correct
\end{choices}

\question Instead consider now a different performance table with the rest being equal.
    \begin{center}
      \begin{tabular}{cc|cc}
          & & \multicolumn{2}{c}{X} \\
          & & $0$ & $1$ \\
          \hline
          \multirow{2}{*}{Y} & $0$ & 30 & 3 \\
          & $1$ & 17 & 5 \\
      \end{tabular}
    \end{center}
    % 17 + 3 <= 20 -> We shouldn't use McNemar
\begin{choices}
    \choice Reject $H_0$
    \choice Accept $H_0$
    \choice We cannot draw a conclusion.
    \choice We shouldn't use McNemar % Correct
\end{choices}

\question The post-hoc Nemenyi test ...
\begin{choices}
    \choice should be used before the Friedman test.
    \choice is used to determine if algorithms have the same ranks.
    \choice compares all pairs of algorithms to find best-performing algorithm after $H_0$ of the
Friedman-test was rejected. % Correct
\choice can not be used with more than 2 algorithms.
\end{choices}

\pagebreak
%-------------------------------------------------------------------------------
\section{Algorithm Selection}

\question In algorithm selection we ...
\begin{choices}
    \choice learn a mapping of algorithm to problem instance.
    \choice learn a mapping of problem instance to algorithm. % Correct
\end{choices}

\question Algorithm portfolios ...
\begin{choices}
    \choice ideally contain very similar algorithms.
    \choice combine algorithms with complementary strengths. % Correct
    \choice should never contain more than $3$ algorithms.
    \choice minimize the risk of poor performance. % Correct
\end{choices}

\question With a \textit{good} algorithm selector we ...
\begin{choices}
    \choice can only perform as good as the single best algorithm.
    \choice can only perform as good as the virtual best algorithm. % Correct
    \choice will likely have a performance better than the single best but worse than the virtual best algorithm. % Correct
\end{choices}

\question We should prefer parallel portfolios over selection ...
\begin{choices}
    \choice when enough resources are available and the problem is easily evaluated in parallel. % Correct
    \choice when algorithms already require parallel resources.
    \choice as it is guaranteed to be faster than algorithm selection.
    \choice as compute resources have gotten very cheap.
\end{choices}

\question To train an algorithm selector we generally ...
\begin{choices}
    \choice only need representative features of the problem instances.
    \choice require runtime data of the some algorithms on most of the problem instances.
    \choice have to evaluate all algorithms in the portfolio on all problem instances of a representative dataset. % Correct
    \choice only need enough data to determine the single best algorithm.
\end{choices}

\question Problem instance features ...
\begin{choices}
    \choice related properties of datasets to algorithm performance. % Correct
    \choice generally do not require domain expertise.
    \choice can not be computed by running a representative algorithm for a short time / on a subset of the data.
    \choice are not important to learn an algorithm selector. Algorithm features are informative enough.
\end{choices}

\question Probing features usually lead to much better results than using just syntactic and information-theoretic features.
\begin{choices}
    \choice True  % Correct
    \choice False
\end{choices}

\question Algorithm selection without problem features is impossible.
\begin{choices}
    \choice True
    \choice False % Correct
\end{choices}

\question In practice ...
\begin{choices}
    \choice all features are equally informative.
    \choice only very complex features are useful.
    \choice often only a small subset of the features are needed. % Correct
\end{choices}

\question Consider the following table containing the classification accuracies of $3$ different ML models.
    \begin{center}
      \begin{tabular}{cc|ccc}
          & & \multicolumn{3}{c}{Algorithm} \\
          & & SVM & CNN & kNN \\
          \hline
          \multirow{5}{*}{\rotatebox[origin=c]{90}{Dataset}} & $0$ & $0.59$ & $0.61$ & $0.55$\\
          & $1$ & $0.92$ & $0.93$ & $0.89$\\
          & $2$ & $0.32$ & $0.97$ & $0.75$\\
          & $3$ & $0.87$ & $0.80$ & $0.88$\\
          & $4$ & $0.95$ & $0.93$ & $0.65$\\
      \end{tabular}
    \end{center}
    Which statements are true?
    \begin{choices}
        \choice The single best algorithm is CNN.  % Correct
        \choice The single best algorithm is SVM.
        \choice The oracle would never select kNN.
        \choice The oracle would select every algorithm at least once. % Correct
    \end{choices}

\question Consider now the corresponding feature table containing two binary features.
    \begin{center}
      \begin{tabular}{cc|cc}
          & & \multicolumn{2}{c}{Feature} \\
          & & F1 & F2 \\
          \hline
          \multirow{5}{*}{\rotatebox[origin=c]{90}{Dataset}} & $0$ & 0 & 0\\
          & $1$ & 0 & 1\\
          & $2$ & 0 & 0\\
          & $3$ & 1 & 1\\
          & $4$ & 0 & 1\\
      \end{tabular}
    Based on these features, is it possible to learn a perfect selector?
    \begin{choices}
        \choice True
        \choice False % Correct  % Should not be possible as in the 01 case either SVM and CNN are not easy to distinguish from each other.
    \end{choices}
    \end{center}

\pagebreak
%-------------------------------------------------------------------------------
\section{HPO Basics}
\question We determine the optimal hyperparameters ...
\begin{choices}
    \choice with respect to the generalization performance. % Correct
    \choice with respect to the training performance.
    \choice based on incomplete runs.
\end{choices}

\question Model parameters ...
\begin{choices}
    \choice are optimized during training. % Correct
    \choice are inputs to the training.
    \choice are equivalent to hyperparameters.
\end{choices}

\question Hyperparameters ....
\begin{choices}
    \choice are outputs of the training.
    \choice often control the complexity of a model. % Correct
    \choice can influence model parameters. % Correct
    \choice can never change during training.
\end{choices}

\question Examples of model parameters are ...
\begin{choices}
    \choice learning rates for neural network training.
    \choice the number of neighbors in $k$NN.
    \choice neural network weights. % Correct
    \choice centroids of $k$-means\footnote{\url{https://en.wikipedia.org/wiki/K-means\_clustering\#Algorithms}}. %Correct.
    \choice number of layers in a neural network.
    \choice support vectors of an SVM. % Correct
    \choice coefficients of a linear model. % Correct
    \choice the reward function used in reinforcement learning.
\end{choices}

\question (Hyperparameter) Tuning is the process of finding good model hyperparameters $\lambda$.
\begin{choices}
    \choice True % Correct
    \choice False
\end{choices}

\question The components of a tuning problem are:  % Trick question. All of them are true
\begin{choices}
\choice The dataset.
\choice The learner (possibly: several competing learners?) that is tuned.
\choice The learner’s hyperparameters and their respective regions-of-interest over which we optimize.
\choice The performance measure, as determined by the application.
\choice A (resampling) procedure for estimating the predictive performance according to the
performance measure.
\end{choices}

\question The performance measure of the tuning problem has to be  identical to the loss function that defines the risk minimization problem for the learner.
\begin{choices}
\choice True
\choice False % Correct.
\end{choices}

\question Tuning
\begin{choices}
    \choice is easy because people properly document their machine learning models and extensively document their behaviour on a plethora of application domains.
    \choice is hard because it is viewed as a black-box problem. % Correct
    \choice only involves very simple configuration spaces.
    \choice is hard because of the common stochasticity of machine learning models. % Correct
\end{choices}

\question Grid search can be disadvantageous compared to random search if
some hyperparameters have little or no
influence on the model.
\begin{choices}
    \choice True
    \choice False
\end{choices}

\question With a tuning budget of $16$ samples and four parameters ...
\begin{choices}
    \choice both grid and random search will evaluate $2$ unique values per parameter.
    \choice grid search will most likely evaluate more values per parameter than random search.
    \choice random search will most likely evaluate $16$ unique values per parameter and grid search $2$. % Correct. GS evaluates budget^(1/parameters) unique values.
    \choice both random search and grid search will evaluate $16$ unique values per parameter.
\end{choices}

\question Evolutionary Algorithms ...
\begin{choices}
    \choice are conceptually simple, yet powerful enough to solve complex problems. % Correct
    \choice can not handle categorical parameters.
    \choice are perfectly suited for expensive problems like HPO.
    \choice have quite a few control parameters which often require tuning. % Correct
\end{choices}

\question Default heuristics ...
\begin{choices}
    \choice are the same thing as static default hyperparameters.
    \choice determine which default to use out of a portfolio of defaults. % Correct
    \choice are very difficult to use.
\end{choices}
\end{questions}
\end{document}
