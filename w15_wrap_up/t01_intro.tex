\input{t00_template.tex}

\usepackage[normalem]{ulem}
\usepackage{pifont}

\subtitle{Wrap Up}


\begin{document}

\maketitle


%----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}{From HPO to AutoML}
  So far we covered
  \begin{itemize}
    \item Mechanisms to select ML algorithms for a data set (algorithm selection)
        SOLLTE DAS RAUS?
    \item HPO as black-box optimization
    \begin{itemize}
      \item Grid- and random search, EAs, BO
    \end{itemize}
    \item HPO as a grey box problem
    \begin{itemize}
      \item Hyperband, BOHB
    \end{itemize}
    \item Neural Architecture Search (NAS)
    \begin{itemize}
      \item One-Shot approaches, DART
    \end{itemize}
    \item Dynamic algorithm configuration (learning to learn)
    \begin{itemize}
      \item Adapt configuration during training
    \end{itemize}
  \end{itemize}  
\end{frame}

\begin{frame}{From HPO to AutoML}
    \begin{center}
      \includegraphics[width = 0.9\linewidth]{images/drawing.pdf}  
    \end{center}
\end{frame}

\section{The Missing Building Blocks}

\begin{frame}{What is missing?}
  \begin{columns}
    \begin{column}{0.5\textwidth}
        What do I need to know as an AutoML user?
        \begin{itemize}
          \item \sout{Nothing, because it is automatic.}
          \item Understand limitations of AutoML and framework.
          \item Know how to interpret the results.
          \item Maybe: Preprocessing and feature extraction.
        \end{itemize}

        \vspace{1em}

        Ingredients to implement an AutoML?
        \begin{itemize}
          \item HPO algorithm
          \item ML / Pipeline framework 
          \item Parallelization / Multifidelity
          \item Process encapsulation and time capping 
          % \item (Preprocessing)
        \end{itemize}
    \end{column}%
    \begin{column}{0.5\textwidth}
      \begin{center}
        Academic view:
        \scalebox{0.45}{
          \input{tikz/automl_overview.tex}
        }

        \vspace{1em}

        Practitioners view:
        \scalebox{0.45}{
          \input{tikz/true_automl_overview.tex}
        }
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

% \begin{frame}{Automate HPO}

%   \begin{columns}
%     \begin{column}{0.59\textwidth}

%       For AutoML the user only supplies \ldots
%       \begin{itemize}
%         \item dataset,
%         \item performance measure and
%         \item usually a time budget
%       \end{itemize}

%       To do HPO we need to \ldots
%       \begin{itemize}
%         \item preprocessing manually,
%         \item decide on an optimization algorithm,
%         \item an ML algorithm (to generate Inducer $\inducer$),
%         \item a search space $\pcs$ and
%         \item a resampling strategy to evaluate $\cost(\conf)$.
%       \end{itemize}

%       To build an AutoML System we have to make these choices automatically $\rightarrow$ Following slides.

%     \end{column}%
%     \begin{column}{0.4\textwidth}
%       \begin{center}
%         \includegraphics[width = \linewidth]{images/tuning.pdf}    
%       \end{center}
%     \end{column}
%   \end{columns}

% \end{frame}

\begin{frame}{Choice of Learning Algorithm}
  \begin{itemize}
    % \item A good AutoML System should consider more than one learning algorithm. More on that later.
    \item A plethora of learners exists, for different data sets different models
        are likely needed.
    \item Studies\lit{\href{https://dl.acm.org/doi/10.5555/2627435.2697065}{Delgado et al., JMLR 2014}} and experience have shown one the following 
        is usually good -- on tabular data:
    \begin{itemize}
      \item Penalized regression, SVM, gradient boosting, random forests, neural networks
      \item Random forests only beaten on few tabular datasets by current AutoML frameworks\lit{\href{https://arxiv.org/abs/1907.00909}{Gijsbers et al., 2019}}.
      \item Example: Auto-Sklearn 2.0\lit{\href{https://arxiv.org/abs/2007.04074}{Feurer et al., 2020}} uses: extra trees, gradient boosting, passive aggressive, random forest, linear model
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Choice of Search Space for a Learning Algorithm}
  \begin{columns}
    \begin{column}{0.6\textwidth}
    % Which hyperparameters should we consider for a given learning algorithm?
    Ranges often selected based on experience
    \begin{itemize}
      \item see other AutoML frameworks: e.g.\ Auto-Sklearn 2.0~\lit{\href{https://arxiv.org/abs/2007.04074}{Feurer et al., 2020}}
      \item Sensitivity analysis often does not exist for ML algorithms
    \end{itemize}
    Options:
    \begin{itemize}
      \item Use huge search space to cover all possibilities \\ 
            (combine with meta-learning for good initial design for Bayesian Optimization)
      \item Use smaller search space that is known to work well for many cases
      % \item FIXME: Wollte Bernd hier nochwas spezielles?
    \end{itemize}
    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        \only<1>{
          \includegraphics[width = 0.8\linewidth]{images/probst2019jmlr_tab1.pdf}
        }
        \only<2>{
          \includegraphics[width = 0.8\linewidth]{images/probst2019jmlr_tab3.pdf}   
        }

        {\tiny Taken from \href{https://www.jmlr.org/papers/volume20/18-444/18-444.pdf}{Probst et al., 2019 JMLR}.}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Choice of Resampling Strategy}
  
  For computation of generalization error / cost:
  \begin{equation*}
    \cost(\conf) = \frac{1}{k}\sum_{i = 1}^k \widehat{GE}_{\dataset_{\text{val}}^i}\left(\inducer(\dataset_{\text{train}}^i, \conf)\right)
  \end{equation*}
  % that defines the objective of the black-box optimization we need a resampling strategy.

  \vspace{1em}
  \begin{columns}
    \begin{column}{0.5\textwidth}
    Rules of thumb:
    \begin{itemize}
      \item Default: 10-fold CV ($k=10$)
      \item Huge datasets: Holdout
      \item Tiny datasets: 10x10 repeated CV
      \item Stratification for imbalanced classes
    \end{itemize}
    \end{column}
    
    \begin{column}{0.5\textwidth}
 Watch out for this:       
    \begin{itemize}
      \item Small sample size situation because of imbalancies    
      \item Leave-one-object out
      \item Time dependencies
      \item A good AutoML system should let you customize resampling
          % errors here can mean: garbage in, garbvgae out
      \item meta-learn good resampling strategy \lit{\href{https://arxiv.org/abs/2007.04074}{Feurer et al., 2020}}
    \end{itemize}
    \end{column}
    \end{columns}
    
\end{frame}

\begin{frame}{Choice of Optimization Algorithm}
  Choose optimization algorithm based on \ldots
  \begin{itemize}
    \item complexity of search space / budget
    \item time-costs of evals
  \end{itemize}

  \vspace{0.5em}

  Complex search space
  \begin{itemize}
    \item[$\rightarrow$] EA with exploratory character, TPE, BO with RF 
    % \item[$\rightarrow$] Make use of Grey-Box Optimizers: Hyperband, BOHB
  \end{itemize}
  Numerical (lower-dim) search space and tight budget
  \begin{itemize}
    \item[$\rightarrow$] BO with GP
  \end{itemize}
  Expensive evals
  \begin{itemize}
    \item[$\rightarrow$] Hyperband, BOHB
  \end{itemize}
  Deep Learning 
  \begin{itemize}
    \item[$\rightarrow$]Parametrize architectures, then HPO, see above
    \item[$\rightarrow$]NAS 
  \end{itemize}

\end{frame}



\begin{frame}{Preprocessing}
  \begin{columns}
    \begin{column}{0.5\textwidth} 
      Ideal AutoML systems should also optimize: 
      % the following steps wrt.\ given cost function:
      \begin{itemize}
        \item[\ding{55}] Data preprocessing
        \item[\ding{55}] Feature engineering
        \item[\ding{55}] Feature selection
          % \item Feature construction  
        \item[\ding{51}] Model training
      \end{itemize}
      \vspace{10em}
      {\tiny \ding{51} = already covered, \ding{55} = not covered so far}
    % We already know how to optimize the ml algorithm. How about the preprocessing steps?
    \end{column}%
    \begin{column}{0.5\textwidth}
      \begin{center}
        \includegraphics[width = \linewidth]{images/AutoMLPipeline.jpg}  
      \end{center}
    \end{column}
  \end{columns}
  
\end{frame}

\section{Common Preprocessing Steps}

\begin{frame}{Preprocessing capabilities differ heavily}
  \begin{columns}
    \begin{column}{0.6\textwidth}
      \vspace*{-1cm}
      \begin{center}
        \includegraphics[width = \linewidth]{images/Truong2019Towards_fig2.pdf}
      \end{center}
    \end{column}%
    \begin{column}{0.3\textwidth}
    \small
      Taken from \href{https://doi.org/10.1109/ICTAI.2019.00209}{Truong et al., 2019 ICTAI}.
      \vspace{1em}

      Highlighted: Non-commercial AutoML frameworks

      \begin{itemize}
        \item detection of feature types: some
        \item preprocess categorical values: some
        \item missing values: all
        \item imbalances: all
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Cleaning}
  Data cleaning can hardly be automatized but a few heuristics exist:
  \begin{itemize}
    \item Remove ID Columns, Columns with mostly unique values.
    \item Outlier detection (in the feature space)
    \item Detect time series or spatial data $\rightarrow$ randomized validation might be flawed.
  \end{itemize}
\end{frame}

\begin{frame}{Categorical Features: Dummy Encoding}
  % Goals:
  % \begin{itemize}
  %   \item Convert all categorical features.
  %   \item Avoid high cardinality categorical features.
  % \end{itemize}
  ML algorithm does not support categorical features + few unique values $\rightarrow$ use dummy encoding.
  \begin{center}
    \resizebox{0.3\linewidth}{!}{
    \begin{tabular}{r|l|l}
    \hline
    SalePrice & Central.Air & Bldg.Type\\
    \hline
    189900 & Y & 1Fam\\
    \hline
    195500 & Y & 1Fam\\
    \hline
    213500 & Y & TwnhsE\\
    \hline
    191500 & Y & TwnhsE\\
    \hline
    236500 & Y & TwnhsE\\
    \hline
    \end{tabular}
    } \\
    \begin{tikzpicture}
      %\useasboundingbox (-2,0);
      \node[single arrow,draw=black,fill=black!10,minimum height=1cm,shape border rotate=270] at (0,-1) {};
    \end{tikzpicture} \\
  \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{r|r|r|r|r|r}
    \hline
    SalePrice & Y & Bldg.Type.2fmCon & Bldg.Type.Duplex & Bldg.Type.Twnhs & Bldg.Type.TwnhsE\\
    \hline
    189900 & 1 & 0 & 0 & 0 & 0\\
    \hline
    195500 & 1 & 0 & 0 & 0 & 0\\
    \hline
    213500 & 1 & 0 & 0 & 0 & 1\\
    \hline
    191500 & 1 & 0 & 0 & 0 & 1\\
    \hline
    236500 & 1 & 0 & 0 & 0 & 1\\
    \hline
    \end{tabular}
  }
  \end{center}
\end{frame}

\begin{frame}{Categorical Features: Target Encoding}
  Avoid high cardinality categorical features because they are problematic for all ML algorithms $\rightarrow$ use Target Encoding (also Impact Encoding).
  \begin{columns}
    \begin{column}{0.7\textwidth}


      \textbf{Goal}: Each categorical feature $\bm{x}$ should be encoded in a single numeric feature $\tilde{\bm{x}}$
      
      \vspace*{-0.5cm}  
      {\footnotesize
      \begin{align*}
      \text{Regression:} \operatorname{Impact}(x) &= \E(\bm{y} | x) - \E(\bm{y}) \\
      \text{Classification:} \operatorname{Impact}(x) &= \operatorname{logit}(P( y = \text{target} | x)) - \operatorname{logit}(P( y = \text{target}))
      \end{align*}
      }
      \vspace*{-0.5cm}  
      \begin{itemize}
        \item Needs regularization (through CV) to prevent target leakage \lit{\href{https://arxiv.org/abs/1611.09477}{Zumel et al., 2019}}
        \item Advantage: Handles unknown categorical levels on test data.
      \end{itemize}
      Alternatives: Factorization Machines, clustering feature levels, feature hashing
    \end{column}%
    \begin{column}{0.3\textwidth}
      \vspace{-1cm}
      \begin{center}
        %\includegraphics[width = \textwidth]{images/scetch_impact_encoding.pdf}
        \resizebox{0.8\columnwidth}{!}{
        \begin{tikzpicture}
          \path
            (0,1) coordinate (A) node[above, inner sep=0] {
              \begin{tabular}{ c c }
                \multicolumn{2}{ c }{$\dataset_{\text{train}}$} \\
                %\hline
                $x$ & $y$  \\
                \hline
                A & 10 \\
                A & 20 \\
                B & 30 \\
              \end{tabular}
            }
            (0,-1) coordinate (B) node[below, inner sep=0] {
              \begin{tabular}{ c c }
                \multicolumn{2}{ c }{$\dataset_{\text{test}}$} \\
                %\hline
                x & y  \\
                \hline
                A & -5 \\
                B & 10 \\
                C & 0 \\
              \end{tabular}
            }
            (1,0) coordinate (C) node[right, inner sep=0] {
              \begin{tabular}{ c c }
                \multicolumn{2}{ c }{Encoding} \\
                %\hline
                $x$ & $\tilde{x}$  \\
                \hline
                A & -5 \\
                B & 10 \\
              \end{tabular}
            };
          %\draw[->] (A)--(B) (0,0) -- node[above]{}(C);
          \draw[myarrow, shorten <=0.1cm, shorten >=0.1cm] (A.south)--(C.north);
          \draw[myarrow, shorten <=0.1cm, shorten >=0.1cm] (C.south)--(B.north);
        \end{tikzpicture}
        }
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Common Preprocessing Steps: Missing Values}
  \begin{columns}
    \begin{column}{0.7\textwidth}
      Missing values:
      \begin{itemize}
        \item Additional factor column indicating missingness
        \item Replace missing values with \emph{out of range} or median/mode
        \item Advanced imputation strategies seldom advantageous (also because data mostly not missing at random)
      \end{itemize}
    \end{column}%
    \begin{column}{0.3\textwidth}
            \vspace{-0.65cm}
      \begin{center}
        %\includegraphics[width = \textwidth]{images/scetch_impact_encoding.pdf}
        \resizebox{0.9\columnwidth}{!}{
        \begin{tikzpicture}
          \path
            (0,1) coordinate (A) node[above, inner sep=0] {
              \begin{tabular}{ c c }
                \multicolumn{2}{ c }{$\dataset_{\text{train}}$} \\
                %\hline
                $x_1$ & $x_2$  \\
                \hline
                10 & 3 \\
                NA & 2 \\
                5  & NA \\
              \end{tabular}
            }
            (0,-1) coordinate (B) node[below, inner sep=0] {
              \begin{tabular}{ c c }
                \multicolumn{2}{ c }{$\tilde{\dataset}_{\text{train}}$} \\
                %\hline
                $x_1$ & $x_2$  \\
                \hline
                10  & 3 \\
                100 & 2 \\
                5   & 30 \\
              \end{tabular}
            }
            (1,0) coordinate (C) node[right, inner sep=0] {
              \begin{tabular}{ c c }
                %\multicolumn{2}{ c }{Replacements} \\
                %\hline
                Feature & NA  \\
                \hline
                $x_1$ & 100 \\
                $x_2$ & 30 \\
              \end{tabular}
            };
          %\draw[->] (A)--(B) (0,0) -- node[above]{}(C);
          \draw[myarrow, shorten <=0.1cm, shorten >=0.1cm] (A.south)--(C.north);
          \draw[myarrow, shorten <=0.1cm, shorten >=0.1cm] (C.south)--(B.north);
        \end{tikzpicture}
        }
        \end{center}
        {\footnotesize
        \emph{Out of range} imputation, useful for tree-based ML algorithms.
        }
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Feature Selection}
    \begin{columns}
      \begin{column}{0.4\textwidth}
        Feature selection:
        \begin{itemize}
          \item Filter
          \item Stepwise selection methods: Needs to be applied on the whole pipeline (impractical!)
          \item Seldom increases performance but decreases computational costs $\rightarrow$ Multi-criteria optimization.
          \begin{itemize}
            \item Combined Feature Selection and HPO: \lit{\href{https://doi.org/10.1145/3377930.3389815}{Binder et al., 2020 GECCO}}
          \end{itemize}
          \item Happens indirectly in learning algorithm: random forest, lasso regression, \ldots %FIXME More examples.
        \end{itemize}
      \end{column}%
      \begin{column}{0.6\textwidth}
        \begin{center}
          \includegraphics[width=0.35\textwidth, trim=450 100 110 60, clip]{images/feat_extr_vs_selection.pdf}%
          \includegraphics[width=0.55\linewidth]{images/Binder2020multiobjective_fig3.pdf}

          {\tiny \hfill Taken from \href{https://doi.org/10.1145/3377930.3389815}{Binder et al., 2020 GECCO}.}
        \end{center}
      \end{column}
    \end{columns}
    
\end{frame}

\begin{frame}{Common Feature Construction Methods}
  \begin{columns}
    \begin{column}{0.6\textwidth}
    Reduction:
    \begin{itemize}
      \item PCA, ICA, autoencoder
    \end{itemize}

    Feature extraction:
    \begin{itemize}
      \item Polynomial Features: $x_j \longrightarrow x_j, x_j^2, x_j^3, ...$
      \item Interactions: $x_j, x_k \longrightarrow x_j, x_k, x_j \cdot x_k$
    \end{itemize}

    Feature generation:
    \begin{itemize}
      \item Transform to ``circular'' features (year, month, day) \\
      e.g.\ $\tilde x_1 = sin(2\pi \cdot x /24)$ and $\tilde x_2 = cos(2\pi \cdot x /24)$
    \end{itemize}
    
    Combine with external data:
    \begin{itemize}
      \item names $\longrightarrow$ gender, ethnicity, age
      \item home address $\longrightarrow$ household income
      \item location + date $\longrightarrow$ weather
    \end{itemize}

    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        \includegraphics[width= \textwidth, trim=0 100 390 60, clip]{images/feat_extr_vs_selection.pdf}
      \end{center}
    \end{column}
  \end{columns}
    
\end{frame}


\begin{frame}{Imbalanced Classes}
  Imbalanced classes:
  \begin{itemize}
    \item over-sampling of minority class
    \item seldom: under-sampling of majority class
    \item Influence of more advanced methods (e.g.\ SMOTE) on the predictive accuracy questionable.
  \end{itemize}
\end{frame}

\section{Practical Problems}

\begin{frame}{Practical Problems: Stability}
  AutoML system should: 
  \begin{itemize}
    \item never fail to return a result
    \item or at least have intermediate results that can be used to continue optimization
    \item terminate within a given time
  \end{itemize}

  Failure points:
  \begin{itemize}
    \item optimizer can crash (e.g.\ the surrogate of MBO) $\longrightarrow$ return intermediate result, continue with warmstart / random search
    \item training can crash $\longrightarrow$ train robust fallback learner (e.g.\ majority/mean predictor)
    \item prediction can crash $\longrightarrow$ use prediction of fallback learner
  \end{itemize}

  Failure classes:
  \begin{itemize}
    \item ``simple'' error $\longrightarrow$ TryCatch
    \item segfault $\longrightarrow$ encapsulation in separate process
    \item no termination $\longrightarrow$ encapsulation with time-out
    \item out-of-memory $\longrightarrow$ problematic because it can affect the whole system if garbage collection fails
  \end{itemize}

\end{frame}

\begin{frame}{Practical Problems: Parallelization}
  Parallelization should consider:
  \begin{itemize}
    \item multiple cores
    \item multiple nodes (complicated, because no unified API)
  \end{itemize}

  Possible parallelization levels:
  \begin{itemize}
    \item training step of the ML algorithm (usually on CPU level)
    \item resampling iterations
    \item multiple hyperparameter configuration candidates
  \end{itemize}

  Possible problems
  \begin{itemize}
    \item heterogeneous runtimes cause idling $\longrightarrow$ use more jobs then workers or asynchronous strategies / kill runtime outliers
    \item main memory or CPU-cache becomes bottleneck
    \item communication 
  \end{itemize}

\end{frame}


\section{Combined Preprocessing and Model Building: Pipelining}

\begin{frame}{Pipelining}

  Most preprocessing steps have parameters or can be switched on/off in the pipeline.

  \vspace{1em}

  \textbf{Goal:} Find optimal preprocessing parameters $\rightarrow$ HPO

  \begin{columns}
    \begin{column}{0.5\textwidth}
    \begin{itemize}
      \item Most preprocessing methods have internal parameters similar to the model of an inducer.
      \item Applying preprocessing to the whole dataset leads to data leakage.
      \item Pipeline has to be optimized as a whole: $\pcs = \pcs_{\text{pipeline}} \times \pcs_\inducer$ within the resampling procedure.
    \end{itemize}
    \end{column}%
    \begin{column}{0.5\textwidth}
      \begin{center}
        \includegraphics[page=19, width=\textwidth, trim=20 60 30 35, clip]{images/mlr3Pipelines_graphics}
      \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Optimizing Pipelines}

  \begin{columns}
    \begin{column}{0.6\textwidth}
      \vspace*{-0.5em}
      \begin{itemize}
        \item Introducing different choices for preprocessing.
        \item Tuning over multiple learning algorithms.
        \item[$\rightarrow$] $\pcs$ becomes hierarchical search space!
      \end{itemize}
      
      \vspace*{0.5em}

      Suitable optimizers:
      \begin{itemize}
        \item TPE
        \item Random Search, Hyperband with sampler that follows the hierarchy
        \item BO with RF surrogate, or GP with 
        \item Evolutionary approaches (similar to NAS)
      \end{itemize}

    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        %\includegraphics[page=7, width=\textwidth, trim=160 0 30 160, clip]{images/mlr3Pipelines_graphics}
        %\includegraphics[width = \textwidth]{images/stacking.pdf}
        \includegraphics[width = \textwidth, trim=160 0 160 5, clip]{images/dag.pdf}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Obtaining Final Model}

  Options:
  \begin{itemize}
    \item Choose the optimal path as linear pipeline.
    \item Build ensemble of best configurations (e.g. \lit{\href{https://papers.nips.cc/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html}{Feurer et al., NIPS 2015}}, \lit{\href{https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf}{H2O AutoML, LeDell et al., 2020}}).
  \end{itemize}
  \begin{center}
    \includegraphics[width = 0.60\textwidth]{images/stacking.pdf}
  \end{center}
    
\end{frame}

\begin{frame}{Current Benchmark}
  \begin{columns}
    \begin{column}{0.43\textwidth}
      \vspace{-0.7cm}
      \begin{center}
        \includegraphics[width = \textwidth]{images/gijsbers_open_2019_tab2.pdf}
      \end{center}
    \end{column}%
    \begin{column}{0.57\textwidth}
      Table taken from \lit{\href{http://arxiv.org/abs/1907.00909}{Gijsbers et al., 2019}}.

      \begin{itemize}
        \item On some datasets AutoML yields big performance improvements
        \item On many datasets RF is equally good
        \item This table does not consider stochasticity of results
      \end{itemize}

    \end{column}
  \end{columns}
\end{frame}

% \begin{frame}[containsverbatim,allowframebreaks]{Software}

% \begin{itemize}
%   \item DataRobot (comercial, gui)
%   \item H20.ai (comercial but open source, r, python)
%   \item TPOT, Tree-based Pipeline Optimization Tool  (2016-cont, open source, evolutionary approach) % show plot https://github.com/EpistasisLab/tpot
%   \item AutoWEKA (2016, open source)
%   \item mlr3automl (2020, prelim)
%   \item Hyperopt-Sklearn (2014-cont) Only HPO
%   \item Auto-Sklearn (2.0) (2015-cont) BO, ensembles, meta-learning
% \end{itemize}

% \end{frame}



\end{document}
