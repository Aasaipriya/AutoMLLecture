\input{t00_template.tex}

\usepackage{ulem}
\usepackage{pifont}

\subtitle{Wrap Up}


\begin{document}

\maketitle


%----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}{From HPO to AutoML}
  So far we covered
  \begin{itemize}
    \item Mechanisms to select ML algorithms for a data set (algorithm selection)
        SOLLTE DAS RAUS?
    \item HPO as black-box optimization
    \begin{itemize}
      \item Grid- and random search, EAs, BO
    \end{itemize}
    \item HPO as a grey box problem
    \begin{itemize}
      \item Hyperband, BOHB
    \end{itemize}
    \item Neural Architecture Search (NAS)
    \begin{itemize}
      \item One-Shot approaches, DART
    \end{itemize}
    \item Dynamic algorithm configuration (learning to learn)
    \begin{itemize}
      \item Adapt configuration during training
    \end{itemize}
  \end{itemize}  
\end{frame}

\begin{frame}{From HPO to AutoML}
    \begin{center}
      \includegraphics[width = 0.9\linewidth]{images/drawing.pdf}  
    \end{center}
\end{frame}

\section{The Missing Building Blocks}

\begin{frame}{What is missing?}
  \begin{columns}
    \begin{column}{0.5\textwidth}
        What do I need to know as an AutoML user?
        \begin{itemize}
          \item \sout{Nothing, because it is automatic.}
          \item Understand limitations of AutoML and framework.
          \item Know how to interpret the results.
          \item Maybe: Preprocessing and feature extraction.
        \end{itemize}

        \vspace{1em}

        Ingredients to implement an AutoML?
        \begin{itemize}
          \item HPO algorithm
          \item ML / Pipeline framework 
          \item Parallelization / Multifidelity
          \item Process encapsulation and time capping 
          % \item (Preprocessing)
        \end{itemize}
    \end{column}%
    \begin{column}{0.5\textwidth}
      \begin{center}
        Academic view:
        \scalebox{0.45}{
          \input{tikz/automl_overview.tex}
        }

        \vspace{1em}

        Practitioners view:
        \scalebox{0.45}{
          \input{tikz/true_automl_overview.tex}
        }
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Automate HPO}

  \begin{columns}
    \begin{column}{0.59\textwidth}

      For AutoML the user only supplies \ldots
      \begin{itemize}
        \item dataset,
        \item performance measure and
        \item usually a time budget
      \end{itemize}

      To do HPO we need to \ldots
      \begin{itemize}
        \item preprocessing manually,
        \item decide on an optimization algorithm,
        \item an ML algorithm (to generate Inducer $\inducer$),
        \item a search space $\pcs$ and
        \item a resampling strategy to evaluate $\cost(\conf)$.
      \end{itemize}

      To build an AutoML System we have to make these choices automatically $\rightarrow$ Following slides.

    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        \includegraphics[width = \linewidth]{images/tuning.pdf}    
      \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Choice of Learning Algorithm}
  \begin{itemize}
    % \item A good AutoML System should consider more than one learning algorithm. More on that later.
    \item A plethora of learners exists, for different data sets different models
        are likely needed.
    \item Studies\footnote{\href{https://dl.acm.org/doi/10.5555/2627435.2697065}{Delgado et al., JMLR 2014}} and experience have shown one the following 
        is usually good -- on tabular data:
    \begin{itemize}
      \item Penalized regression, SVM, gradient boosting, random forests, neural networks
      \item Random forests only beaten on few datasets by current AutoML frameworks\footnote{\href{https://arxiv.org/abs/1907.00909}{Gijsbers et al., 2019}}.
      \item Example: Auto-Sklearn 2.0\footnote{\href{https://arxiv.org/abs/2007.04074}{Feurer et al., 2020}} uses: extra trees, gradient boosting, passive aggressive, random forest, linear model
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Choice of Search Space for a Learning Algorithm}
  \begin{columns}
    \begin{column}{0.6\textwidth}
    % Which hyperparameters should we consider for a given learning algorithm?
    \begin{itemize}
      \item Ranges often selected based on experience
      \begin{itemize}
        \item Compare to other AutoML Frameworks: e.g.\ Auto-Sklearn 2.0~\lit{\href{https://arxiv.org/abs/2007.04074}{Feurer et al., 2020}}
            WAS SOLL DAS HEISSEN
      \end{itemize}
      \item Sensitivity analysis often does not exist for learners
      \item Solution: Analysis of previous HPO runs and learn mapping $\datasets \rightarrow \mathcal{P}(\pcs)$ is risky (leaving out important ranges) and complicated.
      \item Instead: Use big search space $\pcs$ and try to predict good initial design (e.g.\ for Bayesian Optimization).
          NAJA WEISS NICHT OB ICH DEM SO ZUSTIMME
    \end{itemize}
    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        \only<1>{
          \includegraphics[width = 0.8\linewidth]{images/probst2019jmlr_tab1.pdf}
        }
        \only<2>{
          \includegraphics[width = 0.8\linewidth]{images/probst2019jmlr_tab3.pdf}   
        }

        {\tiny Taken from \href{https://www.jmlr.org/papers/volume20/18-444/18-444.pdf}{Probst et al., 2019 JMLR}.}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Choice of Resampling Strategy}
  
  For computation of generalization error / cost:
  \begin{equation}
    \cost(\conf) = \frac{1}{R}\sum_{i = 1}^R \widehat{GE}_{\dataset_{\text{val}, i}}\left(\inducer(\dataset_{\text{train}, i}, \conf)\right)
  \end{equation}
  % that defines the objective of the black-box optimization we need a resampling strategy.

  \vspace{1em}
  \begin{columns}
    \begin{column}{0.5\textwidth}
    Rules of thumb:
    \begin{itemize}
      \item Default: 10-fold CV ($R=10$)
      \item Huge datasets: Holdout
      \item Tiny datasets: 10x10 repeated CV
      \item Stratification for imbalanced classes
    \end{itemize}
    \end{column}
    
    \begin{column}{0.5\textwidth}
 Watch out for this:       
    \begin{itemize}
      \item Small sample size situation because of imbalancies    
      \item Leave-one-object out
      \item Time dependencies
      \item A good AutoML system should let you customize resampling
          % errors here can mean: garbage in, garbvgae out
    \end{itemize}
    \end{column}
    \end{columns}
    
\end{frame}

\begin{frame}{Choice of Optimization Algorithm}
  Choose optimization algorithm based on \ldots
  \begin{itemize}
    \item complexity of search space / budget
    \item time-costs of evals
  \end{itemize}

  \vspace{0.5em}

  Complex search space
  \begin{itemize}
    \item[$\rightarrow$] Random Search, TPE, BO with RF 
    % \item[$\rightarrow$] Make use of Grey-Box Optimizers: Hyperband, BOHB
  \end{itemize}
  Numerical (lower-dim) search space and tight budget
  \begin{itemize}
    \item[$\rightarrow$] BO with GP
  \end{itemize}
  Expensive evals
  \begin{itemize}
    \item[$\rightarrow$] Hyperband, BOHB
  \end{itemize}
  Deep Learning 
  \begin{itemize}
    \item[$\rightarrow$]Parametrize architectures, then HPO, see above
    \item[$\rightarrow$]NAS 
  \end{itemize}

\end{frame}



\begin{frame}{Preprocessing}
  \begin{columns}
    \begin{column}{0.5\textwidth} 
      Ideal AutoML systems should also optimize: 
      % the following steps wrt.\ given cost function:
      \begin{itemize}
        \item[\ding{55}] Data preprocessing
        \item[\ding{55}] Feature engineering
        \item[\ding{55}] Feature selection
          % \item Feature construction  
        \item[\ding{51}] Model training
      \end{itemize}
    % We already know how to optimize the ml algorithm. How about the preprocessing steps?
    \end{column}%
    \begin{column}{0.5\textwidth}
      \begin{center}
        \includegraphics[width = \linewidth]{images/AutoMLPipeline.jpg}  
      \end{center}
    \end{column}
  \end{columns}
  
\end{frame}

\section{Common Preprocessing Steps}

\begin{frame}{Preprocessing not the strength of Non-commercial AutoML}
  \begin{columns}
    \begin{column}{0.6\textwidth}
      \vspace*{-1cm}
      \begin{center}
        \includegraphics[width = \linewidth]{images/Truong2019Towards_fig2.pdf}
      \end{center}
    \end{column}%
    \begin{column}{0.3\textwidth}
    \small
      Taken from \href{https://doi.org/10.1109/ICTAI.2019.00209}{Truong et al., 2019 ICTAI}.
      \vspace{1em}

      Highlighted: Non-commercial AutoML Frameworks
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Cleaning}
  Data cleaning can hardly be automatized but a few heuristics exist:
  \begin{itemize}
    \item Remove ID Columns, Columns with mostly unique values.
    \item Outlier detection (in the feature space)
    \item Detect time series or spatial data $\rightarrow$ randomized validation might be flawed.
  \end{itemize}
\end{frame}

\begin{frame}{Categorical Features: Dummy Encoding}
  % Goals:
  % \begin{itemize}
  %   \item Convert all categorical features.
  %   \item Avoid high cardinality categorical features.
  % \end{itemize}
  ML algorithm does not support categorical features + few unique values $\rightarrow$ use dummy encoding.
  \begin{center}
    \resizebox{0.3\linewidth}{!}{
    \begin{tabular}{r|l|l}
    \hline
    SalePrice & Central.Air & Bldg.Type\\
    \hline
    189900 & Y & 1Fam\\
    \hline
    195500 & Y & 1Fam\\
    \hline
    213500 & Y & TwnhsE\\
    \hline
    191500 & Y & TwnhsE\\
    \hline
    236500 & Y & TwnhsE\\
    \hline
    \end{tabular}
    } \\
    \begin{tikzpicture}
      %\useasboundingbox (-2,0);
      \node[single arrow,draw=black,fill=black!10,minimum height=1cm,shape border rotate=270] at (0,-1) {};
    \end{tikzpicture} \\
  \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{r|r|r|r|r|r}
    \hline
    SalePrice & Y & Bldg.Type.2fmCon & Bldg.Type.Duplex & Bldg.Type.Twnhs & Bldg.Type.TwnhsE\\
    \hline
    189900 & 1 & 0 & 0 & 0 & 0\\
    \hline
    195500 & 1 & 0 & 0 & 0 & 0\\
    \hline
    213500 & 1 & 0 & 0 & 0 & 1\\
    \hline
    191500 & 1 & 0 & 0 & 0 & 1\\
    \hline
    236500 & 1 & 0 & 0 & 0 & 1\\
    \hline
    \end{tabular}
  }
  \end{center}
\end{frame}

\begin{frame}{Categorical Features: Target Encoding}
  Avoid high cardinality categorical features because they are problematic for all ML algorithms $\rightarrow$ use Target Encoding (also Impact Encoding).
  \begin{columns}
    \begin{column}{0.7\textwidth}


      \textbf{Goal}: Each categorical feature $\bm{x}$ should be encoded in a single numeric feature $\tilde{\bm{x}}$
      
      \vspace*{-0.5cm}  
      {\footnotesize
      \begin{align*}
      \text{Regression:} \operatorname{Impact}(x) &= \E(\bm{y} | x) - \E(\bm{y}) \\
      \text{Classification:} \operatorname{Impact}(x) &= \operatorname{logit}(P( y = \text{target} | x)) - \operatorname{logit}(P( y = \text{target}))
      \end{align*}
      }
      \vspace*{-0.5cm}  
      \begin{itemize}
        \item Needs regularization (through CV) to prevent target leakage \lit{\href{https://arxiv.org/abs/1611.09477}{Zumel et al., 2019}}
        \item Advantage: Handles unknown categorical levels on test data.
      \end{itemize}
      Alternatives: Factorization Machines, clustering feature levels, feature hashing
    \end{column}%
    \begin{column}{0.3\textwidth}
      \begin{center}
        \includegraphics[width = \textwidth]{images/scetch_impact_encoding.pdf}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Common Preprocessing Steps: Missing Values}
  Missing values:
  \begin{itemize}
    \item Additional factor column indicating missingness
    \item Replace missing values with \emph{out of range} or median/mode
    \item Advanced imputation strategies seldom advantageous (also because data mostly not missing at random)
  \end{itemize}
\end{frame}

\begin{frame}{Feature Selection}
    \begin{columns}
      \begin{column}{0.4\textwidth}
        Feature selection:
        \begin{itemize}
          \item Filter
          \item Stepwise selection methods: Needs to be applied on the whole pipeline (impractical!)
          \item Seldom increases performance but decreases computational costs $\rightarrow$ Multi-criteria optimization.
          \begin{itemize}
            \item Combined Feature Selection and HPO: \lit{\href{https://doi.org/10.1145/3377930.3389815}{Binder et al., 2020 GECCO}}
          \end{itemize}
          \item Happens indirectly in learning algorithm: random forest, lasso regression, \ldots %FIXME More examples.
        \end{itemize}
      \end{column}%
      \begin{column}{0.6\textwidth}
        \begin{center}
          \includegraphics[width=0.35\textwidth, trim=450 100 110 60, clip]{images/feat_extr_vs_selection.pdf}%
          \includegraphics[width=0.55\linewidth]{images/Binder2020multiobjective_fig3.pdf}

          {\tiny \hfill Taken from \href{https://doi.org/10.1145/3377930.3389815}{Binder et al., 2020 GECCO}.}
        \end{center}
      \end{column}
    \end{columns}
    
\end{frame}

\begin{frame}{Common Feature Construction Methods}
  \begin{columns}
    \begin{column}{0.6\textwidth}
    Reduction:
    \begin{itemize}
      \item PCA, ICA, autoencoder
    \end{itemize}

    Feature extraction:
    \begin{itemize}
      \item Polynomial Features: $x_j \longrightarrow x_j, x_j^2, x_j^3, ...$
      \item Interactions: $x_j, x_k \longrightarrow x_j, x_k, x_j \cdot x_k$
    \end{itemize}

    Feature generation:
    \begin{itemize}
      \item Transform to ``circular'' features (year, month, day) \\
      e.g.\ $\tilde x_1 = sin(2\pi \cdot x /24)$ and $\tilde x_2 = cos(2\pi \cdot x /24)$
    \end{itemize}
    
    Combine with external data:
    \begin{itemize}
      \item names $\longrightarrow$ gender, ethnicity, age
      \item home address $\longrightarrow$ household income
      \item location + date $\longrightarrow$ weather
    \end{itemize}

    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        \includegraphics[width= \textwidth, trim=0 100 390 60, clip]{images/feat_extr_vs_selection.pdf}
      \end{center}
    \end{column}
  \end{columns}
    
\end{frame}


\begin{frame}{Imbalanced Classes}
  Imbalanced classes:
  \begin{itemize}
    \item over-sampling of minority class
    \item seldom: under-sampling of majority class
    \item Influence of more advanced methods (e.g.\ SMOTE) on the predictive accuracy questionable.
  \end{itemize}
\end{frame}


\section{Combined Preprocessing and Model Building: Pipelining}

\begin{frame}{Pipelining}

  Most preprocessing steps have parameters or can be switched on/off in the pipeline.

  \vspace{1em}

  \textbf{Goal:} Find optimal preprocessing parameters $\rightarrow$ HPO

  \begin{columns}
    \begin{column}{0.5\textwidth}
    \begin{itemize}
      \item Most preprocessing methods have states similar to the model of an inducer.
      \item Applying preprocessing to the whole dataset leads to overfitting.
      \item Pipeline has to be optimized as a whole: $\pcs = \pcs_{\text{pipeline}} \times \pcs_\inducer$ within the resampling procedure.
    \end{itemize}
    \end{column}%
    \begin{column}{0.5\textwidth}
      \begin{center}
        \includegraphics[page=19, width=\textwidth, trim=20 60 30 35, clip]{images/mlr3Pipelines_graphics}
      \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Optimizing Pipelines}

  \begin{columns}
    \begin{column}{0.6\textwidth}
      \vspace*{-0.5em}
      \begin{itemize}
        \item Introducing different choices for preprocessing.
        \item Tuning over multiple learning algorithms.
        \item[$\rightarrow$] $\pcs$ becomes hierarchical search space!
      \end{itemize}
      
      \vspace*{0.5em}

      Suitable optimizers:
      \begin{itemize}
        \item Random Search, TPE, Hyperband
        \item BO with RF surrogate
        \item Evolutionary approaches (similar to NAS)
      \end{itemize}

    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        %\includegraphics[page=7, width=\textwidth, trim=160 0 30 160, clip]{images/mlr3Pipelines_graphics}
        %\includegraphics[width = \textwidth]{images/stacking.pdf}
        \includegraphics[width = \textwidth, trim=160 0 160 5, clip]{images/dag.pdf}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Obtaining Final Model}

      Options:
      \begin{itemize}
        \item Choose the optimal path as linear pipeline.
        \item Build ensemble of best configurations (e.g. \lit{\href{https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf}{H2O AutoML, LeDell et al., 2020}}).
      \end{itemize}
      \begin{center}
        \includegraphics[width = 0.66\textwidth]{images/stacking.pdf}
      \end{center}
    
\end{frame}

% \begin{frame}[containsverbatim,allowframebreaks]{Software}

% \begin{itemize}
%   \item DataRobot (comercial, gui)
%   \item H20.ai (comercial but open source, r, python)
%   \item TPOT, Tree-based Pipeline Optimization Tool  (2016-cont, open source, evolutionary approach) % show plot https://github.com/EpistasisLab/tpot
%   \item AutoWEKA (2016, open source)
%   \item mlr3automl (2020, prelim)
%   \item Hyperopt-Sklearn (2014-cont) Only HPO
%   \item Auto-Sklearn (2.0) (2015-cont) BO, ensembles, meta-learning
% \end{itemize}

% \end{frame}



\end{document}
