\input{t00_template.tex}
\subtitle{Wrap Up}


\begin{document}

\maketitle


%----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}{From HPO to AutoML}
  So far we covered
  \begin{itemize}
    \item Mechanisms to select promising ML Algorithms for a Dataset (Algorithm Selection)
    \item HPO as Black-Box optimization
    \begin{itemize}
      \item Grid- and Random Search, Evolutionary Algorithms, Bayesian Optimization
    \end{itemize}
    \item HPO as a Grey-Box-Problem
    \begin{itemize}
      \item Hyperband, BOHB
    \end{itemize}
    \item Optimizing Neural Network Architectures (NAS)
    \begin{itemize}
      \item One-Shot approaches, DART
    \end{itemize}
    \item Dynamic Algorithm Configuration (Learning to Learn)
    \begin{itemize}
      \item Adapt hyperparameter during training. 
    \end{itemize}
  \end{itemize}  
\end{frame}

\begin{frame}{From HPO to AutoML}
    \begin{center}
      \includegraphics[width = 0.9\linewidth]{images/drawing.pdf}  
    \end{center}
\end{frame}

\begin{frame}{Automate HPO}

  \begin{columns}
    \begin{column}{0.59\textwidth}

      For AutoML the user only supplies \ldots
      \begin{itemize}
        \item dataset
        \item performance measure and
        \item possibly a time limit.
      \end{itemize}

      So far HPO additionally needs \ldots
      \begin{itemize}
        \item one learning algorithm (to generate Inducer $\inducer$),
        \item search space $\pcs$ to chose $\conf$ from,
        \item a resampling strategy to evaluate $\cost(\conf)$ and
        \item optimization algorithm.
      \end{itemize}

      To build an AutoML System we have to make these choices automatically.

    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        \includegraphics[width = \linewidth]{images/tuning.pdf}    
      \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Choice of learning algorithm}
  \begin{itemize}
    \item A good AutoML System should consider more than one learning algorithm. More on that later.
    \item A plethora of learning algorithm exists.
    \item Studies\footnote{\href{https://dl.acm.org/doi/10.5555/2627435.2697065}{Delgado et al., JMLR 2014}} and experience have shown that one representative of these categories usually reaches best performance (on tabular data):
    \begin{itemize}
      \item Penalized Regression, SVM, Gradient Boosting, Random Forests, Neural Networks
      \item (tuned) random forests hardly beaten by current AutoML frameworks\footnote{\href{https://arxiv.org/abs/1907.00909}{Gijsbers et al., 2019}}.
      \item Example: Auto-Sklearn 2.0\footnote{\href{https://arxiv.org/abs/2007.04074}{Feurer et al., 2020}} uses: Extra Trees, Gradient Boosting, Passive Aggressive, Random Forest, Linear Model
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Choice of Search Space for a Learning Algorithm}
  \begin{columns}
    \begin{column}{0.6\textwidth}
    Which hyperparameters should we consider for a given learning algorithm?
    \begin{itemize}
      \item Ranges often selected based on experience
      \begin{itemize}
        \item Compare to other AutoML Frameworks: e.g.\ Auto-Sklearn 2.0~\lit{\href{https://arxiv.org/abs/2007.04074}{Feurer et al., 2020}} 
      \end{itemize}
      \item Sensitivity analysis does not exist for each learning algorithm
      \item Solution: Analysis of previous HPO runs and learn mapping $\datasets \rightarrow \mathcal{P}(\pcs)$ is risky (leaving out important ranges) and complicated.
      \item Instead: Use big search space $\pcs$ and try to predict good initial design (e.g.\ for Bayesian Optimization).
    \end{itemize}
    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        \only<1>{
          \includegraphics[width = 0.8\linewidth]{images/probst2019jmlr_tab1.pdf}
        }
        \only<2>{
          \includegraphics[width = 0.8\linewidth]{images/probst2019jmlr_tab3.pdf}   
        }

        {\tiny Taken from \href{https://www.jmlr.org/papers/volume20/18-444/18-444.pdf}{Probst et al., 2019 JMLR}.}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Choice of Resampling Strategy}
    \begin{itemize}
      \item Default: 10-fold CV
      \item Huge datasets: Holdout
      \item Tiny datasets: LOO
      \item For class imbalances:
      \begin{itemize}
        \item use stratification.
        \item ensure that validation split includes minority class.
      \end{itemize}
    \end{itemize}
    $\rightarrow$ Create heuristic or let the user decide.
\end{frame}

\begin{frame}{Choice of Optimization Algorithm}
  Choose optimization algorithm based on \ldots
  \begin{itemize}
    \item complexity of search space and
    \item estimated number of possible evaluations
  \end{itemize}

  \begin{itemize}
    \item Complex search space and many possible evaluations $\rightarrow$ Random Search, TPE, BO with RF as Surrogate
    \begin{itemize}
      \item Make use of Grey-Box Optimizers: Hyperband, BOHB
    \end{itemize}
    \item Simple search space and few possible Evaluations $\rightarrow$ BO with Kriging as Surrogate
    \begin{itemize}
      \item Grey-Box: BOHB
    \end{itemize}
    \item Complex search space and few possible evaluations $\rightarrow$ Use good defaults, Meta-Learning
    \item Deep Neural Network Architecture Search $\rightarrow$ NAS Algorithms and possibly HPO on found architecture
  \end{itemize}
\end{frame}



\begin{frame}[containsverbatim,allowframebreaks]{Preprocessing}
  \begin{center}
    \includegraphics[width = 0.5\linewidth]{images/AutoMLPipeline.jpg}  
  \end{center}

  Ideal ML Pipeline steps for AutoML Systems:
  \begin{itemize}
    \item Data Cleaning
    \item Feature Engineering
    \begin{itemize}
      \item Feature Selection
      \item Feature Preprocessing
      \item Feature Construction  
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Preprocessing not the strength of Non-commercial AutoML}
  \begin{columns}
    \begin{column}{0.6\textwidth}
      \vspace*{-1cm}
      \begin{center}
        \includegraphics[width = \linewidth]{images/Truong2019Towards_fig2.pdf}
      \end{center}
    \end{column}%
    \begin{column}{0.3\textwidth}
    \small
      Taken from \href{https://doi.org/10.1109/ICTAI.2019.00209}{Truong et al., 2019 ICTAI}.
      \vspace{1em}

      Highlighted: Non-commercial AutoML Frameworks
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Cleaning and Feature Selection}
    \begin{columns}
      \begin{column}{0.7\textwidth}

        Data Cleaning can hardly be automatized but a few heuristics exist:
        \begin{itemize}
          \item Remove ID Columns, Columns with mostly unique values
          \item Outlier detection (in the feature space)
          \item Detect time series or spatial data $\rightarrow$ randomized validation might be flawed.
        \end{itemize}

        Feature Selection
        \begin{itemize}
          \item Seldom increases performance but decreases computational costs $\rightarrow$ Multi-criteria optimization.
          \begin{itemize}
            \item Combined Feature Selection and HPO: \lit{\href{https://doi.org/10.1145/3377930.3389815}{Binder et al., 2020 GECCO}}
          \end{itemize}
          \item Happens indirectly in learning algorithm: random forest, lasso regression, \ldots %FIXME More examples.
        \end{itemize}
      \end{column}%
      \begin{column}{0.3\textwidth}
        \begin{center}
          \includegraphics[width = \linewidth]{images/Binder2020multiobjective_fig3.pdf}

          {\tiny Taken from \href{https://doi.org/10.1145/3377930.3389815}{Binder et al., 2020 GECCO}.}
        \end{center}
      \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Preprocessing}
  \begin{itemize}
    \item categorical values:
    \begin{itemize}
      \item Impact Encoding (aka Target Encoding) \\
      \vspace*{-0.5cm}  
        \begin{align*}
          \text{Regression:} \operatorname{Impact}(x) &= \E(\bm{y} | x) - \E(\bm{y}) \\
          \text{Classification:} \operatorname{Impact}(x) &= \operatorname{logit}(P( y = \text{target} | x)) - \operatorname{logit}(P( y = \text{target}))
        \end{align*}
        \vspace*{-0.5cm}  
        \begin{itemize}
          \item Needs regularization (through CV) to prevent target leakage \lit{\href{https://arxiv.org/abs/1611.09477}{Zumel et al., 2019}}
          \item Advantage: Handles unknown categorical levels on test data.
        \end{itemize}
    \end{itemize}
    \item missing values:
    \begin{itemize}
      \item Additional factor column indicating missigness
      \item Replace missing values with out of range or median/mode
      \item Advanced imutation strategies seldomly advantageous (also because data mostly not missing at random)
    \end{itemize}
    \item imbalanced classes:
    \begin{itemize}
      \item over-sampling of minority class
      \item seldomly: under-sampling of majority class
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Feature Construction}
  \begin{itemize}
    \item Generic:
    \begin{itemize}
      \item Polynomial Features
      \item Normalization (Box-Cox-Transformation)
      \item PCA
    \end{itemize}
    \item Heuristiscs:
    \begin{itemize}
      \item Detecting Dates, Hours: Transofrm to ``circular'' features within year, month, day, or whatever periodicity we assume \\
      e.g.\ $\tilde x_1 = sin(2\pi \cdot x /24)$ and $\tilde x_2 = cos(2\pi \cdot x /24)$
    \end{itemize}
    \item Combine with external data:
    \begin{itemize}
      \item names $\rightarrow$ gender, ethnicity, age
      \item home adress $\rightarrow$ household income
      \item location + date $\rightarrow$ weather
    \end{itemize}
  \end{itemize}
    
\end{frame}

\begin{frame}{Pipelining}

Most preprocessing steps have parameters or can be switched on/off in the pipeline.

\textbf{Goal:} Find optimal preprocessing parameters $\rightarrow$ HPO

\begin{columns}
  \begin{column}{0.5\textwidth}
  \begin{itemize}
    \item Most prepocessing methods have states similar to the model of an inducer.
    \item Applying preprocessing to the whole dataset leads to overfitting.
    \item Pipeline has to be optimized as a whole: $\pcs = \pcs_{\text{pipeline}} \times \pcs_\inducer$ within the resampling procedure.
  \end{itemize}
  \end{column}%
  \begin{column}{0.5\textwidth}
    \begin{center}
      \includegraphics[page=19, width=\textwidth, trim=20 60 30 35, clip]{images/mlr3Pipelines_graphics}
    \end{center}
  \end{column}
\end{columns}

\end{frame}

\begin{frame}{Optimizing Pipelines}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      \vspace*{-0.5em}
      \begin{itemize}
        \item Introducing different choices for preprocessing
        \item Tuning over multiple learning algorithms
      \end{itemize}
      $\rightarrow$ $\pcs$ becomes hierarchical search space!
      
      \vspace*{0.5em}

      Suitable optimizers:
      \begin{itemize}
        \item Random Search
        \item BO with RF surrogate
        \item Evolutionary approaches (similar to NAS)
      \end{itemize}

      Possible improvements:
      \begin{itemize}
        \item Instead of only keeping best setting: build ensemble of good configurations (e.g. \lit{\href{https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf}{H2O AutoML, LeDell et al., 2020}})
      \end{itemize}
    \end{column}%
    \begin{column}{0.5\textwidth}
      \begin{center}
        \includegraphics[page=7, width=\textwidth, trim=160 0 30 160, clip]{images/mlr3Pipelines_graphics}
      \end{center}
    \end{column}
  \end{columns}
    
\end{frame}

% \begin{frame}[containsverbatim,allowframebreaks]{Software}

% \begin{itemize}
%   \item DataRobot (comercial, gui)
%   \item H20.ai (comercial but open source, r, python)
%   \item TPOT, Tree-based Pipeline Optimization Tool  (2016-cont, open source, evolutionary approach) % show plot https://github.com/EpistasisLab/tpot
%   \item AutoWEKA (2016, open source)
%   \item mlr3automl (2020, prelim)
%   \item Hyperopt-Sklearn (2014-cont) Only HPO
%   \item Auto-Sklearn (2.0) (2015-cont) BO, ensembles, meta-learning
% \end{itemize}

% \end{frame}



\end{document}
