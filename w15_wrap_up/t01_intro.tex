\input{t00_template.tex}

\usepackage{ulem}
\usepackage{pifont}

\subtitle{Wrap Up}


\begin{document}

\maketitle


%----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}{From HPO to AutoML}
  So far we covered
  \begin{itemize}
    \item Mechanisms to select promising ML Algorithms for a Dataset (Algorithm Selection)
    \item HPO as Black-Box optimization
    \begin{itemize}
      \item Grid- and Random Search, Evolutionary Algorithms, Bayesian Optimization
    \end{itemize}
    \item HPO as a Grey-Box-Problem
    \begin{itemize}
      \item Hyperband, BOHB
    \end{itemize}
    \item Optimizing Neural Network Architectures (NAS)
    \begin{itemize}
      \item One-Shot approaches, DART
    \end{itemize}
    \item Dynamic Algorithm Configuration (Learning to Learn)
    \begin{itemize}
      \item Adapt hyperparameter during training. 
    \end{itemize}
  \end{itemize}  
\end{frame}

\begin{frame}{From HPO to AutoML}
    \begin{center}
      \includegraphics[width = 0.9\linewidth]{images/drawing.pdf}  
    \end{center}
\end{frame}

\section{The Missing Building Blocks}

\begin{frame}{What is missing?}
  \begin{columns}
    \begin{column}{0.5\textwidth}
        What do I need to know as an AutoML user?
        \begin{itemize}
          \item \sout{Nothing, because it is automatic.}
          \item Understand limitations of the framework you are using.
          \item Know how to interpret the results.
          \item Possibly, how to preprocess the data.
        \end{itemize}

        \vspace{1em}

        What do I need to implement an AutoML framework?
        \begin{itemize}
          \item HPO Algorithm
          \item ML Algorithm (= Design Space)
          \item Resampling
          \item (Preprocessing)
        \end{itemize}
    \end{column}%
    \begin{column}{0.5\textwidth}
      \begin{center}
        Academic view:
        \scalebox{0.45}{
          \input{tikz/automl_overview.tex}
        }

        \vspace{1em}

        Practitioners view:
        \scalebox{0.45}{
          \input{tikz/true_automl_overview.tex}
        }
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Automate HPO}

  \begin{columns}
    \begin{column}{0.59\textwidth}

      For AutoML the user only supplies \ldots
      \begin{itemize}
        \item dataset,
        \item performance measure and
        \item possibly a time limit.
      \end{itemize}

      To do HPO we need to \ldots
      \begin{itemize}
        \item preprocessing manually,
        \item decide on an optimization algorithm,
        \item an ML algorithm (to generate Inducer $\inducer$),
        \item a search space $\pcs$ and
        \item a resampling strategy to evaluate $\cost(\conf)$.
      \end{itemize}

      To build an AutoML System we have to make these choices automatically $\rightarrow$ Following slides.

    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        \includegraphics[width = \linewidth]{images/tuning.pdf}    
      \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Choice of Learning Algorithm}
  \begin{itemize}
    \item A good AutoML System should consider more than one learning algorithm. More on that later.
    \item A plethora of learning algorithm exists.
    \item Studies\footnote{\href{https://dl.acm.org/doi/10.5555/2627435.2697065}{Delgado et al., JMLR 2014}} and experience have shown that one representative of these categories usually reaches best performance (on tabular data):
    \begin{itemize}
      \item Penalized Regression, SVM, Gradient Boosting, Random Forests, Neural Networks
      \item (tuned) random forests only beaten on few datasets by current AutoML frameworks\footnote{\href{https://arxiv.org/abs/1907.00909}{Gijsbers et al., 2019}}.
      \item Example: Auto-Sklearn 2.0\footnote{\href{https://arxiv.org/abs/2007.04074}{Feurer et al., 2020}} uses: Extra Trees, Gradient Boosting, Passive Aggressive, Random Forest, Linear Model
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Choice of Search Space for a Learning Algorithm}
  \begin{columns}
    \begin{column}{0.6\textwidth}
    Which hyperparameters should we consider for a given learning algorithm?
    \begin{itemize}
      \item Ranges often selected based on experience
      \begin{itemize}
        \item Compare to other AutoML Frameworks: e.g.\ Auto-Sklearn 2.0~\lit{\href{https://arxiv.org/abs/2007.04074}{Feurer et al., 2020}} 
      \end{itemize}
      \item Sensitivity analysis does not exist for each learning algorithm
      \item Solution: Analysis of previous HPO runs and learn mapping $\datasets \rightarrow \mathcal{P}(\pcs)$ is risky (leaving out important ranges) and complicated.
      \item Instead: Use big search space $\pcs$ and try to predict good initial design (e.g.\ for Bayesian Optimization).
    \end{itemize}
    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        \only<1>{
          \includegraphics[width = 0.8\linewidth]{images/probst2019jmlr_tab1.pdf}
        }
        \only<2>{
          \includegraphics[width = 0.8\linewidth]{images/probst2019jmlr_tab3.pdf}   
        }

        {\tiny Taken from \href{https://www.jmlr.org/papers/volume20/18-444/18-444.pdf}{Probst et al., 2019 JMLR}.}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Choice of Resampling Strategy}
  For the calculation of the generalization error / cost
  \begin{equation}
    \cost(\conf) = \frac{1}{R}\sum_{i = 1}^R \widehat{GE}_{\dataset_{\text{val}, i}}\left(\inducer(\dataset_{\text{train}, i}, \conf)\right)
  \end{equation}
  that defines the objective of the black-box optimization we need a resampling strategy.

  \vspace{1em}

  A heuristic to select a suitable resampling strategy may follow this rule of thumb:
    \begin{itemize}
      \item Default: 10-fold CV ($R=10$)
      \item Huge datasets: Holdout
      \item Tiny datasets: LOO
      \item For class imbalances:
      \begin{itemize}
        \item use stratification,
        \item ensure that validation split includes minority class.
      \end{itemize}
    \end{itemize}
    Or let the user decide.
\end{frame}

\begin{frame}{Choice of Optimization Algorithm}
  Choose optimization algorithm based on \ldots
  \begin{itemize}
    \item complexity of search space and
    \item estimated number of possible evaluations
  \end{itemize}

  \vspace{0.5em}

  Complex search space and many possible evaluations 
  \begin{itemize}
    \item[$\rightarrow$] Random Search, TPE, BO with RF as Surrogate
    \item[$\rightarrow$] Make use of Grey-Box Optimizers: Hyperband, BOHB
  \end{itemize}
  Simple search space and few possible Evaluations 
  \begin{itemize}
    \item[$\rightarrow$] BO with Kriging as surrogate
    \item[$\rightarrow$] Grey-Box: BOHB
  \end{itemize}
  Complex search space and few possible evaluations
  \begin{itemize}
    \item[$\rightarrow$]Use good defaults, Meta-Learning
  \end{itemize}
  Deep Neural Network Architecture Search 
  \begin{itemize}
    \item[$\rightarrow$]NAS Algorithms and possibly HPO on found architecture
  \end{itemize}

\end{frame}



\begin{frame}{Preprocessing}
  \begin{columns}
    \begin{column}{0.5\textwidth} 
      Ideal AutoML system would automatically optimize the following steps wrt.\ given cost function:
      \begin{itemize}
        \item[\ding{55}] Data cleaning
        \item[\ding{55}] Feature engineering
        \begin{itemize}
          \item Preprocessing
          \item Feature selection
          \item Feature construction  
        \end{itemize}
        \item[\ding{51}] Model training
      \end{itemize}
    We already know how to optimize the ml algorithm. How about the preprocessing steps?
    \end{column}%
    \begin{column}{0.5\textwidth}
      \begin{center}
        \includegraphics[width = \linewidth]{images/AutoMLPipeline.jpg}  
      \end{center}
    \end{column}
  \end{columns}
  
\end{frame}

\section{Common Preprocessing Steps}

\begin{frame}{Preprocessing not the strength of Non-commercial AutoML}
  \begin{columns}
    \begin{column}{0.6\textwidth}
      \vspace*{-1cm}
      \begin{center}
        \includegraphics[width = \linewidth]{images/Truong2019Towards_fig2.pdf}
      \end{center}
    \end{column}%
    \begin{column}{0.3\textwidth}
    \small
      Taken from \href{https://doi.org/10.1109/ICTAI.2019.00209}{Truong et al., 2019 ICTAI}.
      \vspace{1em}

      Highlighted: Non-commercial AutoML Frameworks
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Cleaning}
  Data cleaning can hardly be automatized but a few heuristics exist:
  \begin{itemize}
    \item Remove ID Columns, Columns with mostly unique values
    \item Outlier detection (in the feature space)
    \item Detect time series or spatial data $\rightarrow$ randomized validation might be flawed.
  \end{itemize}
\end{frame}

\begin{frame}{Categorical Features}
  Goals:
  \begin{itemize}
    \item Convert all categorical features
    \item Avoid high cardinality categorical features
  \end{itemize}
  ML algorithm does not support categorical features + few unique values:
  \begin{itemize}
    \item Use dummy encoding
  \end{itemize}
  High cardinality categorical features problematic for all ML algorithms:
  \begin{itemize}
    \item Use Impact Encoding (also Target Encoding) \\
    \vspace*{-0.5cm}  
    \begin{align*}
    \text{Regression:} \operatorname{Impact}(x) &= \E(\bm{y} | x) - \E(\bm{y}) \\
    \text{Classification:} \operatorname{Impact}(x) &= \operatorname{logit}(P( y = \text{target} | x)) - \operatorname{logit}(P( y = \text{target}))
    \end{align*}
    \vspace*{-0.5cm}  
    \begin{itemize}
      \item Needs regularization (through CV) to prevent target leakage \lit{\href{https://arxiv.org/abs/1611.09477}{Zumel et al., 2019}}
      \item Advantage: Handles unknown categorical levels on test data.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Common Preprocessing Steps: Missing Values}
  Missing values:
  \begin{itemize}
    \item Additional factor column indicating missingness
    \item Replace missing values with out of range or median/mode
    \item Advanced imputation strategies seldom advantageous (also because data mostly not missing at random)
  \end{itemize}
\end{frame}

\begin{frame}{Feature Selection}
    \begin{columns}
      \begin{column}{0.4\textwidth}
        Feature selection:
        \begin{itemize}
          \item Filter
          \item Stepwise selection methods: Needs to be applied on the whole pipeline (impractical!)
          \item Seldom increases performance but decreases computational costs $\rightarrow$ Multi-criteria optimization.
          \begin{itemize}
            \item Combined Feature Selection and HPO: \lit{\href{https://doi.org/10.1145/3377930.3389815}{Binder et al., 2020 GECCO}}
          \end{itemize}
          \item Happens indirectly in learning algorithm: random forest, lasso regression, \ldots %FIXME More examples.
        \end{itemize}
      \end{column}%
      \begin{column}{0.6\textwidth}
        \begin{center}
          \includegraphics[width=0.35\textwidth, trim=450 100 110 60, clip]{images/feat_extr_vs_selection.pdf}%
          \includegraphics[width=0.55\linewidth]{images/Binder2020multiobjective_fig3.pdf}

          {\tiny \hfill Taken from \href{https://doi.org/10.1145/3377930.3389815}{Binder et al., 2020 GECCO}.}
        \end{center}
      \end{column}
    \end{columns}
    
\end{frame}

\begin{frame}{Common Feature Construction Methods}
  \begin{columns}
    \begin{column}{0.6\textwidth}
    Reduction:
    \begin{itemize}
      \item PCA, ICA, autoencoder
    \end{itemize}

    Feature extraction:
    \begin{itemize}
      \item Polynomial Features: $x_j \longrightarrow x_j, x_j^2, x_j^3, ...$
      \item Interactions: $x_j, x_k \longrightarrow x_j, x_k, x_j \cdot x_k$
    \end{itemize}

    Feature generation:
    \begin{itemize}
      \item Transform to ``circular'' features (year, month, day) \\
      e.g.\ $\tilde x_1 = sin(2\pi \cdot x /24)$ and $\tilde x_2 = cos(2\pi \cdot x /24)$
    \end{itemize}
    
    Combine with external data:
    \begin{itemize}
      \item names $\longrightarrow$ gender, ethnicity, age
      \item home address $\longrightarrow$ household income
      \item location + date $\longrightarrow$ weather
    \end{itemize}

    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        \includegraphics[width= \textwidth, trim=0 100 390 60, clip]{images/feat_extr_vs_selection.pdf}
      \end{center}
    \end{column}
  \end{columns}
    
\end{frame}


\begin{frame}{Imbalanced Classes}
  Imbalanced classes:
  \begin{itemize}
    \item over-sampling of minority class
    \item seldom: under-sampling of majority class
    \item Influence of more advanced methods (e.g.\ SMOTE) on the predictive accuracy questionable.
  \end{itemize}
\end{frame}


\section{Combined Preprocessing and Model Building: Pipelining}

\begin{frame}{Pipelining}

  Most preprocessing steps have parameters or can be switched on/off in the pipeline.

  \vspace{1em}

  \textbf{Goal:} Find optimal preprocessing parameters $\rightarrow$ HPO

  \begin{columns}
    \begin{column}{0.5\textwidth}
    \begin{itemize}
      \item Most preprocessing methods have states similar to the model of an inducer.
      \item Applying preprocessing to the whole dataset leads to overfitting.
      \item Pipeline has to be optimized as a whole: $\pcs = \pcs_{\text{pipeline}} \times \pcs_\inducer$ within the resampling procedure.
    \end{itemize}
    \end{column}%
    \begin{column}{0.5\textwidth}
      \begin{center}
        \includegraphics[page=19, width=\textwidth, trim=20 60 30 35, clip]{images/mlr3Pipelines_graphics}
      \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{Optimizing Pipelines}

  \begin{columns}
    \begin{column}{0.6\textwidth}
      \vspace*{-0.5em}
      \begin{itemize}
        \item Introducing different choices for preprocessing.
        \item Tuning over multiple learning algorithms.
        \item[$\rightarrow$] $\pcs$ becomes hierarchical search space!
      \end{itemize}
      
      \vspace*{0.5em}

      Suitable optimizers:
      \begin{itemize}
        \item Random Search, TPE, Hyperband
        \item BO with RF surrogate
        \item Evolutionary approaches (similar to NAS)
      \end{itemize}

    \end{column}%
    \begin{column}{0.4\textwidth}
      \begin{center}
        %\includegraphics[page=7, width=\textwidth, trim=160 0 30 160, clip]{images/mlr3Pipelines_graphics}
        %\includegraphics[width = \textwidth]{images/stacking.pdf}
        \includegraphics[width = \textwidth, trim=160 0 160 5, clip]{images/dag.pdf}
      \end{center}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Obtaining Final Model}

      Options:
      \begin{itemize}
        \item Choose the optimal path as linear pipeline.
        \item Build ensemble of best configurations (e.g. \lit{\href{https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf}{H2O AutoML, LeDell et al., 2020}}).
      \end{itemize}
      \begin{center}
        \includegraphics[width = 0.66\textwidth]{images/stacking.pdf}
      \end{center}
    
\end{frame}

% \begin{frame}[containsverbatim,allowframebreaks]{Software}

% \begin{itemize}
%   \item DataRobot (comercial, gui)
%   \item H20.ai (comercial but open source, r, python)
%   \item TPOT, Tree-based Pipeline Optimization Tool  (2016-cont, open source, evolutionary approach) % show plot https://github.com/EpistasisLab/tpot
%   \item AutoWEKA (2016, open source)
%   \item mlr3automl (2020, prelim)
%   \item Hyperopt-Sklearn (2014-cont) Only HPO
%   \item Auto-Sklearn (2.0) (2015-cont) BO, ensembles, meta-learning
% \end{itemize}

% \end{frame}



\end{document}
