\videotitle{DARTS: Differentiable Architecture Search}


%----------------------------------------------------
\myframetop{DARTS: Differentiable Architecture Search \litw{\href{https://openreview.net/forum?id=S1eYHoC5FX}{Liu et al, 2018}}}{
	\myit{
		\item Use one-shot model with continuous architecture weight $\alpha$ for each operator
\onslide<2->{
        \myit{
			\item[] $x^{(j)} = \sum_{i<j}\tilde{o}^{(i,j)}(x^{(i)}) = \sum_{i<j}\sum_{o\in\mathcal{O}}
%			\text{softmax}(\alpha)
			\frac{exp(\alpha_{o}^{(i,j)})}{\sum_{o^{\prime}\in\mathcal{O}}exp(\alpha_{o^{\prime}}^{(i,j)})}o(x^{(i)})$
%            \item[-] $o^{(i,j)} \in \argmax_{o\in\mathcal{O}}\alpha_{o}^{(i,j)}$
		}
}

		\begin{columns}
			\column{0.05\textwidth}
			\column{0.3\textwidth}
			    \centering
			    {\includegraphics[scale=0.7]{images/dartsA.pdf}}\\
			    {\scriptsize (a) Initialization}
			\column{0.3\textwidth}
\onslide<1->{
			    \centering
			    {\includegraphics[scale=0.7]{images/dartsB.pdf}}\\
			    {\scriptsize (b) Search end}
}
			\column{0.3\textwidth}
\onslide<4->{
			    \centering
			    {\includegraphics[scale=0.7]{images/dartsC.pdf}}\\
			    {\scriptsize (c) Final cell}
}
			\column{0.05\textwidth}
        \end{columns}
			
		\medskip
		\pause
\onslide<3->{
		\item By optimizing the architecture weights $\alpha$, DARTS assigns importance to each operation
		\myit{
			\item Since the $\alpha$ are continuous, we can optimize them with gradient descent
		}
}		
\medskip	
\pause
	
\vspace*{-0.2cm}
\onslide<4->{
	\item In the end, DARTS discretizes to obtain a single architecture (c)
}
	
	}
}

%----------------------------------------------------------------------

%----------------------------------------------------

\myframe{DARTS: Architecture Optimization}{

	\myit{
		\item The optimization problem (a $\rightarrow$ b) is a bi-level optimization problem:
		\begin{center}
		\footnotesize{
					\begin{tcolorbox}[width = 8cm,halign=center,valign=center]
		%		\begin{eqnarray}
					%\nonumber			
					$\text{min}_{\alpha} \mathcal{L}_{\text{\alert{val}}}(w^*(\alpha), \alert{\alpha})$\\
					%\nonumber			
					$s.t.\ w^*(\alpha) \in \text{argmin}_{w} \mathcal{L}_{\text{\alert{train}}}(\alert{w},\alpha)$
		%		\end{eqnarray}
			\end{tcolorbox}
		}
		\end{center}	
		\medskip
		\pause
		\item This is solved using alternating SGD steps on architectural parameters $\alpha$ and weights $w$ 
	}
	
	\begin{algorithm}[H]
	 %\footnotesize%
	 %create mixed ops $\tilde{o}$ parametrized by $\alpha_{o}^{(i,j)}$ for each edge $(i,j)$\\
	 \TitleOfAlgo{DARTS 1st order\vspace*{-0.3cm}\\\rule{\textwidth}{0.5pt}}

	 \While{\textit{not converged}}{
	  Update one-shot weights $\vec{w}$ by $\nabla_{\vec{w}}\mathcal{L}_{train}(\vec{w}, \vec{\alpha})$\\
	  Update architectural parameters $\alpha$ by $\nabla_{\alpha}\mathcal{L}_{valid}(\vec{w}, \vec{\alpha})$\;
	 }
	 return $\argmax_{o\in\mathcal{O}}\alpha_{o}^{(i,j)}$ for each edge $(i,j)$\;
	\end{algorithm}
	
	\medskip
	\pause
	\myit{
		\item Note: there is no theory showing that this process converges
	}

}
%----------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myframetop{Strong performance on some benchmarks}{

\begin{itemize}
    \item E.g., original CNN search space
    \myit{
    	\item 8 operations on each MixedOp
    	\item 28 MixedOps in total
    	\item $> 10^{23}$ possible architectures
    }
    \item Performance
    \myit{
    	\item $<3\%$ error on CIFAR-10 in less than 1 GPU day of search
	}
\end{itemize}

\vspace{1cm}
\centering
\includegraphics[scale=0.35]{images/darts_normal_cell.png}
\hspace{1cm}
\includegraphics[scale=0.35]{images/darts_reduction_cell.png}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myframetop{Issues -- Non-robust behaviour}{

\myit{
	\item DARTS is very sensitive w.r.t. its own hyperparameters (e.g. one-shot learning rate, $L_2$ regularization, etc.)
    \myit{
    	\visible<2->{
    	\item[-] Tuning these hyperparameters for every new task/search space is computationally expensive
    	}
    	\visible<3->{
    	\item[-] DARTS may return degenerate architectures, e.g., cells composed with only skip connections
    	}
    }
}

%\medskip
\begin{center}
	\only<3>{
		\includegraphics[scale=0.3]{images/normal_DARTS_S3.pdf}
	}
\end{center}
\begin{minipage}{.45\textwidth}
\small
\myit{
\visible<4->{
	\item \alert{RobustDARTS} \lit{\href{https://openreview.net/forum?id=H1gDNyrKDS}{Zela et al, 2020}} -- tracks the curvature of the validation loss and stops the search early based on that
	}
}
\end{minipage}
%\hspace{.5cm}
\begin{minipage}{.45\textwidth}
\myit{
\visible<5->{
\small
	\item \alert{SmoothDARTS} \lit{\href{https://arxiv.org/pdf/2002.05283.pdf}{Chen and Hsieh, 2020}} -- applies random perturbation and adversarial training to avoid bad regions
	}
	}
\end{minipage}

\medskip

\centering
\visible<4->{
	\includegraphics[scale=0.28]{images/valid_eigenvalues_traj.pdf}
}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myframetop{Issues -- Memory constraints}{

\vspace*{-0.15cm}
	\myit{
		\item DARTS keeps the \alert{entire one-shot model in memory}, together with its computed tensors 
    	\visible<2->{
	    	\myit{
	    	   	\item[-] This constrains the search space size and the fidelity used to train the one-shot model
		    	\item[-] \alert{Impossible} to run on large datasets as ImageNet
	    	}
		}
	}

\bigskip

\begin{minipage}{.55\textwidth}
\visible<3->{A lot of research aims to fix this issue:}

\myit{
\visible<4->{
	\item \alert{GDAS} \lit{\href{https://arxiv.org/pdf/1910.04465.pdf}{Dong et al, 2019}} -- samples from a Gumbel Softmax distribution to keep only a single architecture in memory
	}
\smallskip
\visible<5->{
	\item \alert{ProxylessNAS} \lit{\href{https://openreview.net/pdf?id=HylVB3AqYm}{Cai et al, 2019}} -- computes approximate gradients on $\alpha$ keeping only 2 edges between two nodes in memory at a time 
	}
\smallskip
\visible<6->{
	\item \alert{PC-DARTS} \lit{\href{https://arxiv.org/pdf/1907.05737.pdf}{Xu et al, 2020}} -- performs the search on a subset of the 
	%total computed 
	channels in the one-shot model
	}
}
\end{minipage}
\hspace{.5cm}
\begin{minipage}{.2\textwidth}
\visible<4->{
\includegraphics[scale=0.3]{images/gdas.png}
}
\end{minipage}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------
\myframe{Questions to Answer for Yourself / Discuss with Friends}{

	\myit{
		\item Repetition:\\ \alert{What is the main difference between DARTS and the other one-shot NAS methods we saw before?}
\bigskip
		\item Repetition:\\ \alert{How does DARTS optimize the architectural weights and one-shot weights?}
\bigskip
		\item Repetition:\\ \alert{What are DARTS' main issues and how can they be fixed?}
\medskip
		\item Discussion:\\ \alert{RobustDARTS stops the architecture optimization early, before the curvature of the validation loss becomes high. Why do you think this works? 
		\\{[Hint: think about the discretization step after the DARTS search.]}}
	}	 
}
%-----------------------------------------------------------------------
