%\videotitle{One-shot Neural Architecture Search}

%-------------------------------------------------
%-------------------------------------------------

%----------------------------------------------------------------------
\myframe{Convolutional Neural Fabrics \litw{\href{https://arxiv.org/pdf/1606.02492.pdf}{Saxena and Verbeek, 2017}}}{
	\centering
	\includegraphics[width=0.7\textwidth]{images/conv_fabric.png}
	
	\begin{itemize}
	\footnotesize
		\item One path from the input to the output of the lattice determines the network structure.
		\item Paths (architectures) that overlap also share the parameters.
	\end{itemize}
	
}
%----------------------------------------------------------------------

%-----------------------------------------------------------------------
\myframetop{Weight sharing and one-shot models}{

\myit{
	\item All possible architectures are subgraphs of a large supergraph (the \alert{one-shot model})
	\item This one-shot model is \alert{trained as a standard neural network} with mini-batches and stochastic optimizers.
	\item At each mini-batch iteration \alert{weights are shared} between different architectures with common edges/nodes in the supergraph
	}
	
	\centering
	\includegraphics[width=0.2\textwidth]{images/one_shot_model_1.png}\quad\quad
	\includegraphics[width=0.2\textwidth]{images/one_shot_model_2.png}\quad\quad
	\includegraphics[width=0.2\textwidth]{images/one_shot_model_3.png}

}
%----------------------------------------------------------------------

%-----------------------------------------------------------------------
%\myframetop{Basic Principle}{
%	\centering
%	\includegraphics[width=0.7\textwidth]{images/snas_oneshot.png}
%	
%	\only<1>{
%	\begin{itemize}
%	\footnotesize
%		\item The \textbf{one-shot model} is a multi-graph containing all possible DAGs
%		\myit{
%		\footnotesize
%			\item[-] Every DAG represents a single architecture $Z^{(\cdot)}$ in the search space $\mathcal{A}$.
%			\item[-] Nodes represent aggregating operations (e.g. summation, concatenation) for incoming tensors.
%			\item[-] Edges represent operations $O^i$ (in the figure: one color per operation)
%			}
%		\item The row labels in the matrix above represent a pair of nodes $(j,k)$ in the graph and the column labels the operations $O^i$. A value of $1$ means that that operaration is active in the edge connecting node $j$ to $k$.
%	\end{itemize}
%	}
%	
%	\only<2>{
%	\begin{itemize}
%	\footnotesize
%		\item The most important principle in one-shot models is \textbf{weight-sharing} between graphs.
%		\myit{
%		\footnotesize
%			\item[-] The one-shot model is trained as a normal neural network, i.e. with mini-batch training. The question is how to distinguish single architectures in the one-shot model during this training?
%			\item[-] One way is that for each sampled mini-batch also sample stochastically an architecture (DAG) and update only the parameters of that architecture.
%			\item[-] For all subsequent iterations in case a new sampled architecture has common edges (i.e. some entries in the matrices are the same) in the DAG, the weights are shared.
%			}
%	\end{itemize}
%	}
%}
%----------------------------------------------------------------------

%----------------------------------------------------------------------

\myframetop{Impact of DropPath \litw{\href{http://proceedings.mlr.press/v80/bender18a/bender18a.pdf}{Bender et al., 2018}}}{
	\centering
	
	\includegraphics[width=0.9\textwidth]{images/bender_1.png}
	
	\begin{itemize}
	%\footnotesize
		\item One other way to distinguish the single architectures in the one-shot model is as follows:
		\begin{enumerate}
			\item Train the one-shot model as a normal network without sampling any individual path (a matrix with only ones "Basic Principle" slide).
			\item Sample $K$ individual architectures after training the one-shot model and evaluate those on the validation set with the one-shot model weight.
			\item Choose the best on validation and re-train that from scratch and return the test error.
		\end{enumerate}
	\end{itemize}
	
}
%----------------------------------------------------------------------

%----------------------------------------------------------------------

\myframetop{Impact of DropPath \litw{\href{http://proceedings.mlr.press/v80/bender18a/bender18a.pdf}{Bender et al., 2018}}}{
	\centering
	
	\includegraphics[width=0.8\textwidth]{images/droppath.png}
	
    \begin{itemize}
		\item DropPath zeros out one a subset of the operations at each mini-batch training iteration with probability $p$.
		\item ScheduledDropPath starts with $p=0$ and increases that linearly throughout training until a maximum $p_{max}$ in the end.
	\end{itemize}


}
%---------------------------------------------------------------

%----------------------------------------------------------------------

\myframe{Impact of DropPath \litw{\href{http://proceedings.mlr.press/v80/bender18a/bender18a.pdf}{Bender et al., 2018}}}{
	\centering
	
    	\begin{minipage}{0.4\textwidth}
        	\begin{itemize}
				%\footnotesize
				\item If the DropPath rate is properly tuned the correlation between architectures evaluated with the one-shot weights and retrained from scratch (stand-alone models) is high.
				\item This implies that \textbf{selecting the best architecture based on the one-shot weights} is not sub-optimal.
			\end{itemize}
	    \end{minipage}
	    \hspace{1cm}
    	\begin{minipage}{0.5\textwidth}
	        \includegraphics[width=.8\textwidth]{images/bender_correlation.png}    	
    	\end{minipage}

}
%---------------------------------------------------------------

%----------------------------------------------------------------------

\myframetop{Random Search with Weight Sharing \litw{\href{https://arxiv.org/pdf/1902.07638.pdf}{Li and Talwalkar, 2020}}}{
	\centering
	
    \begin{itemize}
		\item Random Search with Weight Sharing \textbf{utilizes the one-shot model to speed up vanilla random search} as follows:
		\myit{
			\only<1>{
			\item[-] At each mini-batch iteration during the training of the one-shot model \alert{sample uniformly at random} one architecture $Z$ from the search space $\mathcal{A}$.
			\item[-] \alert{Update the parameters of the one-shot model} corresponding to only that architecture.
			\item[-] After training the one-shot model finishes, sample uniformly at random $M$ architectures and rank them based on the error on a single mini-batch from the validation set \alert{using the one-shot model parameters} (retraining from scratch is computationaly expensive).
			\item[-] Select the top $K$, where $K < M$, and evaluate those on the full validation set, again using the one-shot parameters.
			\item[-] Return the top performing architecture to \alert{retrain from scratch}.
			}
			\only<2>{
			\item[-] Works \alert{comparably to state-of-the-art NAS methods} on many benchmarks.
			}
		}
	\end{itemize}
	
	\only<2>{
	\includegraphics[width=.75\textwidth]{images/rs_ws.png}
	}
}
%---------------------------------------------------------------


%----------------------------------------------------------------------

\myframetop{Efficient Neural Architecture Search \litw{\href{https://arxiv.org/pdf/1802.03268.pdf}{Pham et al., 2018}}}{
	\centering
	
    \begin{itemize}
		\item Random Search with Weight Sharing \textbf{utilizes the one-shot model to speed up vanilla random search} as follows:
		\myit{
			\only<1>{
			\item[-] At each mini-batch iteration during the training of the one-shot model \alert{sample uniformly at random} one architecture $Z$ from the search space $\mathcal{A}$.
			\item[-] \alert{Update the parameters of the one-shot model} corresponding to only that architecture.
			\item[-] After training the one-shot model finishes, sample uniformly at random $M$ architectures and rank them based on the error on a single mini-batch from the validation set \alert{using the one-shot model parameters} (retraining from scratch is computationaly expensive).
			\item[-] Select the top $K$, where $K < M$, and evaluate those on the full validation set, again using the one-shot parameters.
			\item[-] Return the top performing architecture to \alert{retrain from scratch}.
			}
			\only<2>{
			\item[-] Works \alert{comparably to state-of-the-art NAS methods} on many benchmarks.
			}
		}
	\end{itemize}
	
	\only<2>{
	\includegraphics[width=.75\textwidth]{images/rs_ws.png}
	}
}
%---------------------------------------------------------------

%----------------------------------------------------------------------
\myframe{Questions to Answer for Yourself / Discuss with Friends}{

	\myit{
		\item Repetition:\\ \alert{What are some pros and cons of the cell search space compared to the basic one?}
\bigskip
		\item Repetition:\\ \alert{Explain the way in which level-3 motivs in the hierarchical search space use level-2 motivs.}
\medskip
		\item Repetition:\\ \alert{What are some pros and cons of the hierarchical search space compared to the other ones?}
	}	 
}
%-----------------------------------------------------------------------

