
\input{../latex_main/main.tex}

%\newcommand{\a}[0]{\mathbf{a}}
%\newcommand{\y}[0]{\mathbf{y}}
\newcommand{\q}[0]{\mathbf{q}}
\newcommand{\Xspace}[0]{\mathcal{X}}

\title[AutoML: Overview]{Multi-criteria Optimization}
\subtitle{Bayesian Optimization}
%TODO: change authors!
\author[Bernd Bischl]{Bernd Bischl \and Frank Hutter \and Lars Kotthoff \and \underline{Marius Lindauer}}
\institute{}
\date{}



% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}

	\maketitle



\begin{frame}[allowframebreaks]{Recap: Bayesian Optimization}

Remember the advantages of Bayesian optimization:

\begin{block}{Advantages}
\begin{itemize}
  \item Sample efficient
  \item Can handle noise
  \item Native incorporation of priors
  \item Does not require gradients
  \item Theoretical guarantees
\end{itemize}
\end{block}

To achieve all these advantages for multi-criteria problems, we want to extend Bayesian optimization to multiple cost functions.

\framebreak

\begin{center}
\begin{minipage}{0.75\textwidth}
\begin{algorithm}[H]
    %\DontPrintSemicolon
%    \SetAlgoLined
    \setcounter{AlgoLine}{0}
    \SetKwInOut{Require}{Require}
    \SetKwInOut{Result}{Result}

    \Require{Search space $\pcs$,
    		cost function $\cost$,
    		acquisition function $\acq$, predictive model $\surro$,
    		maximal number of function evaluations $\bobudget$}
    \Result{Best configuration $\finconf$
    (according to $\dataset$ or
    $\surro$)}

	Initialize data $\iter[0]{\dataset}$ with initial observations\;% \leftarrow \varnothing$\;

    \For{$\bocount=1$ \KwTo $\bobudget$}{
		%\While{$B$ not exhausted} {
		Fit predictive model $\iter[\bocount]{\surro}$ on $\iter[\bocount-1]{\dataset}$\;

		Select next query point: $\bonextsample \in \argmax_{\conf \in \pcs} \acq(\conf; \iter[\bocount-1]{\dataset}, \iter[\bocount]{\surro})$\;

		Query $\bonextobs$\;

		Update data: $\iter[\bocount]{\dataset} \leftarrow \iter[\bocount-1]{\dataset} \cup \{\langle \bonextsample, \bonextobs \rangle \}$\;
	}
	\caption*{BO loop}
\end{algorithm}
\end{minipage}
\end{center}
\end{frame}


\begin{frame}{Multi-Criteria Bayesian Optimization}

\textbf{Goal}: Extend Bayesian optimization to multiple cost functions

$$
\min_{\conf \in \pcs}  \cost(\conf) \Leftrightarrow \min_{\conf \in \pcs} \left(\cost_1(\conf), \cost_2(\conf), ..., \cost_m(\conf)\right).
$$



There are four general approaches:

\begin{enumerate}
        \item Scalarization of cost functions.
        \item Multi-criteria acquisition functions.
        \item Multi-criteria optimization of multiple single-criteria acquisition functions.
        \item Multi-criteria surrogate models.
\end{enumerate}

\end{frame}

\begin{frame}{Scalarization}

    \textbf{Idea:} Aggregate all cost functions


    $$\min_{\conf \in \pcs} \sum_{i = 1}^m w_i \cost_i(\conf) \qquad \text{with} \quad w_i \ge 0 $$

    \begin{itemize}
        \item \textbf{Obvious problem:} How to define $w_1, \dots, w_m$?
            \begin{itemize}
                \item Expert knowledge?
                \item Systematic variation?
                \item Random variation?
            \end{itemize}
        \item If expert knowledge is not available a-priori, we need to ensured that different trade-offs between the cost functions are explored.
            \item Simplifies multi-criteria optimization problem to single-criteria \\$\longrightarrow$ Bayesian optimization can be used without adaptions of the general algorithm.
    \end{itemize}

\end{frame}

\begin{frame}{Scalarization: ParEGO}

    Scalarize the cost functions using the augmented Tchebycheff norm

    $$
    c = \max_{i=1,\dots,m}\left(w_i \cost_i(\conf)\right) + \rho \sum_{i=1}^m w_i\cost_i(\conf),
    $$

    \begin{itemize}
        \item The weights $w \in W$ are drawn from
            $$
                W = \left\{ w = (w_1, \dots, w_m) | \sum_{i=1}^m w_i = 1, w_i = \frac{l}{s} \wedge, l \in 0,\dots,s\right\},
            $$
            with $|W| = {{s+m-1}\choose{k-1}}1$.
        \item New weights are drawn in every BO iteration.
        \item $\rho$ is a small parameter suggested to be set to $0.05$.
        \item $s$ selects the number of different weights to draw from.
    \end{itemize}

\end{frame}

\begin{frame}{Why the Tchebycheff norm?}


    $$
    c = \max_{i=1,\dots,m}\left(w_i \cost_i(\conf)\right) + \rho \sum_{i=1}^m w_i\cost_i(\conf),
    $$

    \begin{itemize}
        \item The norm consists of two components:
            \begin{itemize}
                    \item $\max_{i=1,\dots,m}\left(w_i \cost_i(\conf)\right)$ takes only the cost function with maximum weight into account.
                    \item $\sum_{i=1}^m w_i\cost_i(\conf)$ is the weighted sum of all cost functions.
            \end{itemize}
        \item $\rho$ describes the trade-off between these components.
        \item By the randomized weights in each iteration and the usually small value of $\rho = 0.05$, this allows exploration of extreme points of single cost functions.
    \end{itemize}

\end{frame}

\begin{frame}{ParEGO Algorithm}


\begin{center}
\begin{minipage}{0.8\textwidth}
\begin{algorithm}[H]
    %\DontPrintSemicolon
%    \SetAlgoLined
    \setcounter{AlgoLine}{0}
    \SetKwInOut{Require}{Require}
    \SetKwInOut{Result}{Result}

    \Require{Search space $\pcs$,
    		cost function $\cost$,
    		acquisition function $\acq$, predictive model $\surro$,
            maximal number of function evaluations $\bobudget$, $\rho$, $l$, $s$}
    \Result{Best configuration $\finconf$
    (according to $\dataset$ or
    $\surro$)}

	Initialize data $\iter[0]{\dataset}$ with initial observations\;% \leftarrow \varnothing$\;

    \For{$\bocount=1$ \KwTo $\bobudget$}{
		%\While{$B$ not exhausted} {
        Sample $w$ from  $\left\{ w = (w_1, \dots, w_m) | \sum_{i=1}^m w_i = 1, w_i = \frac{l}{s} \wedge, l \in 0,\dots,s\right\}$;

        Compute scalarization $c^{(t)} = \max_{i=1,\dots,m}\left(w_i \cost_i(\conf)\right) + \rho \sum_{i=1}^m w_i\cost_i(\conf)$;

		Fit predictive model $\iter[\bocount]{\surro}$ on $\iter[\bocount-1]{\dataset}$\;

		Select next query point: $\bonextsample \in \argmax_{\conf \in \pcs} \acq(\conf; \iter[\bocount-1]{\dataset}, \iter[\bocount]{\surro})$\;

		Query $\bonextobs$\;

		Update data: $\iter[\bocount]{\dataset} \leftarrow \iter[\bocount-1]{\dataset} \cup \{\langle \bonextsample, \bonextobs \rangle \}$\;
	}
	\caption*{ParEGO loop}
\end{algorithm}
\end{minipage}
\end{center}
\end{frame}


\begin{frame}{ParEgo Example}

    \begin{center}
        \includegraphics[scale=0.4]{images/parego_ex}
    \end{center}
    \vspace{-0.5cm}
    \begin{scriptsize}
    Example of 3 ParEGO iterations. Red circles are initial design points, blue triangles are proposed configurations and green squares proposed configurations of earlier iterations.
    The solid line shows the current scalarization of the current iterations.
    \end{scriptsize}
\end{frame}


\begin{frame}{Hypervolume based Acquisition Functions}

    \textbf{Idea:} Define acquisition function that directly models contribution to the dominated hypervolume.

            $$
            \max(0, S(\mathcal{P} \cup \conf, R) - S(\mathcal{P}, R))
            $$
    \begin{center}
        \includegraphics[scale=0.22]{images/hv_contribution}
    \end{center}

    \begin{itemize}
            \item Fit $m$ single-criteria surrogate models $\surro_1, \dots, \surro_m$
            \item Acquisition function takes all surrogate models into account.
            \item Single-criteria optimization of acquisition function.
    \end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{S-Metric Selection-based EGO}

    Using the Lower Confidence bound $u_{\text{LCB}, 1}(\conf), \dots, u_{\text{LCB}, m}(\conf)$, an optimistic estimate of hypervolume contribution can be calculated.

    \begin{center}
        \includegraphics[scale=0.35]{images/hv_contribution_2}
    \end{center}

    \framebreak

    As potential solution can be predicted beyond the real objective space, additive $\epsilon$-dominance ($\preceq_\epsilon$) is measured
            $$
                c_1(\conf^{(i)}) \le \epsilon+c_1(\conf^{(j)}), \dots, c_m(\conf^{(i)}) \le \epsilon+c_m(\conf^{(j)}),
            $$
            and a penalty is applied to these configurations
            $$
                \Psi(\conf) = -1 + \prod_{j=1}^m(1+u(\conf)_j - c_j).
            $$

    \begin{itemize}
        \item This method is referred to as SMS-EGO.
    \end{itemize}
\end{frame}

\begin{frame}{Further Hypervolume based Acquisition Functions}

    \textbf{Expected Hypervolume Improvement (EHI)}

$$
    u_{EI, \mathcal{H}}(\conf) = \int_{-\infty}^{\infty} p(\cost \mid \conf) \times \mathcal{H}(\conf)\;\; d\cost,\
$$
    with $\mathcal{H}(\conf) = S(\mathcal{P} \cup \conf, R) - S(\mathcal{P}, R))$.

    \begin{itemize}
        \item Direct extension of $u_EI$ to the hypervolume.
        \item Efficient computations for $m \le 3$ exist, beyond that expensive simulation-based computation is required.
    \end{itemize}


    \vspace{0.5cm}
Further hypervolume based acquisition functions:

    \begin{itemize}
        \item \textbf{Stepwise Uncertainty Reduction} (SUR) based on the probability of improvement.
        \item \textbf{Expected Maximin Improvement} (EMI) based on the $\epsilon$-indicator.
    \end{itemize}


\end{frame}

\begin{frame}{Hypervolume based BO Algorithm}


\begin{center}
\begin{minipage}{0.8\textwidth}
\begin{algorithm}[H]
    %\DontPrintSemicolon
%    \SetAlgoLined
    \setcounter{AlgoLine}{0}
    \SetKwInOut{Require}{Require}
    \SetKwInOut{Result}{Result}

    \Require{Search space $\pcs$,
    		cost function $\cost$,
    		acquisition function $\acq$, predictive model $\surro$,
            maximal number of function evaluations $\bobudget$}
    \Result{Best configuration $\finconf$
    (according to $\dataset$ or
    $\surro$)}

	Initialize data $\iter[0]{\dataset}$ with initial observations\;% \leftarrow \varnothing$\;

    \For{$\bocount=1$ \KwTo $\bobudget$}{
		%\While{$B$ not exhausted} {

		Fit predictive models $\iter[\bocount]{\surro_1}, \dots, \iter[\bocount]{\surro_m}$ on $\iter[\bocount-1]{\dataset}$\;

		Select next query point: $\bonextsample \in \argmax_{\conf \in \pcs} \acq(\conf; \iter[\bocount-1]{\dataset}, \iter[\bocount]{\surro_1}, \dots, \iter[\bocount]{\surro_m}))$\;

		Query $\bonextobs$\;

		Update data: $\iter[\bocount]{\dataset} \leftarrow \iter[\bocount-1]{\dataset} \cup \{\langle \bonextsample, \bonextobs \rangle \}$\;
	}
	\caption*{Hypervolume based BO loop}
\end{algorithm}
\end{minipage}
\end{center}
\end{frame}

\begin{frame}{Multi-Criteria Optimization of Acquisition Functions}
    \textbf{Goal:} Apply multi-criteria optimization methods on acquisition to approximate a pareto front.

    \begin{itemize}
       \item Separate surrogate model $\surro_1, \dots, \surro_m$ for each cost function.
       \item Apply Multi-Criteria optimization (e.g. NSGA-II) to approximate pareto front over acquisition functions.
       \item Select a single configuration from the estimated pareto front, e.g., by hypervolume contribution.
    \end{itemize}
\end{frame}


\end{document}
