
\input{../latex_main/main.tex}

\newcommand{\a}[0]{\mathbf{a}}
\newcommand{\y}[0]{\mathbf{y}}
\newcommand{\q}[0]{\mathbf{q}}
\newcommand{\Xspace}[0]{\mathcal{X}}

\title[AutoML: Overview]{Multi-criteria Optimization}
\subtitle{Bayesian Optimization}
%TODO: change authors!
\author[Bernd Bischl]{Bernd Bischl \and Frank Hutter \and Lars Kotthoff \and \underline{Marius Lindauer}}
\institute{}
\date{}



% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}

	\maketitle



\begin{frame}[allowframebreaks]{Recap: Bayesian Optimization}

Remember the advantages of Bayesian optimization:

    \begin{itemize}
        \item Sample efficiency
        \item Noise handling
        \item Constraint handling
        \item ...
    \end{itemize}

To get all these advantages also for multi-criteria problems, the classic Bayesian optimization loop has to be extended for multiple cost functions.

\framebreak

\begin{algorithm}[H]
    \Input{Search Space $\pcs$,
    		black box function $\func$,
    		acquisition function $\acq$, \\
    		maximal number of function evaluations $\bobudget$.
    	}
	\BlankLine
	$\dataset_0$ $\leftarrow$ initial\_design($\pcs$);

	\For{\bocount = $1, 2, \ldots \bobudget - |\dataset_0|$}{
		%\While{$B$ not exhausted} {
		$\surro$ $\leftarrow$ fit predictive model on $\dataset_{\bocount-1}$;

		select $\bonextsample$ by optimizing $\bonextsample \in \argmax_{\conf \in \pcs} \acq(\conf; \dataset_{\bocount-1}, \surro)$;

		Query $\bonextobs := \func(\bonextsample)$;

		Add observation to data $\dataset_{\bocount} := \dataset_{\bocount-1} \cup \{\langle \bonextsample, \bonextobs \rangle \}$;\\
	}
	\Return{Best $\conf$ according to $\dataset_\bocount$ or $\surro$}
	\caption{BO loop}
\end{algorithm}

\end{frame}

\begin{frame}{Multi-Criteria Bayesian Optimization}

\textbf{Goal}: Extend BO for multiple cost functions

$$
\min_{\conf \in \pcs}  \cost(\conf) \Leftrightarrow \min_{\conf \in \pcs} \left(\cost_1(\conf), \cost_2(\conf), ..., \cost_m(\conf)\right).
$$


There are three general approaches:


\begin{enumerate}
        \item Scalarization of cost functions.
        \item Direct optimization of multi-criteria performance indicator.
        \item Multi-criteria optimization of acquisition functions.
\end{enumerate}


\end{frame}

\begin{frame}{Scalarization: ParEGO}

    Scalarize the cost functions using the augmented Tchebycheff norm

    $$
    c = \max_{i=1,\dots,m}\left(w_i \cost_i(\conf)\right) + \rho \sum_{i=1}^m w_i\cost_i(\conf),
    $$

    \begin{itemize}
        \item The weights $w \in W$ are drawn from
            $$
                W = \left\{ w = (w_1, \dots, w_m) | \sum_{i=1}^m w_i = 1, w_i = \frac{l}{s} \wedge, l \in 0,\dots,s\right\},
            $$
            with $|W| = {{s+m-1}\choose{k-1}}$.
        \item New weights are drawn in every BO iteration.
        \item $\rho$ is a small parameter suggested to be set to $0.05$.
        \item $s$ selects the number of differents weights to draw from.
        \item Simplifies multi-criteria optimization problem to single-criteria.
    \end{itemize}

\end{frame}

\begin{frame}{Why the Tchebycheff norm?}


    $$
    c = \max_{i=1,\dots,m}\left(w_i \cost_i(\conf)\right) + \rho \sum_{i=1}^m w_i\cost_i(\conf),
    $$

    \begin{itemize}
        \item The norm consists of two components:
            \begin{itemize}
                    \item $\max_{i=1,\dots,m}\left(w_i \cost_i(\conf)\right)$ takes only the const function with maximum weight into account.
                    \item $\sum_{i=1}^m w_i\cost_i(\conf)$ is the weighted sum of all cost functions.
            \end{itemize}
        \item $\rho$ describes the trade-off between these components.
        \item By the randomized weights in each iteration and the usually small value of $\rho = 0.05$, this allows exploration of extreme points of single cost functions.
    \end{itemize}

\end{frame}

\begin{frame}{ParEGO Algorithm}

\begin{algorithm}[H]
    \Input{Search Space $\pcs$,
    		black box function $\func$,
    		acquisition function $\acq$, $\rho$, $s$, \\
    		maximal number of function evaluations $\bobudget$.
    	}
	\BlankLine
	$\dataset_0$ $\leftarrow$ initial\_design($\pcs$);

	\For{\bocount = $1, 2, \ldots \bobudget - |\dataset_0|$}{
		%\While{$B$ not exhausted} {

        sample $w$ from  $\left\{ w = (w_1, \dots, w_m) | \sum_{i=1}^m w_i = 1, w_i = \frac{l}{s} \wedge, l \in 0,\dots,s\right\}$;

        compute scalarization $c^{(t)} =     \max_{i=1,\dots,m}\left(w_i \cost_i(\conf)\right) + \rho \sum_{i=1}^m w_i\cost_i(\conf)$;

		$\hat c^{(t)}$ $\leftarrow$ fit predictive model on $\dataset_{\bocount-1}$;

		select $\bonextsample$ by optimizing $\bonextsample \in \argmax_{\conf \in \pcs} \acq(\conf; \dataset_{\bocount-1}, \hat c^{(t)})$;

        Query $c_1(\bonextsample), \dots, c_m(\bonextsample) := \func_1(\bonextsample), \dots, \func_m(\bonextsample)$;

		Add observation to data $\dataset_{\bocount} := \dataset_{\bocount-1} \cup \{\langle \bonextsample, c_1(\bonextsample), \dots, c_m(\bonextsample) \rangle \}$;\\
	}
	\Return{Best $\conf$ according to $\dataset_\bocount$ or $\surro$}
	\caption{ParEGO loop}
\end{algorithm}

\end{frame}

\begin{frame}{ParEgo Example}

    \begin{center}
        \includegraphics[scale=0.4]{images/parego_ex}
    \end{center}
    \vspace{-0.5cm}
    \begin{scriptsize}
    Example of 3 ParEGO iterations. Red circles are initial design points, blue triangles are proposed configurations and green squares proposed configurations of earlier iterations.
    The solid line shows the current scalarization of the current iterations.
    \end{scriptsize}
\end{frame}


\begin{frame}{Hypervolume based Acquisition Functions}

    \textbf{Idea:} Use an acquisition function that directly models contribution to the dominated hypervolume.

    \vspace{0.5cm}

    \begin{itemize}
        \item Separate surrogate model $\surro_1, \dots, \surro_m$ for each cost function.
        \item Aggregate contribution of each $\conf$ in regards to the estimated hypervolume contribution
            $$
            \max(0, S(\mathcal{P} \cup \conf, R) - S(\mathcal{P}, R))
            $$
    \end{itemize}

    \begin{center}
        \includegraphics[scale=0.3]{images/hv_contribution}
    \end{center}


\end{frame}

\begin{frame}[allowframebreaks]{S-Metric Selection-based EGO}
    Using the Lower Confidence bound $u_1(\conf), \dots, u_m(\conf)$, an optimistic estimate of hypervolume contribution can be calculated.

    \begin{center}
        \includegraphics[scale=0.35]{images/hv_contribution_2}
    \end{center}

    \framebreak

    As potential solution can be predicted beyond the real objective space, additive $\epsilon$-dominance ($\preceq_\epsilon$) is measured
            $$
                c_1(\conf^{(i)}) \le \epsilon+c_1(\conf^{(j)}), \dots, c_m(\conf^{(i)}) \le \epsilon+c_m(\conf^{(j)}),
            $$
            and a penalty is applied to these configurations
            $$
                \Psi(\conf) = -1 + \prod_{j=1}^m(1+u(\conf)_j - c_j).
            $$

    \begin{itemize}
        \item This method is referred to as SMS-EGO.
        \item Extension exist for more advanced indicators exist, e.g., $\epsilon$-EGO.
    \end{itemize}
\end{frame}


\begin{frame}{Multi-Criteria Optimization of Acquisition Functions}
    \textbf{Goal:} Apply multi-criteria optimization methods on acquisition to approximate a pareto front.

    \begin{itemize}
       \item Separate surrogate model $\surro_1, \dots, \surro_m$ for each cost function.
       \item Apply Multi-Criteria optimization (e.g. NSGA-II) to approximate pareto front over acquisition functions.
       \item Select a single configuration from the estimated pareto front, e.g., by hypervolume contribution.
    \end{itemize}
\end{frame}


\end{document}
