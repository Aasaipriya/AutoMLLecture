
\input{../latex_main/main.tex}

%\newcommand{\a}[0]{\mathbf{a}}
%\newcommand{\y}[0]{\mathbf{y}}
\newcommand{\q}[0]{\mathbf{q}}
\newcommand{\Xspace}[0]{\mathcal{X}}

\title[AutoML: Overview]{Multi-criteria Optimization}
\subtitle{Practical Applications}
\author[Bernd Bischl]{\underline{Bernd Bischl} \and Frank Hutter \and Lars Kotthoff\newline \and Marius Lindauer \and Joaquin Vanschoren}
\institute{}
\date{}



% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}

	\maketitle

\begin{frame}[allowframebreaks]{Practical Applications in Machine Learning}

    \textbf{ROC Optimization}: Balance \emph{true positive} and \emph{false positive} rates
  \begin{itemize}
    \item Typically unbalanced classification tasks with unspecified costs.
    \item Many related measures, e.g., \emph{positive predicted value} or \emph{false discovery rate}, exist and can be of interest as well.
  \end{itemize}

\textbf{Efficient Models}:
    Balance \emph{predictive performance} with \emph{prediction time}, \emph{energy consumption} and/or \emph{model size}.
  \begin{itemize}
    \item Time: Models in production models need to predict fast.
    \item Size / Energy consumption: Models should be deployed on a mobile/edge device and not use much power.
  \end{itemize}

\textbf{Fair Models}:
  Balance \emph{predictive performance} and \emph{fairness}.
  \begin{itemize}
    \item Model has to be fair regarding subgroups in the data, e.g. gender.
    \item Many different approaches to quantify fairness exist.
  \end{itemize}

\end{frame}

\begin{frame}{ROC Optimization - Setup}

  Again, we want to train a \textit{spam detector} on the popular Spam dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/spambase}}.

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{itemize}
        \item Learning algorithm: SVM with RBF kernel.
        \item Hyperparameters to optimize: \\
        \begin{tabular}{rl}
        \texttt{cost} & $[2^{-15}, 2^{15}]$ \\
        $\gamma$ & $[2^{-15}, 2^{15}]$ \\
        Threshold $t$ & $[0,1]$ \\
        \end{tabular}
        \item Objective: \emph{minimize} false positive rate (FPR) and \emph{maximize} true positive rate (TPR), evaluated through 5-fold CV
\end{itemize}
\end{column}%
\begin{column}{0.5\textwidth}
\begin{itemize}
        \item Optimizer: Multi-criteria Bayesian optimization:
            \begin{itemize}
              \item ParEGO with $\rho = 0.05$, $s = 100000$.
              \item Acquisition function $\acq$: \emph{Confidence Bound} with $\alpha = 2$.
              \item Budget: $100$ evaluations
            \end{itemize}
        \item Tuning is conducted on a training holdout and all hyperparameters configurations of the Pareto front are validated on an outer validation set.
\end{itemize}
\end{column}
\end{columns}
\vspace{0.5cm}
{\footnotesize For simplicity we refrain from optimizing the threshold parameter independently posthoc.}
\end{frame}

\begin{frame}{ROC Optimization - Result I}

\begin{columns}
\begin{column}{0.45\textwidth}
  We notice here:
  \begin{itemize}
    \item Compared to the \emph{random search}: Many \emph{ParEGO} evaluations are on the estimated Pareto front.
    \item The Pareto front of \emph{ParEGO} dominates most points from the \emph{random search}.
    \item The dominated hypervolume to the reference point $(1,1)$ is:
    \begin{tabular}{rl}
    \emph{ParEGO:} & 0.965\\
    \emph{random search:} & 0.959\\
    \end{tabular}
  \end{itemize}
  Note: The Pareto front does not reflect the stochastic characteristic of our objective.
\end{column}%
\begin{column}{0.5\textwidth}
  \begin{figure}
  \includegraphics[width=\textwidth]{images/example_parego_spam.png}
  \end{figure}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{ROC Optimization - Result II}

\begin{columns}
\begin{column}{0.45\textwidth}
  We validate the configurations on the Pareto front on a holdout:
  \begin{itemize}
    \item<1-> The performance on the validation set varies slightly.
    \item<1-> The TPR got slightly better but the FPR got slightly worse.
    \item<1-> On the validation set, some configurations get dominated by others.
    \item<2> The dominated hypervolume of the validation set is:
    \begin{tabular}{rl}
    \emph{ParEGO:} & 0.960\\
    \emph{random search:} & 0.961\\
    \end{tabular}
  \end{itemize}
\end{column}%
\begin{column}{0.5\textwidth}
  \begin{figure}
  \includegraphics<1>[width=\textwidth]{images/example_parego_spam_outer.png}
  \includegraphics<2>[width=\textwidth]{images/example_parego_spam_outer_pareto.png}
  \end{figure}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Efficient Models - Overview}

\begin{itemize}
  \item "Efficiency" can be:
  \begin{itemize}
    \item Memory consumption of the model
    \item Training or prediction time
    \item Number of features needed
    \item Energy consumption for prediction
    \item \ldots
  \end{itemize}
  \item Some hyperparameters have a strong impact on the efficiency of a model, e.g.,
  \begin{itemize}
    \item Number of trees in Random Forests or gradient tree boosting,
    \item Number, size and type of layers in neural networks,
    \item L1 regularization penalties,
    \item ...
  \end{itemize}
  \item Other hyperparameters can have no influence on efficiency.
  \item Optimizing over multiple algorithms of varying efficiency at the same time.
\end{itemize}

\end{frame}

\begin{frame}{Efficient Models - Example}
Image Classification on CIFAR-10 dataset\footnote{\url{https://arxiv.org/pdf/1904.09035.pdf}}
  \begin{columns}
  \begin{column}{0.65\textwidth}
    \begin{itemize}
      \item Objective: \emph{accuracy} vs.\ \emph{FLOPS} (floating point operations, per observation)
      \item Learner: \emph{DenseNet-121} (Densely Connected Convolutional Networks)
      \item Search Space:\\
        \begin{tabular}{rl}
          \texttt{growth rate in all four blocks} ($k$) & $[8,32]$ \\ %growth rate: k_l = k_0 + k (l-1), how much information is added in each layer
          \texttt{layers in first block} & $[4, 6]$ \\
          \texttt{layers in second block} & $[4, 12] $ \\
          \texttt{layers in third block} & $[4, 24] $ \\
          \texttt{layers in fourth block} & $[4, 16] $ \\
        \end{tabular}
      \item Tuner: \emph{Particle Swarm Optimization}
    \end{itemize}
  \end{column}%
  \begin{column}{0.35\textwidth}
    \begin{figure}
    \includegraphics[width=\textwidth]{images/Wang_et_al_2019_Evolving_Deep_Neural_Networks_fig7_1.png}
    \end{figure}
  \end{column}
  \end{columns}


\end{frame}

\begin{frame}{Fair Models}
\begin{columns}
\begin{column}{0.4\textwidth}
Dataset: \texttt{Adult}
\begin{itemize}
  \footnotesize
  \item  Source: US Census database, 1994, \url{https://www.openml.org/d/1590}.
  \item 48842 observations
  \item Target: binary, income above 50k
  \item 14 features: \texttt{age, education, hours.per.week, marital.status, native.country, occupation, race, relationship, sex, \ldots}
\end{itemize}
\includegraphics[scale = 0.45]{images/dataset_adult_age_sex.png}
\end{column}%
\begin{column}{0.6\textwidth}

\includegraphics[scale = 0.45]{images/dataset_adult_race.png}%
\includegraphics[scale = 0.45]{images/dataset_adult_education.png}
\end{column}
\end{columns}


\end{frame}

\begin{frame}[allowframebreaks]{Fair Models - Setup}
  A fair model for income prediction on binarized target.
%\begin{columns}
%\begin{column}{0.5\textwidth}
\begin{itemize}
  \item Learner: \emph{eXtreme Gradient Boosting}
  \item Hyperparameters to optimize: \\
  \begin{tabular}{rl}
    \texttt{eta} & $[0.01,0.2]$ \\
    \texttt{gamma} & $[2^{-7},2^6]$ \\
    \texttt{max\_depth} & $\{2, \ldots, 20\}$ \\
    \texttt{colsample\_bytree} & $[0.5,1]$ \\
    \texttt{colsample\_bylevel} & $[0.5,1]$ \\
    \texttt{lambda} & $[2^{-10},2^{10}]$ \\
    \texttt{alpha} & $[2^{-10},2^{10}]$ \\
    \texttt{subsample} & $[0.5,1]$ \\
  \end{tabular}
  \item Objective: minimize \emph{missclassification error} and \emph{fairness}
%\end{itemize}
%\end{column}%
%\begin{column}{0.5\textwidth}
%\begin{itemize}
  \item "Is the rate of classified as high income equal amongst both subgroups given the prevalence in each subgroup?"
  \item Here, a simplified proxy for fairness is defined as the absolute difference in F1-Scores between female ($f$) and male ($m$) population:
  \[
  \loss_{\text{fair}} := |\loss_{\text{F1}}(y_f,\fh(\x_f)) - \loss_{\text{F1}}(y_m,\fh(\x_m))|
  \]
  \item Optimizer: ParEGO with Random Forest surrogate and restricted range of projections to $[0.1, 0.9]$ (No interest in very unfair or bad configurations).
\end{itemize}
\vspace{1em}
Note on this example:
\begin{itemize}
  \item Here, the hyperparameters actually have an effect on the defined \emph{fairness measure}.
  \item However, this is often not the case or not enough to ensure a fair model.
\end{itemize}
%\end{column}
%\end{columns}

\end{frame}

\begin{frame}{Fair Models - Results}

  \begin{figure}
    \centering
    \includegraphics[scale=1.2]{images/Pfisterer_et_al_2019_Multi_Objective_fig4.pdf}
    \caption{Pareto fronts after 20, 70 and 120 tuning iterations.}
  \end{figure}

\end{frame}



\end{document}
