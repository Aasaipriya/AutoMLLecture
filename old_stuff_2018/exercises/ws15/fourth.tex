\documentclass{exam}
\usepackage{amsmath, amsfonts}
\usepackage{verbatim}
\usepackage{graphicx}


\usepackage[hyperfootnotes=false]{hyperref}

\usepackage[usenames,dvipsnames]{color}
\newcommand{\note}[1]{
	\noindent~\\
	\vspace{0.25cm}
	\fcolorbox{Red}{Orange}{\parbox{0.99\textwidth}{#1\\}}
	%{\parbox{0.99\textwidth}{#1\\}}
	\vspace{0.25cm}
}
\renewcommand{\note}[1]{}

\newcommand{\hide}[1]{}

\renewcommand{\vec}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmin}{argmin}

\qformat{\thequestion. \textbf{\thequestiontitle}\hfill[\thepoints]}
\bonusqformat{\thequestion. \textbf{\thequestiontitle}\hfill[\thepoints]}

\pagestyle{headandfoot}
\firstpageheader{Due: 03.12.2015 (23:59 GMT)}{ {\bf MLOAD} \\ Fourth Assignment}{M. Lindauer \& F. Hutter\\ WS 2015/16}
\runningheader{Due: 03.12.2015 (23:59 GMT)}{Fourth Assignment}{WS 2015/16}
\runningfooter{}{}{}
\headrule
\pointsinrightmargin
\bracketedpoints
\marginpointname{pt.}


\begin{document}

After you now know some examples of machine learning classification approaches,
the goal of this fourth exercise is to let you implement some simple Python programs
to get used to machine learning in Python and to understand the importance of hyper-parameter tuning. 

\bigskip

For this exercise assignment, we provide five data sets for binary classification.
You can download them at our ILIAS course webpage.
To load them, you can use already existing functions in \texttt{numpy}\\
\texttt{numpy.loadtxt("x\_train.np")}

\bigskip

All tasks include the submission of some results (besides the code).
To submit these results, please submit a PDF with all the results and your name(s).

\begin{questions}


\titledquestion{k-Nearest Neighbor}[20]

Your first task is to implement k-Nearest Neighbor (k-NN) in Python to classify the given data sets. 
You can use Python Packages such as \texttt{numpy} and \texttt{scipy},
but you are \textbf{not} allowed to use already implemented k-Nearest Neighbor classifiers, such as in \texttt{sklearn}.

Your implementation should allow you to modify the following aspects of k-NN

\begin{itemize}
  \item feature scaling (as a pre-processing method); options should be the module\\ \texttt{sklearn.preprocessing.MinMaxScaler}, \texttt{sklearn.preprocessing.StandardScaler}, or no feature scaling
  \item neighborhood size ($k$)
  \item distance metric with two possible choices: $L_1$ (Manhattan distance) and $L_2$ (Euclidean distance)
\end{itemize}

For the given data sets, you have to assess the prediction accuracy of your k-NN implementation ($k =1$, MinMaxScaling and Euclidean distance) using a $10$-fold cross validation ($\to$ PDF). You can use the modules \texttt{sklearn.metrics.accuracy\_score} and \texttt{sklearn.cross\_validation.KFold} for this task.
	
\titledquestion{Parameter Tuning of k-NN}[10]

Your task is to determine the best possible configurations of your k-NN implementation for the first and second given data set (``set1'' and ``set2'').
As a domain of $k$ use $[1,33]$. Please note that you have to assess the performance of $3 \cdot 33 \cdot 2 = 198$ possible parameter configurations for this task. 

Plot the accuracy performance over the neighborhood size $k$ (as shown in the lecture) for the two given data sets. To generate these plots, you should fix the pre-processing to MinMaxScaler and Euclidean distance. 

Your submitted PDF should include the best found parameter configurations and the accuracy performance plots over $k$ ($\to$ PDF).

\titledquestion{AutoSklearn}[10]

Your third task is to use \texttt{auto-sklearn}\footnote{\url{https://github.com/automl/auto-sklearn}} to address the \emph{CASH} problem in \texttt{sklearn}. \texttt{auto-sklearn} should run for at least one hour on the first data set (``set1''). 
To get the best found parameter configuration, you can use \texttt{print(classifier.show\_models())}, where \texttt{classifier} is a trained instance of the class \texttt{autosklearn.AutoSklearnClassifier}.

Your submitted PDF should include the best parameter configuration found by \texttt{auto-sklearn} and its accuracy performance ($\to$ PDF).

\titledquestion{Mini ML-Competition}[10 (+ 10)]

For each of the used data sets, we have a secret test set. Your fourth task is to submit a program that has as inputs the training data ($X$ and $y$) and the feature matrix for test data and writes a file (\texttt{y\_test.np}) with the binary predictions for the test data. The output file should have exactly one line with either $0$ or $1$ for each sample in the test data (e.g., use \texttt{numpy.savetxt("y\_test.np", y\_test, "\%d")}.
The program call and its output should look like:

\begin{verbatim}
$ python ml.py x_train.np y_train.np x_test.np 
>>> Output written to "y_test.np"
\end{verbatim} 

You are allowed to use whatever you want in Python as long as:

\begin{itemize}
  \item your program will not use more than $5$ minutes and $1$ GB RAM per data set
  \item the installation of the requirements of your program in a \emph{virtualenv} is fairly easy -- please provide a \texttt{requirements.txt} that can be used to install all required packages with \texttt{pip install -r requirements.txt}
  \item you provide a brief explanation of your approach ($\to$ PDF)
  \item You will get $10$ points if your submission satisfies all of the above constraints and your solution performs better than our baseline implementation (using a Random Forest without any pre-processing of the data); you will get further $10$ bonus points for being placed at least at Rank $3$. The ranking of a submission will be according to the average rank across the data sets.
\end{itemize} 

\titledquestion{Feedback}[Bonus: 5]
For each question in this assignment, state:
\begin{itemize}
	\item How long you worked on it.
	\item What you learned.
	\item Anything you would improve in this question if you were teaching the course.
\end{itemize}

\end{questions}


% {\bf This assignment is due on 16.11.2015 (23:59 GMT).} Submit your solution for the tasks by uploading a PDF to our ILIAS\footnote{ \url{https://ilias.uni-freiburg.de/goto.php?target=crs_465155&client_id=unifreiburg}.} course page. The PDF has to include the name of the submitter(s). Teams of at most $2$ students are allowed. Everyone has to submit his/her solution.\\

% Please note that this assignment is optional and you can get bonus points. However, we strongly encourage you to solve the given tasks since we will solve such CSP problems in the next exercise and you will benefit from some experience with modelling and understanding CSP problems. 

\noindent
{\bf This assignment is due on 03.12.2015 (23:59 GMT).} Submit your solution for the tasks by uploading an archive (tar.gz) to our ILIAS\footnote{ \url{https://ilias.uni-freiburg.de/goto.php?target=crs_465155&client_id=unifreiburg}.} course page. The archive has to include the name of the submitter(s). Teams of at most $2$ students are allowed. Everyone has to submit his/her solution. 

\bigskip
General constraints:

\begin{itemize}
  \item The program can be called as stated on the exercise sheet.
  \item The program exactly returns the required output (neither less nor more) -- please use a \texttt{--verbose} option to increase the verbosity level.
  \item Your scripts should be commented to be readable for the tutors. All functions and classes are documented with a docstring. 
  \item Provide a README ($\to$ how to install requirements and run your program(s)) and (if necessary) an installation script if your program requires any other packages.
\end{itemize}

\bigskip

Submissions will get $0$ points if they do not satisfy these constraints.

\end{document}