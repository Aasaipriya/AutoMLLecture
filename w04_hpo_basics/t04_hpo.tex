
\input{../latex_main/main.tex}

\newcommand{\inducer}{\mathcal{I}}
\newcommand{\R}{\mathds{R}}

%The following might look confusing but allows us to switch the notation of the optimization problem independently from the notation of the hyper parameter optimization
\newcommand{\xx}{\conf} %x of the optimizer
\newcommand{\xxi}[1][i]{\lambda_{#1}} %i-th component of xx (not confuse with i-th individual)
\newcommand{\XX}{\Lambda} %search space / domain of f
\newcommand{\f}{\cost} %objective function

\newenvironment{blocki}[1] % itemize block
{
 \begin{block}{#1}\begin{itemize}
}
{
\end{itemize}\end{block}
}

\title[AutoML: Hyperparameter Optimization]{AutoML: Hyperparameter Optimization}
\subtitle{Overview for this Week}
%TODO: change authors!
\author[Bernd Bischl]{\underline{Bernd Bischl} \and Frank Hutter \and Lars Kotthoff \and Marius Lindauer}
\institute{}
\date{}



% \AtBeginSection[] % Do nothing for \section*
% {
% \begin{frame}{Outline}
% \bigskip
% \vfill
% \tableofcontents[currentsection]
% \end{frame}
% }

\begin{document}

\maketitle


%----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}[containsverbatim,allowframebreaks]{Motivating Example}

\begin{itemize}
\item Given a dataset, we want to train a classification tree.
\item We feel that a maximum tree depth of $4$ has worked out well for us previously, so we decide to set this hyperparameter to $4$.
\item The learner ("inducer") $\inducer$ takes the input data, internally performs \textbf{empirical risk minimization}, and returns a fitted tree model $\fh(\x) = f(\x, \hat{\theta})$ of at most depth $\lambda = 4$ that minimizes the empirical risk.
\end{itemize}

% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\begin{center}
\begin{figure}
\includegraphics[width=0.5\textwidth]{images/riskmin_bilevel1.png}
\end{figure}
\end{center}

\framebreak

\begin{itemize}
\item We are \textbf{actually} interested in the \textbf{generalization performance} $GE\left(\fh\right)$ of the estimated model on new, previously unseen data.
\item We estimate the generalization performance by evaluating the model $\fh$ on a test set $\datasettest$: $$
\widehat{GE}_{\datasettest}\left(\fh\right) = \frac{1}{|\datasettest|} \sum\limits_{(\x, y) \in \datasettest} L\left(y, \fh(\x) \right)
$$
\end{itemize}
\vspace*{-0.6cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\begin{center}
\begin{figure}
\includegraphics[width=0.5\textwidth]{images/riskmin_bilevel2.png}
\end{figure}
\end{center}

\framebreak

\begin{itemize}
\item But many ML algorithms are sensitive w.r.t. a good setting of their hyperparameters,
and generalization performance might be bad, if we have chosen a suboptimal configuration:
\begin{itemize}
\item The data may be too complex to be modeled by a tree of depth $4$
\item The data may be much simpler than we thought, and a tree of depth $4$ overfits
\end{itemize}
\item[$\implies$] algorithmically try out different values for the tree depth. For each maximal depth $\lambda$, we have to train the model \textbf{to completion} and evaluate its performance on the test set.
\item We choose the tree depth $\conf$ that is \textbf{optimal} w.r.t. the generalization error of the model.
\end{itemize}

% $\to$ Finding the hyperparameter $\lambda$ that is optimal w.r.t. the generalization performance constitutes an optimization problem.

\end{frame}

% \begin{frame}[containsverbatim,allowframebreaks]{The role of hyperparameters}

% \begin{itemize}
% \item Hyperparameters often control the complexity of a model, i.e., how flexible the model is.
% \item But they can in principle influence any structural property of a model or computational part of the training process.
% \item If a model is not flexible enough, it cannot approximate the relationship between the features and the output well and will underfit.
% \item If a model is too flexible so that it simply \textit{memorizes} the training data, we will face the dreaded problem of overfitting.
% \item[$\implies$] Hence, controlling the model's capacity, i.e., finding suitable hyperparameters,
% can prevent overfitting the model on the training set.
% \end{itemize}

% \end{frame}

\begin{frame}[containsverbatim,allowframebreaks]{Model Parameters vs. Hyperparameters}

It is critical to understand the difference between model parameters and hyperparameters.

\vspace{0.5cm}

\textbf{Model parameters} are optimized during training, typically via loss minimization. They are an \textbf{output} of the training. Examples:
\begin{itemize}
\item The splits and terminal node constants of a tree learner
\item Coefficients $\theta$ of a linear model $\fx = \theta^\top\x$
\end{itemize}

\framebreak

In contrast, \textbf{hyperparameters} (HPs) are not decided during training. They must be specified before the training, they are an \textbf{input} of the training.
Hyperparameters often control the complexity of a model, i.e., how flexible the model is.
But they can in principle influence any structural property of a model or computational part of the training process.

\vspace{0.5cm}

Examples:

\begin{itemize}
\item Tree: The maximum depth of a tree
\item $k$ Nearest Neighbours: Number of neighbours $k$ and distance measure
\item Linear regression: Number and maximal order of interactions
\end{itemize}

\end{frame}


\begin{frame}[containsverbatim,allowframebreaks]{Types of hyperparameters}

We summarize all hyperparameters we want to tune over in a vector $\conf \in \Lambda$ of (possibly) mixed type. HPs can have different types:

\begin{itemize}
\item Real-valued parameters, e.g.:
\begin{itemize}
\item Minimal error improvement in a tree to accept a split
\item Bandwidths of the kernel density estimates for Naive Bayes
\end{itemize}
\item Integer parameters, e.g.:
\begin{itemize}
\item Neighbourhood size $k$ for $k$-NN
\item Minimum number of samples for a split in a random forest
\end{itemize}
\item Categorical parameters, e.g.:
\begin{itemize}
\item Which split criterion for classification trees?
\item Which distance measure for $k$-NN?
\end{itemize}
\end{itemize}

Hyperparameters are often \textbf{hierarchically dependent} on each other, e.g., \emph{if} we use
a kernel-density estimate for Naive Bayes, what is its width?
% with polynomials of the features up to a certain maximal degree $d$, then
% we must specify whether to also include polynomial interaction terms like e.g. $x_j^{d-d'}x_m^{d'}$ or not
% and up to which degree $d' \leq d$.
\end{frame}

\begin{frame}{Tuning}

\vskip 3em
Recall: \textbf{Hyperparameters} $\conf$ are parameters that are \emph{inputs} to the
training problem, in which a learner $\inducer$ minimizes the empirical risk on a training data set in order
to find optimal \textbf{model parameters} $\theta$ which define the fitted model $\fh$.
\vskip 2em

\textbf{(Hyperparameter) Tuning} is the process of finding good model hyperparameters $\conf$.

% \begin{frame}[containsverbatim,allowframebreaks]{{Hyperparameter Tuning}
% \begin{itemize}
% \item Optimize hyperparameters for learner w.r.t. prediction error
% Tuner proposes configuration, eval by resampling, tuner receives performance, iterate
% \end{itemize}
% \begin{columns}[c, onlytextwidth]
% \column{0.45\textwidth}
% FIGURE SOURCE: No source
% \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=1.2\textwidth]{images/chain.jpg}
% \column{0.45\textwidth}
% FIGURE SOURCE: https://drive.google.com/open?id=1wY3aUZxIMZPje3vR0t2yWiDMx_osXRCi
% \includegraphics[trim={1cm 0cm 1cm 0cm}, clip, width=1.2\textwidth]{images/tuning_process.jpg}
% \end{columns}

% \end{frame}

\end{frame}


\begin{frame}[containsverbatim,allowframebreaks]{Tuning: A bi-level optimization problem}

\vspace{0.2cm}

We face a \textbf{bi-level} optimization problem: The well-known risk minimization problem to find $\hat f$ is \textbf{nested} within the outer hyperparameter optimization (also called second-level problem):

\begin{center}
\begin{figure}
% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\includegraphics[width=0.7\textwidth]{images/riskmin_bilevel3.png}
\end{figure}
\end{center}

\framebreak

\begin{itemize}
\item For a learning algorithm $\inducer$ (also inducer) with $d$ hyperparameters, the hyperparameter \textbf{configuration space} is:
$$\Lambda=\Lambda_{1} \times \Lambda_{2} \times \ldots \times \Lambda_{d}$$
where $\Lambda_{i}$ is the domain of the $i$-th hyperparameter.
\item The domains can be continuous, discrete or categorical.
\item For practical reasons, the domain of a continuous or integer-valued hyperparameter is typically bounded.
\item A vector in this configuration space is denoted as $\conf \in \Lambda$.
\item A learning algorithm $\inducer$ takes a (training) dataset $\dataset$ and a hyperparameter configuration $\conf \in \Lambda$ and returns a trained model (through risk minimization).

\vspace*{-0.2cm}
\begin{eqnarray*}
\inducer: \left(\mathcal{X} \times \mathcal{Y}\right)^n \times \Lambda &\to& \mathcal{H} \\
(\dataset, \Lambda) &\mapsto& \inducer(\dataset, \Lambda) = \hat f_{\dataset, \Lambda}
\end{eqnarray*}
% \item Additionally, some hyperparameters may only need to be specified if another hyperparameter (or combination of hyperparameters) takes on a certain value.
\end{itemize}

% \vspace{0.5cm}

% Note that

% In contrast to the first-level (empirical) risk minimization problem, hyperparameter optimization is also referred to as \textbf{second-level} optimization. The first-level problem can be seen as a subroutine called by the second-level problem: Each evaluation of $\Lambda$ requires to solve the first-level optimization problem.


% \framebreak

% \begin{itemize}
% \item search for the \textbf{inducer} hyperparameter $\Lambda$
% \item that minimizes the \textbf{generalization error}
% $$
% \min_{\Lambda} \E_{\D_n \sim \Pxy, (\xv, y) \sim \Pxy} \left(V\left(y, \hat f_{\D, \Lambda}(\xv)\right)\right).
% $$
% \end{itemize}

% We compare: In empirical risk minimization, we

% \begin{itemize}
% \item search for the \textbf{model} parameter $\thetab$
% \item that minimizes the \textbf{empirical risk}
% $$
% \min_{\thetab} \sum_{(\xi, \yi) \in \datasettrain} L\left(\yi, \fxi\right).
% $$
% \end{itemize}

% In hyperparameter optimization, we

% \begin{itemize}
% \item search for the \textbf{inducer} hyperparameter $\Lambda$
% \item that minimizes the \textbf{test error}
% $$
% \min_{\Lambda \in \Lambda} \sum_{(\xi, \yi) \in \datasettest} V\left(\inducer(\datasettrain, \Lambda)(\xi), \yi\right).
% $$
% \end{itemize}

% \framebreak

% \framebreak

% The hyperparameter optimization problem is difficult in many ways:

\framebreak

We formally state the nested hyperparameter tuning problem as:

$$
\min_{\conf \in \Lambda} \widehat{GE}_{\datasettest}\left(\inducer(\datasettrain, \Lambda)\right)
$$

\begin{itemize}
\item The learner $\inducer(\datasettrain, \conf)$ takes a training dataset as well as hyperparameter settings $\Lambda$ (e.g.\ the maximal depth of a classification tree) as an input.
\item $\inducer(\datasettrain, \conf)$ performs empirical risk minimization on the training data and returns the optimal model $\hat f$ for the given hyperparameters.
\item Note that for the estimation of the generalization error, more sophisticated resampling strategies like cross-validation can be used.
\end{itemize}

\framebreak

The components of a tuning problem are:

\begin{itemize}
\item The dataset
\item The learner (possibly: several competing learners?) that is tuned %(e.g. a decision tree classifier)
\item The learner's hyperparameters and their respective regions-of-interest over which we optimize % (e.g. $\texttt{tree depth} \in \{1, 2, ..., 20\}$)
\item The performance measure, as determined by the application.\\ Not necessarily identical to the loss function that defines the risk minimization problem for the learner!\\
% We could even be interested in multiple measures simultaneously, e.g., accuracy and computation time of our model, TPR and PPV, etc.
\item A (resampling) procedure for estimating the predictive performance according to the performance measure.
 % The expected performance on unseen data can be estimated by holdout (i.e., a single train-test-split) or more advanced techniques like cross-validation.
% More on this later.
\end{itemize}

% \framebreak

% \begin{center}
% \begin{figure}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
% \includegraphics[width=1.2\textwidth]{images/autotune_in_model_fit.pdf}
% \end{figure}
% \end{center}

\end{frame}




% \framebreak

% Possible scenarios for finding default hyperparameters:

% \begin{itemize}
% \item If the learner's performance is fairly insensitive to changes of a hyperparameter, we don't really have to worry as long as we remain within the range of reasonable values.
% \item Constant default: we can benchmark the learner across a broad range of data sets and scenarios and try to find hyperparameter values that work well in many different situations. Quite optimistic?
% \item Dynamic (heuristic) default: We can benchmark the learner across a broad range of data sets and scenarios and try to find an easily computable function that sets the hyperparameter in a data dependent way,
% e.g. using \texttt{mtry}$ = p/3$ for RF.\\
% How to construct or learn that heuristic function, though...?
% \item In some cases, can try to set hyperparameters optimally by extracting more info from the fitted model. E.g. \texttt{ntrees} for a random forest (does OOB error increase or decrease if you remove trees from the ensemble?).
% \end{itemize}
% \end{frame}



\begin{frame}[allowframebreaks,containsverbatim]{Nested Crossvalidation}
Selecting the best model from a set of potential candidates (e.g., different classes of learners, different hyperparameter settings, different feature sets, different preprocessing, ....) is an important part of most  machine learning problems.

\begin{blocki}{Problem}
    \item We cannot evaluate our finally selected learner on the same resampling splits that we have used to perform model selection for it, e.g., to tune its hyperparameters.
    \item By repeatedly evaluating the learner on the same test set, or the same CV splits, information
      about the test set leaks into our evaluation.
    \item Danger of overfitting to the resampling splits / overtuning!
    \item The final performance estimate will be optimistically biased.
    \item One could also see this as a problem similar to multiple testing.
\end{blocki}

\framebreak

\begin{itemize}
\item All parts of the model building (including model selection, preprocessing) should be embedded in the model-finding process \textbf{on the training data}.
\item The test set should only be touched once, so we have no way of \textit{cheating}. The test dataset is only used once \emph{after} a model is completely trained, after deciding for example on specific hyper-parameters.

Only if we do this, the performance estimates we obtained from the test set are \textbf{unbiased estimates} of the true performance.

\item For steps that themselves require resampling (e.g., hyperparameter tuning) this results
  in \textbf{nested resampling}, i.e., resampling strategies for both.
  \begin{itemize}
  \item tuning: an inner resampling loop to find what works best based on training data
  \item evaluation: an outer resampling loop to evaluate on data not used for tuning to get honest estimates of the expected performance on new data
  \end{itemize}
\end{itemize}

\framebreak

\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.4\textwidth}
\hspace*{-0.3cm}
Simplest method to achieve this: a 3-way split
\begin{itemize}
\item During tuning, a learner is trained on the \textbf{training set},
  evaluated on the  \textbf{validation set}.
\item After the best model configuration $\conf^\star$ is selected, we re-train on the joint (training+validation) set and evaluate the model's performance on the \textbf{test set}.
\end{itemize}

\column{0.5\textwidth}
\hspace*{-0.7cm}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1kBxfxdUzyUP2-_Y5kWWJ1TnWsm-C4AjHQeTaYPeFo24/edit?usp=sharing
\includegraphics[width=1.2\textwidth]{images/train_valid_test.pdf}
\end{center}
\end{columns}
\framebreak


\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.4\textwidth}
\hspace*{-0.3cm}
\begin{itemize}
\item Effectively, the tuning step is now simply part of a more complex training procedure.
\item We could see this as removing the hyperparameters from the inputs of the algorithm and making it \textit{self-tuning}.
\end{itemize}

\column{0.5\textwidth}
\hspace*{-0.7cm}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
\includegraphics[width=1.2\textwidth]{images/autotune_in_model_fit.pdf}
\end{center}
\end{columns}

\framebreak

More precisely: the combined training \& validation set is actually the training set for the \textit{self-tuning} endowed algorithm.

\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.45\textwidth}
\hspace*{-0.3cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1kBxfxdUzyUP2-_Y5kWWJ1TnWsm-C4AjHQeTaYPeFo24/edit?usp=sharing
\includegraphics[width=1.3\textwidth]{images/train_valid_test.pdf}
\column{0.45\textwidth}
\hspace*{-0.7cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
\includegraphics[width=1.3\textwidth]{images/autotune_in_model_fit.pdf}
\end{columns}

\framebreak

Just like we can generalize holdout splitting to resampling to get more reliable estimates of the predictive performance, we can generalize the training/validation/test approach to \textbf{nested resampling}.
\vskip 2em
This results in two resampling loops, whereas the resampling for the tuning is nested in the resampling for the outer evaluation, i.e., one resampling strategy for  tuning and another for outer evaluation.
% \vskip 2em
% Let's look at this for a simple example: say we want to find out whether a logistic regression model or a $k$-NN classifier performs better on some data set.

% \framebreak
% FIGURE SOURCE: https://docs.google.com/presentation/d/1W9nPHJa39Tkzzp37ZeRsEaSZjXMdFjH-rmIpimOKZKA/edit#slide=id.g61fd3b961b_0_1260
% \begin{center}\includegraphics[page = 1, width = \textwidth]{images/Outer_CV.pdf}\end{center}

% \begin{center}\includegraphics[page = 2]{images/Outer_CV.pdf}\end{center}

% \begin{center}\includegraphics[page = 3]{images/Outer_CV.pdf}\end{center}

% \begin{center}\includegraphics[page = 4]{images/Outer_CV.pdf}\end{center}

% \framebreak

\framebreak

\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.45\textwidth}
\hspace*{-0.3cm}
Assume we want to tune over a set of candidate HP configurations $\conf_i; i = 1, \dots$ with 4-fold CV in the inner resampling and 3-fold CV in the outer loop. The outer loop is visualized as the light green and dark green parts.

\column{0.5\textwidth}
\vspace*{-0.3cm}

% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = \textwidth]{images/Nested_Resampling.png}\end{center}
\end{columns}

\framebreak

\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.45\textwidth}
\hspace*{-0.3cm}
\begin{footnotesize}
In each iteration of the outer loop we:
\begin{itemize}
\item Split off the light green testing data
\item Run the tuner on the dark green part of the data, e.g.,
  evaluate each $\conf_i$ through fourfold CV on the dark green part
\item Return the winning $\conf^\star$ that performed best on the grey inner test sets
\item Re-train the model on the full outer dark green train set
\item Evaluate it on the outer light green test set
\end{itemize}
\end{footnotesize}

\column{0.5\textwidth}
\vspace*{-0.3cm}
% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = \textwidth]{images/Nested_Resampling.png}\end{center}
\end{columns}

\begin{footnotesize}
The error estimates on the outer samples (light green) are unbiased because this data was strictly excluded from the model-building process of the model that was tested on.
\end{footnotesize}

\end{frame}

\begin{frame}{Why is tuning so hard?}
\begin{itemize}
\item Tuning is derivative-free (black box problem): It is usually impossible to compute derivatives of the objective (i.e., the resampled performance measure) that we optimize with regard to the HPs. All we can do is evaluate the performance for a given hyperparameter configuration.
\item Every evaluation requires one or multiple train and predict steps of the learner. I.e., every evaluation is very \textbf{expensive}.
\item Even worse: the answer we get from that evaluation is \textbf{not exact, but stochastic} in most settings, as we use resampling (and often stochastic learners).
\item Categorical and dependent hyperparameters aggravate our difficulties: the space of hyperparameters we optimize over has a non-metric, complicated structure.
\end{itemize}

\end{frame}


\begin{frame}[containsverbatim,allowframebreaks]{Grid search}

\begin{columns}
\begin{column}{0.49\textwidth}
\begin{itemize}
\item Simple technique which is still quite popular, tries all
HP combinations on a multi-dimensional discretized grid
\item For each hyperparameter a finite set of candidates is predefined
\item Then, we simply search all possible combinations in arbitrary order
\end{itemize}
\end{column}
\begin{column}{0.49\textwidth}
\vspace*{-0.8cm}
\begin{center}
\begin{figure}
\includegraphics[width=0.9\textwidth]{images/grid.png}
\caption*{Grid search over 10x10 points}
\end{figure}
\end{center}
\end{column}
\end{columns}

\framebreak

\begin{blocki}{Advantages}
\item Very easy to implement
\item All parameter types possible
\item Parallelizing computation is trivial
\end{blocki}

\begin{blocki}{Disadvantages}
\item Scales badly: Combinatorial explosion
\item Inefficient: Searches large irrelevant areas
\item Low resolution in each dimension
\item Arbitrary: Which values / discretization?
\end{blocki}
\end{frame}


\begin{frame}[containsverbatim,allowframebreaks]{Random search}



\begin{columns}
\begin{column}{0.49\textwidth}
\begin{itemize}
\item Small variation of grid search
\item Uniformly sample from the region-of-interest
\end{itemize}
\end{column}
\begin{column}{0.49\textwidth}
\vspace*{-0.8cm}
\begin{center}
\begin{figure}
\includegraphics[width=0.9\textwidth]{images/random.png}
\caption*{Random search over 100 points}
\end{figure}
\end{center}
\end{column}
\end{columns}

\framebreak

\begin{blocki}{Advantages}
\item Like grid search: Very easy to implement, all parameter types possible, trivial parallelization
\item Anytime algorithm: Can stop the search whenever our budget for computation is exhausted, or continue until we reach our performance goal.
\item No discretization: each individual parameter is tried with a different value every time
\end{blocki}

\begin{blocki}{Disadvantages}
\item Inefficient: many evaluations in areas with low likelihood for improvement
\item Scales badly: high dimensional hyperparameter spaces need \emph{lots} of samples to cover.
\end{blocki}
\end{frame}





\begin{frame}[containsverbatim,allowframebreaks]{Evolutionary algorithms}

\textbf{Evolutionary algorithms} (EA) are a class of stochastic, metaheuristic optimization techniques whose mode of operation is inspired by the evolution of natural organisms.

\vspace{0.5cm}

\begin{center}
\begin{tabular}{ c | c }
\textbf{Definition} & \textbf{Correspondence} \\[0.05cm]
\hline \\[0.01cm]
solution candidate $\xx \in \XX$ & Chromosome of an individual \\[0.1cm]
$\xxi$& $i$-th gene of chromosome\\[0.1cm]
Set of solution candidates $\mathcal{P}$ & Population \\[0.1cm]
Objective function $\f: \XX \to \R$ & Fitness function
\end{tabular}
\end{center}

For hyperparameter optimization a solution candidate is a hyperparmeter configuration $\conf$ and the objective function is $\f(\xx) = \widehat{GE}_{\datasettest}\left(\inducer(\datasettrain, \xx)\right)$

\end{frame}

\begin{frame}[containsverbatim,allowframebreaks]{History of evolutionary algorithms}

Similar techniques differ in genetic representation:

\begin{itemize}
\item \textbf{Genetic algorithms}: uses binary problem representation, therefore closest to the biological model of evolution.
\item \textbf{Evolution strategies}: uses direct problem representation, e.g. vector of real numbers.
\item \textbf{Genetic programming}: create structures that convert an input into a fixed output (e.g. computer programs); solution candidates are represented as trees.
\item \textbf{Evolutionary programming}: similar to GP, but solution candidates are not represented by trees, but by finite state machines.
\end{itemize}

The boundaries between the terms become increasingly blurred and are often used synonymously.

\end{frame}


\begin{frame}{Structure of an evolutionary algorithm}

\begin{center}
\begin{figure}
\centering
\begin{tikzpicture}[node distance=0.8cm, auto,]
%nodes
\node (init) {Initialize population};
\node[below = 0.3cm of init](rating1) {Rate population};
\node[below = 0.3cm of rating1](selection1) {Parent selection};
\node[below = 0.3cm of selection1](variation) {Variation};
\node[below = 0.3cm of variation](rating2) {Rate offspring};
\node[below = 0.3cm of rating2](selection2) {Survival selection};
\node[below = 0.3cm of selection2](stop) {Stop};
\node[below = 1cm of stop](dummy2) {};
\node[below = 0.2cm of stop](dummy3) {};
\node[right = 0.01cm of dummy3](dummy4) {yes};
\node[left = 1.1cm of rating2](dummy1) {no};
\draw[->] (init) to (rating1) node[midway, above]{};
\draw[->] (rating1) to (selection1) node[midway, above]{};
\draw[->] (selection1) to (variation) node[midway, above]{};
\draw[->] (variation) to (rating2) node[midway, above]{};
\draw[->] (rating2) to (selection2) node[midway, above]{};
\draw[->] (selection2) to (stop) node[midway, above]{};
\draw[->] (stop) to (dummy2) node[midway, above]{};
\draw[->] (stop) to [bend left=90, looseness=2](selection1) node[midway, above]{};
\end{tikzpicture}
\end{figure}
\end{center}

\end{frame}

\begin{frame}[allowframebreaks]{Example of an evolutionary algorithm}

In the following, methods for the individual steps of an evolutionary algorithm are presented.

\vspace{0.5cm}

These are demonstrated using the one-dimensional Ackley function, which we want to optimize on the $[-30, 30]$ interval.

\vspace{0.5cm}

In this case, each individual has exactly one chromosome. The chromosome is (obviously) encoded as a real number: $\xx_i \in \R$.

Usually for the optimization of a function $\f:\R^n \to \R$ individuals are coded as real vectors $\xx_i \in \R^n$.


\framebreak


\begin{center}
\begin{figure}
\includegraphics[width=\textwidth, height=6cm]{images/ea_ex1.png}
\end{figure}
\end{center}

\end{frame}


\begin{frame}{Step 1: Initialize population}

We start with a randomly selected population $\mathcal{P} = \{\xx_1, ..., \xx_\mu\}$ of the size $\mu = 20$ and rate it. The fitness function in this case is the function we want to minimize.


\begin{center}
\begin{figure}
\includegraphics[width=\textwidth, height=6cm]{images/ea_ex2.png}
\end{figure}
\end{center}


\end{frame}

\begin{frame}[containsverbatim,allowframebreaks]{Step 2: Parent selection}


In the first step of an iteration, $\lambda$ parents are chosen, who create offspring in the next step. 

\begin{footnotesize}
    \textbf{Note:} Do not confuse the hyperparameters setting $\conf$ (bold) with the EA parameter $\lambda$.    
\end{footnotesize}

\begin{columns}
\begin{column}{0.5\textwidth}
  
  Possibilities for selection of parents:

  \begin{itemize}
    \item \textbf{Neutral selection: }choose individual with a probability $1/\mu$.
    \item \textbf{Fitness-proportional selection: }draw individuals with probability proportional to their fitness.
  \end{itemize}
\end{column}%
\begin{column}{0.5\textwidth}
  \begin{itemize}
    \item \textbf{Tournament Selection: }randomly select $k$ individuals for a "Tournament Group". Of the drawn individuals, the best one (with the highest fitness value) is then chosen. In order to draw $\lambda$ individuals, the procedure must be performed $\lambda$-times.
  \end{itemize}

  \begin{figure}
    \includegraphics[width = \linewidth]{images/tournament_selection.png}
  \end{figure}

\end{column}
\end{columns}

\framebreak

In our example we choose $\lambda = 5$ offspring by neutral selection (red individuals).

\begin{center}
\begin{figure}
\includegraphics[width=\textwidth, height=5cm]{images/ea_ex3.png}
\end{figure}
\end{center}

\end{frame}

\begin{frame}{Step 3: Variation}

New individuals are now generated from the parent population. This is done by

\begin{itemize}
\item Recombination: combine two parents into one offspring.
\item Mutation: change an individual.
\end{itemize}

Recombination and mutation are not always performed both, sometimes only recombination or mutation is performed.

\end{frame}

\begin{frame}{Recombination for numeric representations}

Two individuals $\xx, \tilde\xx \in \R^n$ in numerical representation can be recombined as follows:

\begin{itemize}
\item \textbf{Uniform crossover}: choose gene $i$ with probability $p$ of 1st parent and probability $1-p$ of 2nd parent.
\item \textbf{Intermediate recombination}: new individual is created from the mean value of two parents $\frac{1}{2}(\xx + \tilde\xx)$
\item \textbf{Simulated Binary Crossover (SBX)}: generate \textbf{two offspring} based on

$$
\bar\xx \pm \frac{1}{2} \beta (\tilde\xx - \xx)
$$

with $\bar\xx = \frac{1}{2} (\xx + \tilde\xx)$.
\end{itemize}

\end{frame}

\begin{frame}{Recombination for bit strings}

  For example, two individuals $\xx, \tilde\xx \in \{0, 1\}^n$ encoded as bit strings can be recombined as follows:

  \begin{itemize}
  \item \textbf{1-point crossover}: select crossover $k \in \{1, ..., n - 1\}$ randomly and select the first $k$ bits from 1st parent, the last $n-k$ bits from 2nd parent.

  \footnotesize
  \begin{center}
  \begin{tabular}{c @{\hspace{2\tabcolsep}} *{6}{c}}
   " " & \textcolor{red}{1} & \textcolor{blue}{1} & " " & " " & "  " & \textcolor{red}{1}  \\
   " " & \textcolor{red}{0} & \textcolor{blue}{0} & " " & " " & "  " & \textcolor{red}{0}  \\ \cmidrule{2-3}
   " " & \textcolor{red}{0} & \textcolor{blue}{1} & " " &$\Rightarrow$ & "  " & \textcolor{blue}{1}  \\
   " " & \textcolor{red}{1} & \textcolor{blue}{1} & " " & " " & "  " & \textcolor{blue}{1}  \\
   " " & \textcolor{red}{1} & \textcolor{blue}{0} & " " & " " & "  " & \textcolor{blue}{0}
  \end{tabular}
  \end{center}
  \normalsize

  \item \textbf{Uniform crossover}: select bit $i$ with probability $p$ of 1st parent and probability $1-p$ of 2nd parent.

  \end{itemize}


  % \footnotesize
  % \begin{center}
  % \begin{tabular}{c @{\hspace{2\tabcolsep}} *{6}{c}}
  %  " " & \textcolor{red}{1} & \textcolor{blue}{1} & " " & " " & "  " & \textcolor{red}{1}  \\
  %  " " & \textcolor{red}{0} & \textcolor{blue}{0} & " " & " " & "  " & \textcolor{blue}{0}  \\
  %  " " & \textcolor{red}{0} & \textcolor{blue}{1} & " " &$\Rightarrow$ & "  " & \textcolor{blue}{1}  \\
  %  " " & \textcolor{red}{1} & \textcolor{blue}{1} & " " & " " & "  " & \textcolor{blue}{1}  \\
  %  " " & \textcolor{red}{1} & \textcolor{blue}{0} & " " & " " & "  " & \textcolor{red}{1}
  % \end{tabular}
  % \end{center}
  % \normalsize

  \end{frame}
\begin{frame}[containsverbatim,allowframebreaks]{Mutation for numeric representations}
  \textbf{Mutation:} individuals are changed, for example for $\xx \in \R^n$
  \begin{itemize}
  \item \textbf{Uniform mutation:} choose a random gene $\xxi$ and replace it with a value uniformly distributed (within the feasible range).
  \item \textbf{Gauss mutation}: $\tilde\xx = \xx \pm \sigma \mathcal{N}(0, \boldsymbol{I})$
  \item \textbf{Polynomial mutation:} polynomial distribution instead of normal distribution
  \begin{center}
  \begin{figure}
    \includegraphics[height = 3.5cm, width = 4cm]{images/polynomial_mutation.png}\\
    \scriptsize{Source: K. Deb, Analysing mutation schemes for real-parameter genetic algorithms, 2014}
  \end{figure}
   \end{center}
   \framebreak
  More exact:
  $$
  {\tilde \xxi} = \xxi + (\xxi[i,\text{upper}] - \xxi[i,\text{lower}]) \delta_{i}
  $$
  with $\xxi[i,\text{upper}]$ ($\xxi[i,\text{lower}]$) as upper (lower) bound for $\xxi$.
  $\delta_{i}$ results as:
  \footnotesize
  $$
  \delta_{i} =
  \begin{cases}
  [2r_{i}+(1-2r_{i})(1-\delta)^{\eta_{m}+1}]^{\frac{1}{\eta +1}} -1, & r_{i} < 0.5 \\
  1 - [2(1-r_{i})+2(r_{i}-\frac{1}{2})(1-\delta)^{\eta_{m}+1}]^{\frac{1}{\eta_{m} +1}}, &  \text{else.}
  \end{cases}
  $$
  with  $\delta = \frac{min\{(x_{i} - \xxi[i,\text{upper}]), (\xxi[i,\text{upper}]-\xxi)\}}{\xxi[i,\text{upper}] - \xxi[i,\text{upper}]}$.
  \normalsize
  \vspace{0.5cm}
  Here $r_{i} \in [0,1]$ is a uniformly distributed number, $\eta_{m}$ is the distribution index of the mutation and is chosen by the user.\\
  % Remark: A $\eta_{m}$ of the order of $\eta_{m} \in [20,100]$ is common.
  \normalsize
  \end{itemize}
  \vspace{0.5cm}
  In our example, we have chosen a Gauss mutation with $\sigma = 2$, we do not apply a recombination.
  \begin{center}
    \begin{figure}
    \includegraphics[width=\textwidth, height=5cm]{images/ea_ex4.png}
    \end{figure}
    \end{center}
  \end{frame}
  \begin{frame}{Mutation for bit strings}
  For example, an individual $\xx \in \{0, 1\}^n$ encoded as a bit string can be mutated as follows:
  \vspace{0.5cm}
  \textbf{Mutation:}
  \begin{itemize}
  \item \textbf{Bitflip}: for each index $k \in \{1, ..., n\}$: bit $k$ is flipped with probability $p \in (0,1)$.
  \item If $(a)$ is an arbitrary bit sequence to which a bitflip mutation is applied, $(b)$ is obtained.
  \end{itemize}
  \footnotesize
  \begin{center}
  \begin{tabular}{c @{\hspace{2\tabcolsep}} *{5}{c}}
    &
    \itshape (a) &
    \itshape " " &
    \itshape " " &
    \itshape " " &
    \itshape (b)
  \\[1ex]
  " " & 1 & " " & " " & "  " & \textcolor{red}{0}  \\
  " " & 0 & " " & " " & "  " & 0  \\
  " " & 0 & " " & $\Rightarrow$ & "  " & \textcolor{red}{1}  \\
  " " & 1 & " " & " " & "  " & \textcolor{red}{0}  \\
  " " & 1 & " " & " " & "  " & 1
  \end{tabular}
  \end{center}
  \normalsize
  \end{frame}
  \begin{frame}[allowframebreaks]{Step 4: Survival selection}
  Now individuals are chosen who survive. Two common strategies are:
  \begin{itemize}
  \item \textbf{$(\mu, \lambda)$-selection}: we select from the $\lambda$ descendants the $\mu$ best ($\lambda \ge \mu$ necessary).
  \textbf{But:} best overall individual can get lost!
  \item \textbf{$(\mu + \lambda)$-selection}: $\mu$ parents and $\lambda$ offspring are lumped together and the $\mu$ best individuals are chosen.
  Best individual safely survives.
  \end{itemize}

  \framebreak

  In our example, a $(\mu + \lambda)$ selection was used. The selected points are green.

  \begin{center}
    \begin{figure}
      \includegraphics[width=\textwidth, height=5cm]{images/ea_ex5.png}
    \end{figure}
  \end{center}

\end{frame}

\begin{frame}{Evolutionary Algorithms}

  \begin{blocki}{Advantages}
    \item Can solve a variety of optimization problems (including HPO)
    \item All parameter types possible
    \item Highly parallelizable
    \item Conceptually simple, yet powerful enough to solve complex problems
    \end{blocki}

    \begin{blocki}{Disadvantages}
    \item Stagnation: Optimization process does not progress any more %FIXME: JR: Isn't that the same as the point below?
    \item Premature Convergence: Algorithm converges to a single solution, which is not as good as expected
    \item Diversity of population structures: Loss of population diversity for solving complex optimization problems
    \item Lacks balance between exploration and exploitation
    \end{blocki}

\end{frame}

\begin{frame}{More Tuning Algorithms}

There are many more methods for hyperparmeter tuning, some of which will be discussed in later parts of this lecture:


\begin{itemize}
\item Stochastic local search, e.g. simulated annealing
\item Bayesian optimization
\item Hyperband
\item Iterated F-Racing
\item many more $\dots$
\end{itemize}

\end{frame}


\begin{frame}{Expert Knowledge}

\begin{itemize}
	\item Expert knowledge can help to guide hyperparameter optimization
	\item For example, some hyperparameters might not be sampled uniformly
\end{itemize}


Example: regularization hyperparameter ($C$ or \emph{cost}) of SVM: $[0.001, 1000.0]$

\begin{itemize}
	\item The distance between $999.9$ and $1000.0$ should not be the same as between $0.1$ and $0.2$.
  \item We might want to sample here from from a log-scale, e.g., $[10^{\lambda_l}, 10^{\lambda_u}]$ with $\lambda_l = -3$ and $\lambda_u = 3$.
\end{itemize}

\begin{figure}[htb]
\centering
  \begin{tikzpicture}[auto]%[scale=1.5]
    \draw [->](-0.3,0)-- (12.3,0) coordinate;
    \draw [->](-0.3,-2)-- (12.3,-2) coordinate;
    \foreach \x/\xtext/\xxtext in {-3/-3/0.001, -2/-2/ , -1/-1/, 0/0/, 1/1/, 2/2/100, 3/3/1000} {
      \draw [very thick] ({\x*2+6},-2pt) -- ++(0, 4pt) node[xshift = -6pt, yshift=-3pt,anchor=south west,baseline]{\strut$\xtext$};
      \draw [very thick] ({10^(\x)*0.012},-2cm+2pt) -- ++(0,-4pt) node[anchor=north]{$\xxtext$};
      \draw [->] ({\x*2+6},-2pt) .. controls ({\x*2+6},-0.5) and ({10^(\x)*0.012},-1.5) .. ({10^(\x)*0.012},-2cm+2pt);
    }
    \node[] at (-0.7,-0.1) (t1) {$\lambda$};
    \node[] at (-0.7,-1.9) (t2) {$10^{\lambda}$};
  \end{tikzpicture}
\end{figure}


\end{frame}
\begin{frame}{Tuning Example}
Tuning $(\text{cost},\text{gamma}) \in [10^{-3},10^{3}]^2$ for the \emph{SVM} with \emph{random search}, \emph{grid search} and \emph{CMAES}\footnote{EA that samples offspring from a multi-variate normal distribution. For details see: \url{https://arxiv.org/pdf/1604.00772.pdf}} using a 5-fold CV on the \texttt{spam} and \texttt{sonar} data set for AUC:
\begin{columns}
\begin{column}{0.3\textwidth}
  \vspace{1em}
  % \resizebox{\linewidth}{!}{
  %   \begin{tabular}{l|l|l|l}
  %   Parameter&Type & Min & Max \\
  %   \hline
  %   \texttt{cost}  & double & $10^{-3}$ & $10^{3}$ \\
  %   \texttt{gamma} & double & $10^{-3}$ & $10^{3}$ \\
  %   \end{tabular}
  % }
  \footnotesize

  We notice here:
  
  \begin{itemize}
    \item \emph{Grid search} has many evaluations with bad performance (\emph{gamma}$>1$).
    \item \emph{Random search} can lead to unexplored areas even in promising regions. 
    \item \emph{CMAES} can run into local minimum.
  \end{itemize}
\end{column}%
\begin{column}{0.7\textwidth}
  \vspace{-1em}
  \begin{figure}
  \includegraphics[width=\textwidth]{images/benchmark_scatter.png}
  \end{figure}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Tuning Example (cont.)}
The \emph{optimization curve} shows the best found configuration until a given time point.
\begin{columns}
\begin{column}{0.3\textwidth}
  \footnotesize
  \only<1>{
    
    Note:

    \begin{itemize}
      \item For \emph{random search} and \emph{grid search} the chronological order on the x-axis is arbitrary.
      \item The curve shows the performance on the tuning validation (\emph{inner resampling}).
      \item This curve only shows the progress on one outer training set, but with a 10-fold CV for outer resampling we have 10 outer training sets\ldots
    \end{itemize}
  }
  \only<2-3>{
    \begin{itemize}
      \item<2-> The outer 10-fold CV gives us 10 optimization curves.
      \item<3-> The median at each time point gives us an estimate of the average optimization progress.
    \end{itemize}
  }
  \only<4>{
    \begin{itemize}
      \item Remember: The final model will be trained on the \emph{outer training set} with the configuration $\conf^\star$ that lead to the best performance on the \emph{inner test set}.
      \item To compare the effectiveness of the tuning strategies we have to look at the performance that $\conf^\star$ gives us on the \emph{outer test set}.
    \end{itemize}
  }
\end{column}
\begin{column}{0.7\textwidth}
  \begin{center}
  \begin{figure}
  \only<1>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_1.png}}
  \only<2>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_all.png}}
  \only<3>{\includegraphics[width=\textwidth]{images/benchmark_curve_median.png}}
  \only<4>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_all_median.png}}
  \end{figure}
  \end{center}
\end{column}
\end{columns}
  

  %   \footnotesize
  %     \item Tuning does not necessarily improve the performance of the learner, because e.g.\ tuning is badly configured or default values are determined by \emph{clever} heuristics.
  %     \item Tuning error can be overly optimistic (see \emph{nested resampling}).
  %   \end{itemize}
  % }
  % \only<2>{
  %   \begin{itemize}
  %     \item Effect of the chosen resampling split (objective noise) can dominate tuning effect.
  %   \end{itemize}
  % }
\end{frame}

\begin{frame}{Tuning Example: Validation}

\begin{columns}
\begin{column}{0.3\textwidth}
  \footnotesize

  The box plots show the distribution of the AUC-values that were measured on the \emph{outer test set} through the 10-fold CV.

  Note:

  \begin{itemize}
    \item The box plots do not indicate significant differences\footnotemark.
    \item The performance differs from the results obtained on the inner resampling.
  \end{itemize}

\end{column}
\begin{column}{0.7\textwidth}
  \begin{center}
  \begin{figure}
  \includegraphics[width=\textwidth]{images/benchmark_boxplot_tuners.png}
  \end{figure}
  \end{center}
\end{column}
\end{columns}
\footnotetext[2]{Box plots hide that the measured values are paired (because outer resampling splits are identical). Practical hints for testing learners, see Demšar, Statistical Comparisons of Classifiers over Multiple Data Sets, 2006 }
\end{frame}

\begin{frame}{Tuning Example: Validation}

\begin{columns}
\begin{column}{0.3\textwidth}
  \footnotesize

  \begin{itemize}
    \item<1-> Comparison of \textit{self-tuning} \emph{SVMs} with an SVM that was configured with $\conf = (\text{cost},\text{gamma}) = (1,1)$ shows that tuning is necessary.
    \item<2-> However, some \emph{SVM} implementations come with \emph{clever} heuristics (here: \texttt{R}-package \texttt{e1071}).
  \end{itemize}
\end{column}
\begin{column}{0.7\textwidth}
  \begin{center}
  \begin{figure}
  \only<1>{\includegraphics[width=\textwidth]{images/benchmark_boxplot_default.png}}
  \only<2>{\includegraphics[width=\textwidth]{images/benchmark_boxplot_all.png}}
  \end{figure}
  \end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[allowframebreaks]{Tuning: Further Practical Hints}
\begin{itemize}
  \item Tuning can lead to overfitting.
  \item Often the optimal hyperparameter setting correlates with the size of the training data. Therefore the optimal setting on the \emph{inner resampling} data does not have to be optimal for the \emph{outer resampling}.
  \item For small data sets the relative size of the \emph{inner training set} should not differ much from \emph{outer training set}. Example: $n = 200$, outer resampling: 5-fold CV, inner resampling: 3-fold CV: $n_{\text{inner train}} = \frac{4}{5} \cdot {2}{3} \cdot n = 0.53 * n = 107$ vs.\ inner and outer resampling: 10-fold CV: $\frac{9}{10} \cdot \frac{9}{10} \cdot n = 0.81 * n = 162$ 
  \item Feature selection and preprocessing should also be considered as tuning and has to be validated accordingly.
  \item If $\conf^\star$ is close to the border of $\Lambda$, consider log-scales or widening the search space.
  \item Resampling strategies depend on dataset sizes:
  \begin{itemize}
    \item Small datasets: More resampling iterations necessary to obtain reliable performance estimate.
    \item Large datasets: Less resampling iterations possible due to runtime but holdout can be sufficient to estimate performance.
    \item Very large datasets: Use special tuning algorithms that adapt training set size. (e.g.\ \emph{Hyperband})
  \end{itemize}
  \item For unbalanced and multi-class datasets $n$ has to be higher to obtain reliable performance estimates.
  \item \emph{Grid search} might work well for $\Lambda \in \mathbb{R}^2$ but becomes infeasible for higher dimensions.
  \item The objective function that results from resampling the learner with a given input hyperparameter setting $\conf$ can become stochastic if 1) the learner is stochastic (e.g.\ random forest) 2) each time different splits are generated. If the optimizer does not respect that you run the risk to select $\conf^\ast$ that performed best by chance.
  \item The optimal tuning strategy also depends on the available hardware  resources: A random search parallelized on 64 cores is probably more efficient then a smart optimizer that does not parallelize.
  \item Some learners can crash for certain hyperparameter configurations. The tuning process should not fail and lose all progress due to the crash.
  \item For many real-world applications you want to optimize multiple criteria (e.g.\ sensitivity and specificity). This will be topic in a later lecture.
  \item The hyperparameter setting can affect the runtime of the training. Sometimes it might be beneficial to automatically terminate the training process if it takes too long during tuning.
  \item To chose the correct tuning method you should check the following points:
  \begin{itemize}
    \item Is my problem stochastic? (Most likely it is.) Does the tuner support stochastic problems?
    \item What type is my search-space?
      \begin{itemize}
        \item Purely real-valued?
        \item Integer only?
        \item Purely categorical?
        \item Mixed?
        \item Hierarchical?
      \end{itemize}
    \item Can the learner make use of parallelization internally? In this case it is often preferable to parallelize the learner instead of the tuning.
  \end{itemize}
\end{itemize}
\end{frame}

\end{document}
