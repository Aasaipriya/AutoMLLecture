
\input{../latex_main/main.tex}

\newcommand{\inducer}{\mathcal{I}}
\newcommand{\R}{\mathds{R}}

\newenvironment{blocki}[1] % itemize block
{
 \begin{block}{#1}\begin{itemize}
}
{
\end{itemize}\end{block}
}

\title[AutoML: Hyperparameter Optimization]{AutoML: Hyperparameter Optimization}
\subtitle{Overview for this Week}
%TODO: change authors!
\author[Bernd Bischl]{\underline{Bernd Bischl} \and Frank Hutter \and Lars Kotthoff \and Marius Lindauer}
\institute{}
\date{}



% \AtBeginSection[] % Do nothing for \section*
% {
% \begin{frame}{Outline}
% \bigskip
% \vfill
% \tableofcontents[currentsection]
% \end{frame}
% }

\begin{document}

\maketitle


%----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}[containsverbatim,allowframebreaks]{Motivating Example}

\begin{itemize}
\item Given a dataset, we want to train a classification tree.
\item We feel that a maximum tree depth of $4$ has worked out well for us previously, so we decide to set this hyperparameter to $4$.
\item The learner ("inducer") $\inducer$ takes the input data, internally performs \textbf{empirical risk minimization}, and returns a fitted tree model $\fh(\mathbf{x}) = f(\x, \hat{\theta})$ of at most depth $\lambda = 4$ that minimizes the empirical risk.
\end{itemize}

% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\begin{center}
\begin{figure}
\includegraphics[width=0.5\textwidth]{images/riskmin_bilevel1.png}
\end{figure}
\end{center}

\framebreak

\begin{itemize}
\item We are \textbf{actually} interested in the \textbf{generalization performance} $GE\left(\fh\right)$ of the estimated model on new, previously unseen data.
\item We estimate the generalization performance by evaluating the model $\fh$ on a test set $\datasettest$: $$
\widehat{GE}_{\datasettest}\left(\fh\right) = \frac{1}{|\datasettest|} \sum\limits_{(\x, y) \in \datasettest} L\left(y, \fh(\mathbf{x}) \right)
$$
\end{itemize}
\vspace*{-0.6cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\begin{center}
\begin{figure}
\includegraphics[width=0.5\textwidth]{images/riskmin_bilevel2.png}
\end{figure}
\end{center}

\framebreak

\begin{itemize}
\item But many ML algorithms are sensitive w.r.t. a good setting of their hyperparameters,
and generalization performance might be bad, if we have chosen a suboptimal configuration:
\begin{itemize}
\item The data may be too complex to be modeled by a tree of depth $4$
\item The data may be much simpler than we thought, and a tree of depth $4$ overfits
\end{itemize}
\item[$\implies$] algorithmically try out different values for the tree depth. For each maximal depth $\lambda$, we have to train the model \textbf{to completion} and evaluate its performance on the test set.
\item We choose the tree depth $\conf$ that is \textbf{optimal} w.r.t. the generalization error of the model.
\end{itemize}

% $\to$ Finding the hyperparameter $\lambda$ that is optimal w.r.t. the generalization performance constitutes an optimization problem.

\end{frame}

% \begin{frame}[containsverbatim,allowframebreaks]{The role of hyperparameters}

% \begin{itemize}
% \item Hyperparameters often control the complexity of a model, i.e., how flexible the model is.
% \item But they can in principle influence any structural property of a model or computational part of the training process.
% \item If a model is not flexible enough, it cannot approximate the relationship between the features and the output well and will underfit.
% \item If a model is too flexible so that it simply \enquote{memorizes} the training data, we will face the dreaded problem of overfitting.
% \item[$\implies$] Hence, controlling the model's capacity, i.e., finding suitable hyperparameters,
% can prevent overfitting the model on the training set.
% \end{itemize}

% \end{frame}

\begin{frame}[containsverbatim,allowframebreaks]{Model Parameters vs. Hyperparameters}

It is critical to understand the difference between model parameters and hyperparameters.

\vspace{0.5cm}

\textbf{Model parameters} are optimized during training, typically via loss minimization. They are an \textbf{output} of the training. Examples:
\begin{itemize}
\item The splits and terminal node constants of a tree learner
\item Coefficients $\theta$ of a linear model $\fx = \theta^\top\x$
\end{itemize}

\framebreak

In contrast, \textbf{hyperparameters} (HPs) are not decided during training. They must be specified before the training, they are an \textbf{input} of the training.
Hyperparameters often control the complexity of a model, i.e., how flexible the model is.
But they can in principle influence any structural property of a model or computational part of the training process.

\vspace{0.5cm}

Examples:

\begin{itemize}
\item The maximum depth of a tree
\item $k$ and which distance measure to use for $k$-NN
\item the number and maximal order of interactions to be included in a linear regression model
\end{itemize}

\end{frame}


\begin{frame}[containsverbatim,allowframebreaks]{Types of hyperparameters}

We summarize all hyperparameters we want to tune over in a vector $\conf \in \Lambda$ of (possibly) mixed type. HPs can have different types:

\begin{itemize}
\item Real-valued parameters, e.g.:
\begin{itemize}
\item Minimal error improvement in a tree to accept a split
\item Bandwidths of the kernel density estimates for Naive Bayes
\end{itemize}
\item Integer parameters, e.g.:
\begin{itemize}
\item Neighborhood size $k$ for $k$-NN
\item $mtry$ in a random forest
\end{itemize}
\item Categorical parameters, e.g.:
\begin{itemize}
\item Which split criterion for classification trees?
\item Which distance measure for $k$-NN?
\end{itemize}
\end{itemize}

Hyperparameters are often \textbf{hierarchically dependent} on each other, e.g., \emph{if} we use
a kernel-density estimate for Naive Bayes, what is its width?
% with polynomials of the features up to a certain maximal degree $d$, then
% we must specify whether to also include polynomial interaction terms like e.g. $x_j^{d-d'}x_m^{d'}$ or not
% and up to which degree $d' \leq d$.
\end{frame}

\begin{frame}{Tuning}

\vskip 3em
Recall: \textbf{Hyperparameters} $\conf$ are parameters that are \emph{inputs} to the
training problem, in which a learner $\inducer$ minimizes the empirical risk on a training data set in order
to find optimal \textbf{model parameters} $\theta$ which define the fitted model $\fh$.
\vskip 2em

\textbf{(Hyperparameter) Tuning} is the process of finding good model hyperparameters $\conf$.

% \begin{frame}[containsverbatim,allowframebreaks]{{Hyperparameter Tuning}
% \begin{itemize}
% \item Optimize hyperparameters for learner w.r.t. prediction error
% Tuner proposes configuration, eval by resampling, tuner receives performance, iterate
% \end{itemize}
% \begin{columns}[c, onlytextwidth]
% \column{0.45\textwidth}
% FIGURE SOURCE: No source
% \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=1.2\textwidth]{images/chain.jpg}
% \column{0.45\textwidth}
% FIGURE SOURCE: https://drive.google.com/open?id=1wY3aUZxIMZPje3vR0t2yWiDMx_osXRCi
% \includegraphics[trim={1cm 0cm 1cm 0cm}, clip, width=1.2\textwidth]{images/tuning_process.jpg}
% \end{columns}

% \end{frame}

\end{frame}


\begin{frame}[containsverbatim,allowframebreaks]{Tuning: A bi-level optimization problem}

\vspace{0.2cm}

We face a \textbf{bi-level} optimization problem: The well-known risk minimization problem to find $\hat f$ is \textbf{nested} within the outer hyperparameter optimization (also called second-level problem):

\begin{center}
\begin{figure}
% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\includegraphics[width=0.7\textwidth]{images/riskmin_bilevel3.png}
\end{figure}
\end{center}

\framebreak

\begin{itemize}
\item For a learning algorithm $\inducer$ (also inducer) with $d$ hyperparameters, the hyperparameter \textbf{configuration space} is:
$$\Lambda=\Lambda_{1} \times \Lambda_{2} \times \ldots \Lambda_{d}$$
where $\Lambda_{i}$ is the domain of the $i$-th hyperparameter.
\item The domains can be continuous, discrete or categorical.
\item For practical reasons, the domain of a continuous or integer-valued hyperparameter is typically bounded.
\item A vector in this configuration space is denoted as $\conf \in \Lambda$.
\item A learning algorithm $\inducer$ takes a (training) dataset $\dataset$ and a hyperparameter configuration $\conf \in \Lambda$ and returns a trained model (through risk minimization)

\vspace*{-0.2cm}
\begin{eqnarray*}
\inducer: \left(\mathcal{X} \times \mathcal{Y}\right)^n \times \Lambda &\to& \mathcal{H} \\
(\dataset, \Lambda) &\mapsto& \inducer(\dataset, \Lambda) = \hat f_{\dataset, \Lambda}
\end{eqnarray*}
% \item Additionally, some hyperparameters may only need to be specified if another hyperparameter (or combination of hyperparameters) takes on a certain value.
\end{itemize}

% \vspace{0.5cm}

% Note that

% In contrast to the first-level (empirical) risk minimization problem, hyperparameter optimization is also referred to as \textbf{second-level} optimization. The first-level problem can be seen as a subroutine called by the second-level problem: Each evaluation of $\Lambda$ requires to solve the first-level optimization problem.


% \framebreak

% \begin{itemize}
% \item search for the \textbf{inducer} hyperparameter $\Lambda$
% \item that minimizes the \textbf{generalization error}
% $$
% \min_{\Lambda} \E_{\D_n \sim \Pxy, (\xv, y) \sim \Pxy} \left(V\left(y, \hat f_{\D, \Lambda}(\xv)\right)\right).
% $$
% \end{itemize}

% We compare: In empirical risk minimization, we

% \begin{itemize}
% \item search for the \textbf{model} parameter $\thetab$
% \item that minimizes the \textbf{empirical risk}
% $$
% \min_{\thetab} \sum_{(\xi, \yi) \in \datasettrain} L\left(\yi, \fxi\right).
% $$
% \end{itemize}

% In hyperparameter optimization, we

% \begin{itemize}
% \item search for the \textbf{inducer} hyperparameter $\Lambda$
% \item that minimizes the \textbf{test error}
% $$
% \min_{\Lambda \in \Lambda} \sum_{(\xi, \yi) \in \datasettest} V\left(\inducer(\datasettrain, \Lambda)(\xi), \yi\right).
% $$
% \end{itemize}

% \framebreak

% \framebreak

% The hyperparameter optimization problem is difficult in many ways:

\framebreak

We formally state the nested hyperparameter tuning problem as:

$$
\min_{\conf \in \Lambda} \widehat{GE}_{\datasettest}\left(\inducer(\datasettrain, \Lambda)\right)
$$

\begin{itemize}
\item The learner $\inducer(\datasettrain, \conf)$ takes a training dataset as well as hyperparameter settings $\Lambda$ (e.g. the maximal depth of a classification tree) as an input.
\item $\inducer(\datasettrain, \conf)$ performs empirical risk minimization on the training data and returns the optimal model $\hat f$ for the given hyperparameters.
\item Note that for the estimation of the generalization error, more sophisticated resampling strategies like cross-validation can be used.
\end{itemize}

\framebreak

The components of a tuning problem are:

\begin{itemize}
\item The dataset
\item The learner (possibly: several competing learners?) that is tuned %(e.g. a decision tree classifier)
\item The learner's hyperparameters and their respective regions-of-interest over which we optimize % (e.g. $\texttt{tree depth} \in \{1, 2, ..., 20\}$)
\item The performance measure, as determined by the application.\\ Not necessarily identical to the loss function that defines the risk minimization problem for the learner!\\
% We could even be interested in multiple measures simultaneously, e.g., accuracy and computation time of our model, TPR and PPV, etc.
\item A (resampling) procedure for estimating the predictive performance.
 % The expected performance on unseen data can be estimated by holdout (i.e., a single train-test-split) or more advanced techniques like cross-validation.
% More on this later.
\end{itemize}

% \framebreak

% \begin{center}
% \begin{figure}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
% \includegraphics[width=1.2\textwidth]{images/autotune_in_model_fit.pdf}
% \end{figure}
% \end{center}

\end{frame}




% \framebreak

% Possible scenarios for finding default hyperparameters:

% \begin{itemize}
% \item If the learner's performance is fairly insensitive to changes of a hyperparameter, we don't really have to worry as long as we remain within the range of reasonable values.
% \item Constant default: we can benchmark the learner across a broad range of data sets and scenarios and try to find hyperparameter values that work well in many different situations. Quite optimistic?
% \item Dynamic (heuristic) default: We can benchmark the learner across a broad range of data sets and scenarios and try to find an easily computable function that sets the hyperparameter in a data dependent way,
% e.g. using \texttt{mtry}$ = p/3$ for RF.\\
% How to construct or learn that heuristic function, though...?
% \item In some cases, can try to set hyperparameters optimally by extracting more info from the fitted model. E.g. \texttt{ntrees} for a random forest (does OOB error increase or decrease if you remove trees from the ensemble?).
% \end{itemize}
% \end{frame}









\begin{frame}[containsverbatim,allowframebreaks]{Why is tuning so hard?}
\begin{itemize}
\item Tuning is derivative-free (black box problem): It is usually impossible to compute derivatives of the objective (i.e., the resampled performance measure) that we optimize with regard to the HPs. All we can do is evaluate the performance for a given hyperparameter configuration.
\item Every evaluation requires one or multiple train and predict steps of the learner. I.e., every evaluation is very \textbf{expensive}.
\item Even worse: the answer we get from that evaluation is \textbf{not exact, but stochastic} in most settings, as we use resampling.
\item Categorical and dependent hyperparameters aggravate our difficulties: the space of hyperparameters we optimize over has a non-metric, complicated structure.
\end{itemize}

\end{frame}


\begin{frame}[containsverbatim,allowframebreaks]{Grid search}

\begin{columns}
\begin{column}{0.49\textwidth}
\begin{itemize}
\item Simple technique which is still quite popular, tries all
HP combinations on a multi-dimensional discretized grid
\item For each hyperparameter a finite set of candidates is predefined
\item Then, we simply search all possible combinations in arbitrary order
\end{itemize}
\end{column}
\begin{column}{0.49\textwidth}
\begin{center}
\begin{figure}
\includegraphics[width=0.8\textwidth]{images/grid.png}
\caption*{Grid search over 10x10 points}
\end{figure}
\end{center}
\end{column}
\end{columns}

\framebreak

\begin{blocki}{Advantages}
\item Very easy to implement
\item All parameter types possible
\item Parallelizing computation is trivial
\end{blocki}

\begin{blocki}{Disadvantages}
\item Scales badly: Combinatorial explosion
\item Inefficient: Searches large irrelevant areas
\item Arbitrary: Which values / discretization?
\end{blocki}
\end{frame}


\begin{frame}[containsverbatim,allowframebreaks]{Random search}



\begin{columns}
\begin{column}{0.49\textwidth}
\begin{itemize}
\item Small variation of grid search
\item Uniformly sample from the region-of-interest
\end{itemize}
\end{column}
\begin{column}{0.49\textwidth}
\begin{center}
\begin{figure}
\includegraphics[width=0.8\textwidth]{images/random.png}
\caption*{Random search over 100 points}
\end{figure}
\end{center}
\end{column}
\end{columns}

\framebreak

\begin{blocki}{Advantages}
\item Like grid search: Very easy to implement, all parameter types possible, trivial parallelization
\item Anytime algorithm: Can stop the search whenever our budget for computation is exhausted, or continue until we reach our performance goal.
\item No discretization: each individual parameter is tried with a different value every time
\end{blocki}

\begin{blocki}{Disadvantages}
\item Inefficient: many evaluations in areas with low likelihood for improvement
\item Scales badly: high dimensional hyperparameter spaces need \emph{lots} of samples to cover.
\end{blocki}
\end{frame}




\begin{frame}[containsverbatim,allowframebreaks]{Evolutionary algorithms}

\textbf{Evolutionary algorithms} (EA) are a class of stochastic, metaheuristic optimization techniques whose mode of operation is inspired by the evolution of natural organisms.

\vspace{0.5cm}

\begin{center}
\begin{tabular}{ c | c}
\textbf{Definition} & \textbf{Correspondence} \\[0.05cm]
\hline \\[0.01cm]
solution candidate $\x\in \mathcal{S}$ & Chromosome of an individual \\[0.1cm]
$\x_i$& $i$-th gene of chromosome\\[0.1cm]
Set of solution candidates $\mathcal{P}$ & Population \\[0.1cm]
Objective function $f: \mathcal{S} \to \R$ & Fitness function
\end{tabular}
\end{center}

\end{frame}

\begin{frame}[containsverbatim,allowframebreaks]{History of evolutionary algorithms}

Similar techniques differ in genetic representation:

\begin{itemize}
\item \textbf{Genetic algorithms}: uses binary problem representation, therefore closest to the biological model of evolution.
\item \textbf{Evolution strategies}: uses direct problem representation, e.g. vector of real numbers.
\item \textbf{Genetic programming}: create structures that convert an input into a fixed output (e.g. computer programs); solution candidates are represented as trees.
\item \textbf{Evolutionary programming}: similar to GP, but solution candidates are not represented by trees, but by finite state machines.
\end{itemize}

The boundaries between the terms become increasingly blurred and are often used synonymously.

\end{frame}


\begin{frame}{Structure of an evolutionary algorithm}

\begin{center}
\begin{figure}
\centering
\begin{tikzpicture}[node distance=0.8cm, auto,]
%nodes
\node (init) {Initialize population};
\node[below = 0.3cm of init](rating1) {Rate population};
\node[below = 0.3cm of rating1](selection1) {Parent selection};
\node[below = 0.3cm of selection1](variation) {Variation};
\node[below = 0.3cm of variation](rating2) {Rate offspring};
\node[below = 0.3cm of rating2](selection2) {Survival selection};
\node[below = 0.3cm of selection2](stop) {Stop};
\node[below = 1cm of stop](dummy2) {};
\node[below = 0.2cm of stop](dummy3) {};
\node[right = 0.01cm of dummy3](dummy4) {yes};
\node[left = 1.1cm of rating2](dummy1) {no};
\draw[->] (init) to (rating1) node[midway, above]{};
\draw[->] (rating1) to (selection1) node[midway, above]{};
\draw[->] (selection1) to (variation) node[midway, above]{};
\draw[->] (variation) to (rating2) node[midway, above]{};
\draw[->] (rating2) to (selection2) node[midway, above]{};
\draw[->] (selection2) to (stop) node[midway, above]{};
\draw[->] (stop) to (dummy2) node[midway, above]{};
\draw[->] (stop) to [bend left=90, looseness=2](selection1) node[midway, above]{};
\end{tikzpicture}
\end{figure}
\end{center}

\end{frame}

\begin{frame}[allowframebreaks]{Example of an evolutionary algorithm}

In the following, methods for the individual steps of an evolutionary algorithm are presented.

\vspace{0.5cm}

These are demonstrated using the one-dimensional Ackley function, which we want to optimize on the $[-30, 30]$ interval.

\vspace{0.5cm}

In this case, each individual has exactly one chromosome. The chromosome is (obviously) encoded as a real number: $x_i \in \R$.

Usually for the optimization of a function $f:\R^n \to \R$ individuals are coded as real vectors $\x_i \in \R^n$.


\framebreak


\begin{center}
\begin{figure}
\includegraphics[width=\textwidth, height=6cm]{images/ea_ex1.png}
\end{figure}
\end{center}

\end{frame}


\begin{frame}{Step 1: initialize population}

We start with a randomly selected population $\mathcal{P} = \{\x_1, ..., \x_\mu\}$ of the size $\mu = 20$ and rate it. The fitness function in this case is the function we want to minimize.


\begin{center}
\begin{figure}
\includegraphics[width=\textwidth, height=6cm]{images/ea_ex2.png}
\end{figure}
\end{center}


\end{frame}

\begin{frame}[containsverbatim,allowframebreaks]{Step 2: parent selection}


In the first step of an iteration, $\lambda$ parents are chosen, who create offspring in the next step.

Possibilities for selection of parents:
\begin{itemize}
\item \textbf{Neutral selection: }choose individual with a probability $1/\mu$.
\item \textbf{Fitness-proportional selection: }draw individuals with probability proportional to their fitness.
\item \textbf{Tournament Selection: }randomly select $k$ individuals for a "Tournament Group". Of the drawn individuals, the best one (with the highest fitness value) is then chosen. In order to draw $\lambda$ individuals, the procedure must be performed $\lambda$-times.
\end{itemize}
\vspace*{-0.2cm}
\begin{figure}
\includegraphics[width = 7cm, height = 2.5cm ]{images/tournament_selection.png}
\end{figure}

\framebreak

In our example we choose $\lambda = 5$ offspring by neutral selection (red individuals).

\begin{center}
\begin{figure}
\includegraphics[width=\textwidth, height=5cm]{images/ea_ex3.png}
\end{figure}
\end{center}

\end{frame}

\begin{frame}{Step 3: variation}

New individuals are now generated from the parent population. This is done by

\begin{itemize}
\item Recombination: combine two parents into one offspring.
\item Mutation: change an individual.
\end{itemize}

Recombination and mutation are not always performed both, sometimes only recombination or mutation is performed.

\end{frame}

\begin{frame}{Recombination for numeric representations}

Two individuals $\x, \tilde\x \in \R^n$ in numerical representation can be recombined as follows:

\begin{itemize}
\item \textbf{Uniform crossover}: choose gene $i$ with probability $p$ of 1st parent and probability $1-p$ of 2nd parent.
\item \textbf{Intermediate recombination}: new individual is created from the mean value of two parents $\frac{1}{2}(\x + \tilde\x)$
\item \textbf{Simulated Binary Crossover (SBX)}: generate \textbf{two offspring} based on

$$
\bar\x \pm \frac{1}{2} \beta (\tilde\x - \x)
$$

with $\bar\x = \frac{1}{2} (\x + \tilde\x)$.
\end{itemize}

\end{frame}

\begin{frame}{Recombination for bit strings}

  For example, two individuals $\x, \tilde\x \in \{0, 1\}^n$ encoded as bit strings can be recombined as follows:

  \begin{itemize}
  \item \textbf{1-point crossover}: select crossover $k \in \{1, ..., n - 1\}$ randomly and select the first $k$ bits from 1st parent, the last $n-k$ bits from 2nd parent.

  \footnotesize
  \begin{center}
  \begin{tabular}{c @{\hspace{2\tabcolsep}} *{6}{c}}
   " " & \textcolor{red}{1} & \textcolor{blue}{1} & " " & " " & "  " & \textcolor{red}{1}  \\
   " " & \textcolor{red}{0} & \textcolor{blue}{0} & " " & " " & "  " & \textcolor{red}{0}  \\ \cmidrule{2-3}
   " " & \textcolor{red}{0} & \textcolor{blue}{1} & " " &$\Rightarrow$ & "  " & \textcolor{blue}{1}  \\
   " " & \textcolor{red}{1} & \textcolor{blue}{1} & " " & " " & "  " & \textcolor{blue}{1}  \\
   " " & \textcolor{red}{1} & \textcolor{blue}{0} & " " & " " & "  " & \textcolor{blue}{0}
  \end{tabular}
  \end{center}
  \normalsize

  \item \textbf{Uniform crossover}: select bit $i$ with probability $p$ of 1st parent and probability $1-p$ of 2nd parent.

  \end{itemize}


  % \footnotesize
  % \begin{center}
  % \begin{tabular}{c @{\hspace{2\tabcolsep}} *{6}{c}}
  %  " " & \textcolor{red}{1} & \textcolor{blue}{1} & " " & " " & "  " & \textcolor{red}{1}  \\
  %  " " & \textcolor{red}{0} & \textcolor{blue}{0} & " " & " " & "  " & \textcolor{blue}{0}  \\
  %  " " & \textcolor{red}{0} & \textcolor{blue}{1} & " " &$\Rightarrow$ & "  " & \textcolor{blue}{1}  \\
  %  " " & \textcolor{red}{1} & \textcolor{blue}{1} & " " & " " & "  " & \textcolor{blue}{1}  \\
  %  " " & \textcolor{red}{1} & \textcolor{blue}{0} & " " & " " & "  " & \textcolor{red}{1}
  % \end{tabular}
  % \end{center}
  % \normalsize

  \end{frame}


  \begin{frame}[containsverbatim,allowframebreaks]{Mutation for numeric representations}

    \textbf{Mutation:} individuals are changed, for example for $\x \in \R^n$
    \begin{itemize}
    \item \textbf{Uniform mutation:} choose a random gene $x_i$ and replace it with a value uniformly distributed (within the feasible range).
    \item \textbf{Gauss mutation}: $\tilde\x = \x \pm \sigma \mathcal{N}(0, \boldsymbol{I})$
    \item \textbf{Polynomial mutation:} polynomial distribution instead of normal distribution

    \begin{center}
    \begin{figure}
      \includegraphics[height = 3.5cm, width = 4cm]{images/polynomial_mutation.png}\\
      \scriptsize{Source: K. Deb, Analysing mutation schemes for real-parameter genetic algorithms, 2014}
    \end{figure}
     \end{center}

     \framebreak

    More exact:

    $$
    {\tilde x_{i}} = x_{i} + (x_{i,upper} - x_{i,lower}) \delta_{i}
    $$

    with $x_{i,upper} (x_{i,lower})$ as upper (lower) bound for $x_{i}$.

    $\delta_{i}$ results as:

    \footnotesize
    $$
    \delta_{i} =
    \begin{cases}
    [2r_{i}+(1-2r_{i})(1-\delta)^{\eta_{m}+1}]^{\frac{1}{\eta +1}} -1, & r_{i} < 0.5 \\
    1 - [2(1-r_{i})+2(r_{i}-\frac{1}{2})(1-\delta)^{\eta_{m}+1}]^{\frac{1}{\eta_{m} +1}}, &  \text{else.}
    \end{cases}
    $$
    with  $\delta = \frac{min\{(x_{i} - x_{i,lower}), (x_{i,upper}-x_{i})\}}{x_{i,upper} - x_{i,lower}}$.

    \normalsize

    \vspace{0.5cm}

    Here $r_{i} \in [0,1]$ is a uniformly distributed number, $\eta_{m}$ is the distribution index of the mutation and is chosen by the user.\\
    % Remark: A $\eta_{m}$ of the order of $\eta_{m} \in [20,100]$ is common.
    \normalsize
    \end{itemize}



    \vspace{0.5cm}

    In our example, we have chosen a Gauss mutation with $\sigma = 2$, we do not apply a recombination.

    \begin{center}
      \begin{figure}
      \includegraphics[width=\textwidth, height=5cm]{images/ea_ex4.png}
      \end{figure}
      \end{center}

    \end{frame}

    \begin{frame}{Mutation for bit strings}

    For example, an individual $\x \in \{0, 1\}^n$ encoded as a bit string can be mutated as follows:

    \vspace{0.5cm}

    \textbf{Mutation:}
    \begin{itemize}
    \item \textbf{Bitflip}: for each index $k \in \{1, ..., n\}$: bit $k$ is flipped with probability $p \in (0,1)$.
    \item If $(a)$ is an arbitrary bit sequence to which a bitflip mutation is applied, $(b)$ is obtained.
    \end{itemize}

    \footnotesize
    \begin{center}
    \begin{tabular}{c @{\hspace{2\tabcolsep}} *{5}{c}}
      &
      \itshape (a) &
      \itshape " " &
      \itshape " " &
      \itshape " " &
      \itshape (b)

    \\[1ex]
    " " & 1 & " " & " " & "  " & \textcolor{red}{0}  \\
    " " & 0 & " " & " " & "  " & 0  \\
    " " & 0 & " " & $\Rightarrow$ & "  " & \textcolor{red}{1}  \\
    " " & 1 & " " & " " & "  " & \textcolor{red}{0}  \\
    " " & 1 & " " & " " & "  " & 1
    \end{tabular}
    \end{center}
    \normalsize

    \end{frame}

    \begin{frame}[allowframebreaks]{Step 4: survival selection}

    Now individuals are chosen who survive. Two common strategies are:

    \begin{itemize}
    \item \textbf{$(\mu, \lambda)$-selection}: we select from the $\lambda$ descendants the $\mu$ best ($\lambda \ge \mu$ necessary).

    \textbf{But:} best overall individual can get lost!
    \item \textbf{$(\mu + \lambda)$-selection}: $\mu$ parents and $\lambda$ offspring are lumped together and the $\mu$ best individuals are chosen.

    Best individual safely survives.
    \end{itemize}

    \framebreak

    In our example, a $(\mu + \lambda)$ selection was used. The selected points are green.

    \begin{center}
      \begin{figure}
      \includegraphics[width=\textwidth, height=5cm]{images/ea_ex5.png}
      \end{figure}
      \end{center}


    \end{frame}


\begin{frame}{Tuning Example}

Tuning random forest with random search and 5CV on the \texttt{sonar} data set for AUC:



\begin{columns}
\begin{column}{0.49\textwidth}
\begin{center}
\begin{tabular}{|l|l|l|l}
Parameter&Type & Min & Max \\
\hline
\texttt{num.trees} & integer& 3 & 500 \\
\texttt{mtry}& integer& 5 & 50\\
\texttt{min.node.size} & integer& 10 & 100\\
\end{tabular}
\end{center}
\end{column}
\begin{column}{0.49\textwidth}
\begin{center}
\begin{figure}
\includegraphics[width=0.8\textwidth]{images/curve.png}
\end{figure}
\end{center}
\end{column}
\end{columns}


\end{frame}



\end{document}
