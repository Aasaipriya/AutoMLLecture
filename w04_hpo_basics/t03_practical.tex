\input{t00_template.tex}
\subtitle{Example and Practical Hints}



\begin{document}

\maketitle


%----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}{Expert Knowledge}

\begin{itemize}
	\item Knowledge about hyperparameters can help to guide the optimization
	\item For example, some hyperparameters might not be sampled uniformly
\end{itemize}

    \vspace{0.5cm}
Example: regularization hyperparameter ($C$ or \emph{cost}) of SVM: $[0.001, 1000.0]$

\begin{itemize}
	\item The distance between $999.9$ and $1000.0$ should not be the same as between $0.1$ and $0.2$.
  \item We might want to sample here from from a log-scale, e.g., $[10^{\conf_l}, 10^{\conf_u}]$ with $\conf_l = -3$ and $\conf_u = 3$.
\end{itemize}

\begin{figure}[htb]
\centering
  \begin{tikzpicture}[auto]%[scale=1.5]
    \draw [->](-0.3,0)-- (12.3,0) coordinate;
    \draw [->](-0.3,-2)-- (12.3,-2) coordinate;
    \foreach \x/\xtext/\xxtext in {-3/-3/0.001, -2/-2/ , -1/-1/, 0/0/, 1/1/, 2/2/100, 3/3/1000} {
      \draw [very thick] ({\x*2+6},-2pt) -- ++(0, 4pt) node[xshift = -6pt, yshift=-3pt,anchor=south west,baseline]{\strut$\xtext$};
      \draw [very thick] ({10^(\x)*0.012},-2cm+2pt) -- ++(0,-4pt) node[anchor=north]{$\xxtext$};
      \draw [->] ({\x*2+6},-2pt) .. controls ({\x*2+6},-0.5) and ({10^(\x)*0.012},-1.5) .. ({10^(\x)*0.012},-2cm+2pt);
    }
    \node[] at (-0.7,-0.1) (t1) {$\conf$};
    \node[] at (-0.7,-1.9) (t2) {$10^{\conf}$};
  \end{tikzpicture}
\end{figure}


\end{frame}
\begin{frame}{Tuning Example}
Tuning $(\text{cost},\gamma) \in [10^{-3},10^{3}]^2$ for the \emph{SVM} with \emph{random search}, \emph{grid search} and \emph{CMAES}\footnote{A popular EA that samples offspring from a multi-variate normal distribution} using a 5-fold CV on the \texttt{spam} data set for AUC:
\begin{columns}
\begin{column}{0.45\textwidth}
  \vspace{1em}
  % \resizebox{\linewidth}{!}{
  %   \begin{tabular}{l|l|l|l}
  %   Parameter&Type & Min & Max \\
  %   \hline
  %   \texttt{cost}  & double & $10^{-3}$ & $10^{3}$ \\
  %   \texttt{gamma} & double & $10^{-3}$ & $10^{3}$ \\
  %   \end{tabular}
  % }

  We notice here:

  \begin{itemize}
    \item \emph{Grid search} has many evaluations with bad performance ($\gamma>1$).
    \item \emph{Random search} can lead to underexplored areas even in promising regions.
    \item \emph{CMAES} only explores a small region.
  \end{itemize}
\end{column}%
\begin{column}{0.5\textwidth}
  \vspace{-1em}
  \begin{figure}
  \includegraphics[width=0.9\textwidth]{images/benchmark_scatter.png}
  \end{figure}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Tuning Example (cont.)}
The \emph{optimization curve} shows the best found configuration until a given time point.
\begin{columns}
\begin{column}{0.4\textwidth}
  \footnotesize
  \only<1>{

    Note:

    \begin{itemize}
      \item For \emph{random search} and \emph{grid search} the chronological order on the x-axis is arbitrary.
      \item The curve shows the performance on the tuning validation (\emph{inner resampling}) on a single fold
    \end{itemize}
  }
  \only<2-3>{
    \begin{itemize}
      \item<2-> The outer 10-fold CV gives us 10 optimization curves.
      \item<3-> The median at each time point gives us an estimate of the average optimization progress.
    \end{itemize}
  }
  \only<4>{
    \begin{itemize}
      \item Remember: The final model will be trained on the \emph{outer training set} with the configuration $\finconf$ that lead to the best performance on the \emph{inner test set}.
      \item To compare the effectiveness of the tuning strategies we have to look at the performance that $\finconf$ gives us on the \emph{outer test set}.
    \end{itemize}
  }
\end{column}
\begin{column}{0.6\textwidth}
  \vspace{-1em}
  \begin{figure}
  \only<1>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_1.png}}
  \only<2>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_all.png}}
  \only<3>{\includegraphics[width=\textwidth]{images/benchmark_curve_median.png}}
  \only<4>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_all_median.png}}
  \end{figure}
\end{column}
\end{columns}


  %   \footnotesize
  %     \item Tuning does not necessarily improve the performance of the learner, because e.g.\ tuning is badly configured or default values are determined by \emph{clever} heuristics.
  %     \item Tuning error can be overly optimistic (see \emph{nested resampling}).
  %   \end{itemize}
  % }
  % \only<2>{
  %   \begin{itemize}
  %     \item Effect of the chosen resampling split (objective noise) can dominate tuning effect.
  %   \end{itemize}
  % }
\end{frame}

\begin{frame}{Tuning Example: Validation}

\begin{columns}
\begin{column}{0.4\textwidth}
  \footnotesize

  The box plots show the distribution of the AUC-values that were measured on the \emph{outer test set} with a 10-fold CV.

  Note:

  \begin{itemize}
    \item The box plots do not indicate significant differences.
    \item The performance differs from the results obtained on the inner resampling.
  \end{itemize}

\end{column}
\begin{column}{0.6\textwidth}
  \vspace{-1em}
  \begin{figure}
  \includegraphics[width=\textwidth]{images/benchmark_boxplot_tuners.png}
  \end{figure}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Tuning Example: Validation}

\begin{columns}
\begin{column}{0.4\textwidth}
  \footnotesize

  \begin{itemize}
    \item<1-> Comparison of \textit{self-tuning} \emph{SVMs} with an SVM that was configured with $\conf = (\text{cost},\gamma) = (1,1)$ shows that tuning is necessary.
    \item<2-> However, some \emph{SVM} implementations have \emph{clever} heuristics:
        \begin{itemize}
            \item $\gamma = \frac{1}{p*var(X)}$
        \end{itemize}
  \end{itemize}
\end{column}
\begin{column}{0.6\textwidth}
  \begin{figure}
  \only<1>{\includegraphics[width=\textwidth]{images/benchmark_boxplot_default.png}}
  \only<2>{\includegraphics[width=\textwidth]{images/benchmark_boxplot_all.png}}
  \end{figure}
\end{column}
\end{columns}
\end{frame}

%\begin{frame}{Practical Hints: Nested Resampling}
%\begin{itemize}
%   \item For small data sets the relative size of the \emph{inner training set} should not differ much from \emph{outer training set}. Example: \\$n = 200$,
%   \begin{itemize}
%     \item outer resampling: 5-fold CV, inner resampling: 3-fold CV: $n_{\text{inner train}} = \frac{4}{5} \cdot {2}{3} \cdot n = 0.53 * n = 107$
%     \item inner and outer resampling: 10-fold CV: $\frac{9}{10} \cdot \frac{9}{10} \cdot n = 0.81 * n = 162$
%   \end{itemize}
%   \item Resampling strategies depend on dataset sizes:
%  \begin{itemize}
%    \item Small datasets: More resampling iterations necessary to obtain reliable performance estimate (e.g.\ repeated CV).
%    \item Large datasets: Less resampling iterations possible due to runtime, but holdout can be sufficient to estimate performance.
%    \item For unbalanced and multi-class datasets $n$ has to be higher to obtain reliable performance estimates, i.e.\ they should be treated like small datasets if not sufficiently big.
%  \end{itemize}
%\end{itemize}
%\end{frame}

\begin{frame}[allowframebreaks]{More Practical Hints}
\begin{itemize}
  % \item Tuning can lead to overfitting.
  \item The optimal hyperparameter setting can depent on the size of the training data. This can be a reason why the optimal configuration determined on the \emph{inner resampling} data does not have to be optimal for the \emph{outer resampling}.

  \item If $\finconf$ is close to the border of $\pcs$, consider log-scales or widening the search space.

  \item \begin{minipage}[t]{.55\linewidth}\raggedright
          \emph{Grid search} worked well on our example because $\pcs \in \mathbb{R}^2$ and the search space was defined with expert knowledge. In general grid search is ineffective as unimportant hyperparameters are explored while other parameters are kept constant.
        \end{minipage}%
        \begin{minipage}[t]{.4\linewidth}
        \raisebox{-\height}{\includegraphics[width = 0.8\linewidth]{images/grid_search_im.png}}
        \end{minipage}

\frambreak

  \item Tuning algorithm and implementation differ in the type of hyperparameters they support, e.g., real-valued, integer, categorical, mixed, hierarchical.

  \item The selection of $\finconf$ should consider the stochastic characteristic of the objective.

  \item Use parallelization right:
  \begin{itemize}
    \item An embarrassingly parallel tuner (e.g.\ random search) can be more efficient then an \emph{smart} optimizer that is purely sequential if many cores are available.
    \item Make use of the internal parallelization of the learner before using parallel tuning (unless it is embarrassingly parallel).
  \end{itemize}

\end{itemize}
\end{frame}

\end{document}
