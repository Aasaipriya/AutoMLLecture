\input{t00_template.tex}
\subtitle{Practical Application}



\begin{document}

\maketitle


%----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}{Expert Knowledge}

\begin{itemize}
	\item Expert knowledge can help to guide hyperparameter optimization
	\item For example, some hyperparameters might not be sampled uniformly
\end{itemize}


Example: regularization hyperparameter ($C$ or \emph{cost}) of SVM: $[0.001, 1000.0]$

\begin{itemize}
	\item The distance between $999.9$ and $1000.0$ should not be the same as between $0.1$ and $0.2$.
  \item We might want to sample here from from a log-scale, e.g., $[10^{\conf_l}, 10^{\conf_u}]$ with $\conf_l = -3$ and $\conf_u = 3$.
\end{itemize}

\begin{figure}[htb]
\centering
  \begin{tikzpicture}[auto]%[scale=1.5]
    \draw [->](-0.3,0)-- (12.3,0) coordinate;
    \draw [->](-0.3,-2)-- (12.3,-2) coordinate;
    \foreach \x/\xtext/\xxtext in {-3/-3/0.001, -2/-2/ , -1/-1/, 0/0/, 1/1/, 2/2/100, 3/3/1000} {
      \draw [very thick] ({\x*2+6},-2pt) -- ++(0, 4pt) node[xshift = -6pt, yshift=-3pt,anchor=south west,baseline]{\strut$\xtext$};
      \draw [very thick] ({10^(\x)*0.012},-2cm+2pt) -- ++(0,-4pt) node[anchor=north]{$\xxtext$};
      \draw [->] ({\x*2+6},-2pt) .. controls ({\x*2+6},-0.5) and ({10^(\x)*0.012},-1.5) .. ({10^(\x)*0.012},-2cm+2pt);
    }
    \node[] at (-0.7,-0.1) (t1) {$\conf$};
    \node[] at (-0.7,-1.9) (t2) {$10^{\conf}$};
  \end{tikzpicture}
\end{figure}


\end{frame}
\begin{frame}{Tuning Example}
Tuning $(\text{cost},\text{gamma}) \in [10^{-3},10^{3}]^2$ for the \emph{SVM} with \emph{random search}, \emph{grid search} and \emph{CMAES}\footnote{EA that samples offspring from a multi-variate normal distribution. For details see: \url{https://arxiv.org/pdf/1604.00772.pdf}} using a 5-fold CV on the \texttt{spam} and \texttt{sonar} data set for AUC:
\begin{columns}
\begin{column}{0.45\textwidth}
  \vspace{1em}
  % \resizebox{\linewidth}{!}{
  %   \begin{tabular}{l|l|l|l}
  %   Parameter&Type & Min & Max \\
  %   \hline
  %   \texttt{cost}  & double & $10^{-3}$ & $10^{3}$ \\
  %   \texttt{gamma} & double & $10^{-3}$ & $10^{3}$ \\
  %   \end{tabular}
  % }

  We notice here:
  
  \begin{itemize}
    \item \emph{Grid search} has many evaluations with bad performance (\emph{gamma}$>1$).
    \item \emph{Random search} can lead to unexplored areas even in promising regions. 
    \item \emph{CMAES} can run into local minimum.
  \end{itemize}
\end{column}%
\begin{column}{0.6\textwidth}
  \vspace{-1em}
  \begin{figure}
  \includegraphics[width=\textwidth]{images/benchmark_scatter.png}
  \end{figure}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Tuning Example (cont.)}
The \emph{optimization curve} shows the best found configuration until a given time point.
\begin{columns}
\begin{column}{0.4\textwidth}
  \footnotesize
  \only<1>{
    
    Note:

    \begin{itemize}
      \item For \emph{random search} and \emph{grid search} the chronological order on the x-axis is arbitrary.
      \item The curve shows the performance on the tuning validation (\emph{inner resampling}).
      \item This curve only shows the progress on one outer training set, but with a 10-fold CV for outer resampling we have 10 outer training sets\ldots
    \end{itemize}
  }
  \only<2-3>{
    \begin{itemize}
      \item<2-> The outer 10-fold CV gives us 10 optimization curves.
      \item<3-> The median at each time point gives us an estimate of the average optimization progress.
    \end{itemize}
  }
  \only<4>{
    \begin{itemize}
      \item Remember: The final model will be trained on the \emph{outer training set} with the configuration $\finconf$ that lead to the best performance on the \emph{inner test set}.
      \item To compare the effectiveness of the tuning strategies we have to look at the performance that $\finconf$ gives us on the \emph{outer test set}.
    \end{itemize}
  }
\end{column}
\begin{column}{0.6\textwidth}
  \vspace{-1em}
  \begin{figure}
  \only<1>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_1.png}}
  \only<2>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_all.png}}
  \only<3>{\includegraphics[width=\textwidth]{images/benchmark_curve_median.png}}
  \only<4>{\includegraphics[width=\textwidth]{images/benchmark_curve_iter_all_median.png}}
  \end{figure}
\end{column}
\end{columns}
  

  %   \footnotesize
  %     \item Tuning does not necessarily improve the performance of the learner, because e.g.\ tuning is badly configured or default values are determined by \emph{clever} heuristics.
  %     \item Tuning error can be overly optimistic (see \emph{nested resampling}).
  %   \end{itemize}
  % }
  % \only<2>{
  %   \begin{itemize}
  %     \item Effect of the chosen resampling split (objective noise) can dominate tuning effect.
  %   \end{itemize}
  % }
\end{frame}

\begin{frame}{Tuning Example: Validation}

\begin{columns}
\begin{column}{0.4\textwidth}
  \footnotesize

  The box plots show the distribution of the AUC-values that were measured on the \emph{outer test set} through the 10-fold CV.

  Note:

  \begin{itemize}
    \item The box plots do not indicate significant differences\footnotemark.
    \item The performance differs from the results obtained on the inner resampling.
  \end{itemize}

\end{column}
\begin{column}{0.6\textwidth}
  \vspace{-1em}
  \begin{figure}
  \includegraphics[width=\textwidth]{images/benchmark_boxplot_tuners.png}
  \end{figure}
\end{column}
\end{columns}
\footnotetext[2]{Box plots hide that the measured values are paired. How to properly test for significance was explained in the lecture on validation.}
\end{frame}

\begin{frame}{Tuning Example: Validation}

\begin{columns}
\begin{column}{0.4\textwidth}
  \footnotesize

  \begin{itemize}
    \item<1-> Comparison of \textit{self-tuning} \emph{SVMs} with an SVM that was configured with $\conf = (\text{cost},\text{gamma}) = (1,1)$ shows that tuning is necessary.
    \item<2-> However, some \emph{SVM} implementations come with \emph{clever} heuristics (here: \texttt{R}-package \texttt{e1071}).
  \end{itemize}
\end{column}
\begin{column}{0.6\textwidth}
  \begin{figure}
  \only<1>{\includegraphics[width=\textwidth]{images/benchmark_boxplot_default.png}}
  \only<2>{\includegraphics[width=\textwidth]{images/benchmark_boxplot_all.png}}
  \end{figure}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Practival Hints: Nested Resampling}
\begin{itemize}
   \item For small data sets the relative size of the \emph{inner training set} should not differ much from \emph{outer training set}. Example: \\$n = 200$, 
   \begin{itemize}
     \item outer resampling: 5-fold CV, inner resampling: 3-fold CV: $n_{\text{inner train}} = \frac{4}{5} \cdot {2}{3} \cdot n = 0.53 * n = 107$
     \item inner and outer resampling: 10-fold CV: $\frac{9}{10} \cdot \frac{9}{10} \cdot n = 0.81 * n = 162$ 
   \end{itemize}
   \item Resampling strategies depend on dataset sizes:
  \begin{itemize}
    \item Small datasets: More resampling iterations necessary to obtain reliable performance estimate (e.g.\ repeated CV).
    \item Large datasets: Less resampling iterations possible due to runtime, but holdout can be sufficient to estimate performance.
    \item For unbalanced and multi-class datasets $n$ has to be higher to obtain reliable performance estimates, i.e.\ they should be treated like small datasets if not sufficiently big.
  \end{itemize} 
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Practical Hints: Tuning}
\begin{itemize}
  % \item Tuning can lead to overfitting.
  \item The optimal hyperparameter setting can correlate with the size of the training data. This can be a reason why the optimal setting on the \emph{inner resampling} data does not have to be optimal for the \emph{outer resampling}.
  
  \item For very large datasets use special tuning algorithms that adapt training set size. (e.g.\ \emph{Hyperband})
  
  \item Feature selection and preprocessing should also be considered as tuning and has to be validated accordingly.
  
  \item If $\finconf$ is close to the border of $\pcs$, consider log-scales or widening the search space.
  
  \item \begin{minipage}[t]{.55\linewidth}\raggedright
          \emph{Grid search} worked well on our example because $\pcs \in \mathbb{R}^2$ and the search space was defined with expert knowledge. In general grid search is ineffective because of the limited resolution per dimension.
        \end{minipage}%
        \begin{minipage}[t]{.4\linewidth}
        \raisebox{-\height}{\includegraphics[width = \linewidth]{images/automl_p7_fig1-1.pdf}}
        \end{minipage}

  \item Each tuning technique and implementation has different conditions that he objective has to meet. These can include:
  \begin{itemize}
    \item Type of the search space: E.g.\ real-valued, integer only, categorical, mixed, hierarchical.
    \item Noisiness of the objective: Usually ML problems are stochastic (due to resampling and stochastic learners).
    \item The selection of $\finconf$ should consider the stochastic characteristic of the objective. Part of the good performance of the best configuration could be attributed to noise. \emph{Grid Search} and \emph{Random Search} are overly optimistic.
  \end{itemize}

  \item Use parallelization right:
  \begin{itemize}
    \item An embarrassingly parallel tuner (e.g.\ random search) can be more efficient then an \emph{smart} optimizer that is purely sequential if many cores are available.
    \item Make use of the internal parallelization of the learner before using parallel tuning (unless it is embarrassingly parallel).
  \end{itemize}
  
  \item For many real-world applications you want to optimize multiple criteria (e.g.\ sensitivity and specificity). This will be topic in a later lecture.
\end{itemize}
\end{frame}

\end{document}
