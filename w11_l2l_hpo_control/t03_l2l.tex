
\input{../latex_main/main.tex}



\title[AutoML: Learning to Control]{AutoML: Automated Machine Learning}
\subtitle{Learning to Control}
\author{Marius Lindauer}
\institute{\vspace*{2em}\includegraphics[width=10em]{../latex_main/images/automl_hannover.png}}
\date{}



% \AtBeginSection[] % Do nothing for \section*
% {
%   \begin{frame}{Outline}
%     \bigskip
%     \vfill
%     \tableofcontents[currentsection]
%   \end{frame}
% }

\begin{document}
	
	\maketitle
	

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Learn}

\centering
\huge
Learning to learn by gradient descent by gradient descent\\
by Andrychowicz et al. '16


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Learn}

\begin{block}{Idea}
\begin{itemize}
	\item Learn algorithms directly, e.g., how to search in the weight space
	\item First idea: learn weight updates of a neural network
\end{itemize}
\end{block}

\pause

\begin{block}{Learning to learn by gradient descent by gradient descent\newline \lit{Andrychowicz et al'16}}
Weight updates (note: \alert{$\theta$ denote DNN weights!}):
\begin{equation}
\theta_{t+1} = \theta_t - \alpha_t \nabla f(\theta_t) \nonumber
\end{equation}
\pause
Even more general:
\begin{equation}
\theta_{t+1} = \theta_t + g_t(\nabla f(\theta_t), \phi) \nonumber
\end{equation}
where $g$ is the optimizer and $\phi$ are the parameters of the optimizer $g$.\\
\pause
$\leadsto$ \alert{Goal: Optimize $f$ wrt $\theta$ by learning $g$ (resp. $\phi$)}
\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Learn: Objective \lit{Andrychowicz et al'16}}

\vspace{-0.5cm}
\begin{equation}
\mathcal{L}(\phi) = \mathbb{E}\left[ f(\theta^*(f,\phi)) \right]\nonumber
\end{equation}

where $\mathcal{L}$ is a loss function and $\theta^*(f,\phi)$ are the optimized weights $\theta^*$ by using the optimizer parameterized with $\phi$ on function $f$.

\pause

%\vspace{-0.5cm}
\begin{equation}
\mathcal{L}(\phi) = \mathbb{E}\left[\sum_{t=1}^T w_t f(\theta_t)\right]\nonumber
\end{equation}

\pause
where $w_t$ are arbitrary weights associated with each time step
and 

\pause
\vspace{-0.5cm}
\begin{eqnarray}
\theta_{t+1} = \theta_t + g_t\nonumber\\
\begin{pmatrix}g_t\\h_{t+1}\end{pmatrix} = m(\nabla_\theta f(\theta_t), h_t, \phi)\nonumber
\end{eqnarray}

\pause
$\leadsto$ Goal: Learn $m$ via $\phi$ by using gradient descent by optimizing $\mathcal{L}$ \\
\pause
$\leadsto$ ``Learning to learn gradient descent by gradient descent''
\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Learn: LSTM approach \lit{Andrychowicz et al'16}}

\begin{description}
\item[Optimizee] Target network to be trained
\item[Optimizer] LSTM with hidden state $h_t$ that predicts weight updates $g_t$
\end{description}

\medskip

\includegraphics[width=1\textwidth]{images/learning_to_learn_lstm}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Learn: Coordinatewise LSTM optimizer \lit{Andrychowicz et al'16}}

\centering
\includegraphics[width=.7\textwidth]{images/l2l_shared_lstm.png}

\begin{itemize}
\item One LSTM for each coordinate (i.e., weight)
\item All LSTMs have shared parameters $\phi$
\item Each coordinate has its own separate hidden state
\pause
\item[$\leadsto$] We can train the LSTM on $k$ weights and apply it larger DNNs\\ with $k'$ weights, where $k \leq k'$
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Learn with LSTM: Results \lit{Andrychowicz et al'16}}

\centering
\includegraphics[width=0.7\textwidth]{images/l2l_mnist_base}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Learn with LSTM: Results \lit{Andrychowicz et al'16}}

Changing the original architecture of the DNN:
\smallskip

\centering
\includegraphics[width=0.9\textwidth]{images/l2l_mnist_okchange}

$\leadsto$ learnt optimizer is robust against some architectural changes

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Learn with LSTM: Results\newline \lit{Andrychowicz et al'16}}

Changing the activation function to ReLU:
\smallskip

\centering
\includegraphics[width=0.6\textwidth]{images/l2l_mnist_relu}

$\leadsto$ fails on other activation functions

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning Step Size Controllers \lit{Daniel et al'16}}

\huge
\centering
Learning Step Size Controllers for Robust Neural Network Training\\
by Daniel et al. '16



\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning Step Size Controllers \lit{Daniel et al'16}}

\begin{itemize}
\item \alert{Idea:} Learn the hyperparameters of the weight update 
\end{itemize}

\begin{equation}
\theta_{t+1} = \theta_t - \alpha_t \nabla f(\theta_t) \nonumber
\end{equation}

\begin{itemize}
\pause
\item For SGD, this would be for example the learning rate $\alpha$
\pause
\item \alert{Note}: $\alpha$ have to be adapted in the course of the training
\begin{itemize}
\item similar to learning rate schedules (e.g., cosine annealing)
\end{itemize}
\pause
\item \alert{Note(ii)}: later we denote the learnt hyperparameters as $\lambda$
\medskip
\pause
\item \alert{Idea:} Use reinforcement learning to learn a policy $\pi: s \mapsto a$ to control the learning rate (or other adaptive hyperparameters)
\end{itemize}



\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recap: Reinforcement Learning \lit{Barto \& Sutton; RL Book}}

\begin{center}
\includegraphics[width=0.6\textwidth]{images/suttonbarto_rl.png}
\end{center}

\pause

\begin{eqnarray}
\pi^* \in \argmax_{\pi} \mathbb{E}_{\pi} \left[ \sum^{T}_{t=1} \gamma^{t-1} r_t \mid s_0 \right] \nonumber
\end{eqnarray}

The goal is to find the optimal policy $\pi^{*}: s \mapsto a$ starting in a state $s_0$\\ (or in expectation of $s_0$) s.t. following $\pi^{*}$ from $s$ maximizes the expected (discounted) reward~$R$.


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{RL for Step Size Controllers: State \lit{Daniel et al'16}}

\textbf{Predictive change in function value:}

$$s_1 = \log \left( \text{Var}(\Delta \tilde{f}_i ) \right)$$
$$\Delta \tilde{f}_i = \tilde{f}(x_i; \theta + \delta \theta) - f(x_i; \theta)$$

where $\tilde{f}(x_i; \theta + \delta \theta)$ is done by a first order Taylor expansion

\pause
\textbf{Disagreement of function values:}
$$ s_2 = \log \left(\text{Var}(f(x_i; \theta)) \right)$$

\pause

\textbf{Discounted Average} (smoothing noise from mini-batches):
$$\hat{s}_i \leftarrow \gamma \hat{s_i} + (1 - \gamma) s_i$$

\pause

\textbf{Uncertainty Estimate} (noise level):
$$s_{K+i} \leftarrow \gamma s_{K+i} + (1-\gamma) (s_i - \hat{s}_i)^2$$


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{RL for Step Size Controllers: Learning \lit{Daniel et al'16}}

Reward (average loss improvement over time):

$$r = \frac{1}{T-1} \sum_{t=2}^T \left(\log(\loss_{t-1}) - \log(\loss_t)\right)$$

\pause

Optimal Policy:

$$\pi^*(\lambda \mid s) \in \argmax_{\pi} \int \int p(s) \pi(\lambda\mid s)r(\lambda,s) \texttt{d}s \texttt{d}\lambda $$

\pause

The policy $\pi$ is learnt as a controller $\lambda = g(s, \phi)$; leading to

$$\pi^*(\phi) \in \argmax_{\pi} \int \int p(s) \pi(\phi)r(g(s;\phi),s) \texttt{d}s \texttt{d}\lambda $$

\pause

\begin{itemize}
\item can be learnt for example via Relative Entropy Policy Search (REPS) \lit{Peter et al. 2010}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{RL for Step Size Controllers: Training \lit{Daniel et al'16}}

\begin{itemize}
\item Goal: obtain robust policies,\\ i.e., good performance for many different DNN architectures
\begin{itemize}
\item[$\leadsto$] Sample architectures e.g., with different numbers of filters and layers
\item[$\leadsto$] (Sub-)Sample dataset
\item[$\leadsto$] Sample number of optimization steps
\end{itemize}
\end{itemize}

\pause 
\medskip
\centering
\includegraphics[width=0.6\textwidth]{images/l2stepsizecontroler_mnist_training.png}

"Ours" refers to the approach by \lit{Daniel et al'16} and $\eta$ is the learning rate

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning Black-box Optimization~\lit{Chen et al'17}}

\huge
\centering
Learning to Learn without Gradient Descent by Gradient Descent\\
by Chen et al'17


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning Black-box Optimization~\lit{Chen et al'17}}

\begin{block}{Black Box Optimization Setting}
\begin{enumerate}
\item Given the current state of knowledge $h_t$ propose a query point $x_t$
\item Observe the response $y_t$
\item Update any internal statistics to produce $h_{t+1}$
\end{enumerate}
\end{block}

\pause

\begin{block}{Learning Black Box Optimization}
Essentially, same idea as before:
\begin{eqnarray}
h_t, x_t = \text{RNN}_\phi(h_{t-1}, x_{t-1}, y_{t-1}) \nonumber \\
y_t \sim p(y|x_t)\nonumber
\end{eqnarray}

\begin{itemize}
\item Using recurrent neural network (RNN) to predict next $x_t$.
\item $h_t$ is the internal hidden state 
\pause
\item \alert{Remark:} in a black-box setting, we don't have gradient information!
\end{itemize}

\end{block}



\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning Black-box Optimization: Loss Functions\newline \lit{Chen et al'17}}

\begin{itemize}
\item Sum loss: Provides more information than final loss
\end{itemize}
\begin{equation}
\mathcal{L}_{\text{sum}}(\phi) = \mathbb{E}_{f,y_{1:T-1}}\left[\sum_{t=1}^T f(x_t)\right]\nonumber
\end{equation}

\pause

\begin{itemize}
\item EI loss: Try to learn behavior of Bayesian optimizer based on expected improvement (EI)
\begin{itemize}
\item requires model (e.g., GP)
\end{itemize}
\end{itemize}
\begin{equation}
\mathcal{L}_{\text{EI}}(\phi) = - \mathbb{E}_{f,y_{1:T-1}}\left[\sum_{t=1}^T \text{EI}(x_t | y_{1:t-1})\right]\nonumber
\end{equation}

\pause

\begin{itemize}
\item Observed Improvement Loss:
\end{itemize}

\begin{equation}
\mathcal{L}_{\text{OI}}(\phi) = \mathbb{E}_{f,y_{1:T-1}}\left[\sum_{t=1}^T \min \left\{f(x_t) - \min_{i<t}(f(x_i)),0 \right\}  \right]\nonumber
\end{equation}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning Black-box Optimization: Results\newline \lit{Chen et al'17}}

\centering
\includegraphics[width=0.6\textwidth]{images/l2bo_hartmann3}

\begin{itemize}
\item Hartmann3 is an artifical function with 3 dimensions
\pause
\item[$\leadsto$] $\mathcal{L}_{\text{OI}}$ and $\mathcal{L}_{\text{EI}}$ perform best
\item[$\leadsto$] $\mathcal{L}_{\text{OI}}$ easier to compute than $\mathcal{L}_{\text{EI}}$\\ because we need a predictive model to compute EI 
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Optimize via Reinforcement Learning\newline \lit{Li and Malik'17}}

\centering\huge
Learning to Optimize\\
by Li and Malik '17

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Optimize via Reinforcement Learning\newline \lit{Li and Malik'17}}

\centering
\includegraphics[width=0.6\textwidth]{images/l2o_comic}

\tiny
Source: \url{https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Optimize via Reinforcement Learning\newline \lit{Li and Malik'17}}

\begin{block}{Reinforcement Learning for Learning to Optimize}
\begin{description}
\item[State] current location, objective values and gradients evaluated at the current and past locations
\pause
\item[Action] Step update $\Delta x$
\pause
\item[Transition] $x_t \leftarrow x_{t-1} + \Delta x$
\pause
\item[Cost/Reward] Objective value at the current location
\begin{itemize}
\item Since the RL agent will optimize the cumulative cost, this is equivalent to $\mathcal{L}_{\text{sum}}$
\item encourages the policy to reach the minimum of the objective function as quickly as possible
\end{itemize}
\pause
\item[Policy] DNN predicting $\mu_d$ of Gaussian (with constant variance $\sigma^2$)\\ for dimension $d$; sample $\Delta x_d \sim \mathcal{N}(\mu_d, \sigma^2)$
\pause
\item[Training Set] randomly generated objective functions
\end{description}
\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning to Optimize via Reinforcement Learning\newline Results \lit{Li and Malik'17}}

\centering
\includegraphics[width=0.5\textwidth]{images/l2o_dnn}

\begin{itemize}
\item 2-layer DNN with ReLUs
\item Training datasets for training RL agent:\\ four multivariate Gaussians and sampling 25 points from each
\begin{itemize}
\item[$\leadsto$] hard toy problem
\end{itemize}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Learning Acquisition Functions \lit{Volpp et al.'19}}

\centering\huge
Meta-Learning Acquisition Functions for Bayesian Optimization\\
by Volpp et al '19

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Learning Acquisition Functions \lit{Volpp et al.'19}}

\begin{itemize}
\item Instead of learning everything, it might be sufficient to \alert{learn hand-design heuristics}
\pause
\item In Bayesian Optimization (BO), the most critical hand-design heuristic is the acquisition function
\begin{itemize}
\item trade-off between exploitation and exploration
\item Depending on the problem at hand, you might need a different acquisition function
\pause
\item Choices:
\begin{itemize}
\item probability of improvement (PI)
\item expected improvement (EI)
\item upper confidence bounds (UCB)
\item entropy search (ES) -- quite expensive!
\item knowledge gradient (KG)
\item ...
\end{itemize} 
\end{itemize}
\pause
\item \alert{Idea:} Learn a \emph{neural acquisition function} from data
\end{itemize}

$\leadsto$ Replace acquisition function 

\end{frame}
%----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Bayesian Optimization: Algorithm}

\begin{algorithm}[H]
\Input{Search Space $\mathcal{X}$,
black box function $f$, 
\alert{acquisition function $\alpha$,}
maximal number of function evaluations $m$
}
\BlankLine
$\mathcal{D}_0$ $\leftarrow$ initial\_design($\mathcal{X}$); \\
\For{n = $1, 2, \ldots m - |D_0|$}{
$\surro: \conf \mapsto y$ $\leftarrow$ fit predictive model on $\mathcal{D}_{n-1}$;\\
select $x_{n}$ by optimizing $x_{n} \in \argmax_{x \in \mathcal{X}} \alert{\alpha(x; \mathcal{D}_{n-1}, \surro)}$;\\
Query $y_{n} := f(x_{n})$;\\
Add observation to data $D_{n} := D_{n-1} \cup \{\langle x_{n}, y_{n} \rangle \}$;
}
\Return{Best $x$ according to $D_m$ or $\surro$}
\caption{Bayesian Optimization (BO)}
\end{algorithm}


\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{Neural Acquisition Function \lit{Volpp et al.'19}}

Although the \alert{acquisition function $\alpha$} depends on the history $\mathcal{D}_{n-1}$ and the predictive model $\surro$, $\alpha$ mainly makes use of the \alert{predictive mean $\mu$ and variance $\sigma^2$}.

\pause
\bigskip

Neural acquisition function (AF):

\begin{eqnarray}
\alpha_\theta(x) = \alpha_\theta(\mu_t(x), \sigma_t(x)) \nonumber
\end{eqnarray}

where $\theta$ are the parameters of a neural network,\\ and $\mu$ and $sigma$ are its inputs.

\pause 
\begin{itemize}
\item Since the input is not $x$, it allows to learn scalable acquisition function
\item No calibration of hyperparameter necessary, once the neural AF is learnt
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{RL to train Neural AF \lit{Volpp et al.'19}}

\begin{description}
\item[Policy $\pi_\theta$:] Neural acquisition function $\alpha_\theta$
\pause
\item[Episode:] run of $\pi$ on $f\in \mathcal{F}'$
\begin{itemize}
\item $\mathcal{F}$ is a set of functions we can sample functions from
\end{itemize}
\pause
\item[State $s_t$:] $\mu_t$ and $\sigma_t$ on a set of points $\xi_t$
\pause
\item[Action $a_t$:] Sampled point $x_t$
\pause
\item[Reward $r_t$:] negative simple regret: $r_t = f(x^*) - f(\hat{x})$
\begin{itemize}
\item assumes that we can estimate the optimal $x^*$ for \emph{training} functions
\end{itemize}
\pause
\item[Transition probability]: Noisy evaluation of $f$ and the predictive model update
\end{description}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{State \lit{Volpp et al.'19}}

\begin{itemize}
\item The state is described by a discrete set of points $\xi_t = \{\xi_n\}^N_{n=1}$
\pause
\item We feed these points through the predictive model and the neural AF to obtain $\alpha_\theta(\xi_i) = \alpha_\theta(\mu_t(\xi_i), \sigma_t(\xi_i)) $
\pause
\item $\alpha_\theta(\xi_i)$ are interpreted as the logits of multinomial distribution, s.t.
$$\pi_\alpha(\cdot \mid s_t) = \text{Mult}\left[\alpha_\theta(\xi_1), \ldots, \alpha_\theta(\xi_N) \right] $$
\pause
\item Due to curse of dimensionality, we need a two step approach for~$\xi_t$
\begin{enumerate}
\item sample $\xi_{\text{global}}$ using a coarse Sobol grid
\item sample $\xi_{\text{local}}$ using local optimization starting from the best samples in $\xi_{\text{global}}$
\end{enumerate}
\item[$\leadsto$] $\xi_t = \xi_{\text{global}} \cup \xi_{\text{lokal}}$ 
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c,fragile]{Learning Acquisition Functions\newline Results \lit{Volpp et al.'19}}

\centering
\includegraphics[width=1.0\textwidth]{images/l2acq.png}


\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}[c]{Results on Artificial Functions \lit{Volpp et al.'19}}

\includegraphics[width=1.0\textwidth]{images/l2acq_results.png}

\medskip

\begin{itemize}
\item Approach by \lit{Volpp et al. '19} called MetaBO
\item MetaBO performs better than other acquisition functions\\ (EI, GP-UBC, PI) and other baselines (Random, TAF)
\end{itemize}

\pause

\alert{Assumption}: You have a family of functions at hand\\ that resembles your target functions.

\end{frame}
%-----------------------------------------------------------------------


\end{document}
