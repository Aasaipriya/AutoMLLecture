\documentclass{exam}
\usepackage{amsmath, amsfonts}
\usepackage{bm}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[super]{nth}
\usepackage{booktabs}

\DeclareMathOperator*{\argmin}{argmin}

\usepackage[hyperfootnotes=false]{hyperref}

\usepackage[usenames,dvipsnames]{color}
\newcommand{\note}[1]{
	\noindent~\\
	\vspace{0.25cm}
	\fcolorbox{Red}{Orange}{\parbox{0.99\textwidth}{#1\\}}
	%{\parbox{0.99\textwidth}{#1\\}}
	\vspace{0.25cm}
}


\input{../macros.tex}
%\renewcommand{\hide}[1]{#1}

\qformat{\thequestion. \textbf{\thequestiontitle}\hfill[\thepoints]}
\bonusqformat{\thequestion. \textbf{\thequestiontitle}\hfill[\thepoints]}

\pagestyle{headandfoot}

%%%%%% MODIFY FOR EACH SHEET!!!! %%%%%%
\newcommand{\duedate}{07.06.19 (10:00)}
\newcommand{\due}{{\bf This assignment is due on \duedate.} }
\firstpageheader
{Due: \duedate \\ Points: 6}
{{\bf\lecture}\\ \assignment{6}}
{\lectors\\ \semester}

\runningheader
{Due: \duedate}
{\assignment{6}}
{\semester}
%%%%%% MODIFY FOR EACH SHEET!!!! %%%%%%

\firstpagefooter
{}
{\thepage}
{}

\runningfooter
{}
{\thepage}
{}

\headrule
\pointsinrightmargin
\bracketedpoints
\marginpointname{pt.}




\newcommand{\parents}{p}
\newcommand{\negation}[1]{\overline{#1}}
%\newcommand{\tuple}[1]{\langle #1 \rangle}
\newcommand{\tuple}[1]{\left<#1\right>}
\newcommand{\dom}[1]{dom(#1)}              % domain

\newcommand{\false}{false}
\newcommand{\true}{true}
\newcommand{\TRUE}{{\mbox{\scriptsize \em TRUE}}}
\newcommand{\FALSE}{{\mbox{\scriptsize \em FALSE}}}

\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bI}{\bm{I}}
\newcommand{\bw}{\bm{w}}
\newcommand{\ba}{\bm{a}}
\newcommand{\bb}{\bm{b}}
\newcommand{\bk}{\bm{k}}  
\newcommand{\inv}{^{-1}}

\newcommand{\norm}{{\mathcal{N}}}

\newcommand\transpose{^{\textrm{\tiny{\sf{T}}}}}

\begin{document}
	\gccs
	Now that you have learned about the theory behind GPs, you will have to use that theory to implement GPs yourself.
	\begin{questions}
		\titledquestion{Gaussian Processes}[6]
		The exercise is mostly concerned with equations 2.11 and 2.12 in chapter 2 of "Gaussian Processes for Machine Learning\footnote{\url{http://www.gaussianprocess.org/gpml/chapters/RW2.pdf}}"
		\begin{parts}
		\part[2] Equation 2.11 gives the predictive distribution using an explicit feature space formulation as:
		\begin{equation*}
		f_{\star} | \bx_{\star}, \bX, \by \sim \norm(\frac{1}{\sigma_n^2} \phi(\bx_{\star})\transpose A\inv \Phi \by,
		\phi(\bx_{\star})\transpose A\inv \phi(\bx_{\star}))
		\end{equation*}
		where  $\Phi = \phi(\bX) \text{ and } A = \sigma_n^{-2} \Phi \Phi\transpose + \Sigma_p\inv$.\\
		Equation 2.12 is an alternative formulation that requires only an inversion of an $n\times n$ matrix instead of an $N\times N$ one, where $n$ is the number of data points and $N$ the number
		of features. Equation 2.12 is given as:
		\begin{equation*}
		f_{\star} | \bx_{\star}, \bX, \by \sim \norm (\phi_{\star}\transpose \Sigma_p \Phi(\Phi\transpose \Sigma_p \Phi + \sigma_n^2I)^{-1}\by, \\
		\nonumber{}  \phi_{\star}\transpose \Sigma_p\phi_{\star} - \phi_{\star}\transpose \Sigma_p \Phi(\Phi\transpose \Sigma_p \Phi + \sigma_n^2I)\inv
		\Phi\transpose \Sigma_p \phi_{\star})
		\end{equation*}
		Show the equivalence of 2.11 and 2.12.\\
		(\textit{It suffices to show that $\frac{1}{\sigma_n^2} \phi(\bx_{\star})\transpose A\inv \Phi \by = \phi_\star\transpose \Sigma_p\Phi (\Phi\transpose\Sigma_{p}\Phi + \sigma_n^2I )\inv \by$}).
		
		Try to apply the following rules wherever possible and clearly state at each step what you did.
		\begin{table}[h]\centering
			\begin{tabular}{cl}
				Rules & \\
				\toprule
				(1) & $(AB)\inv = B\inv A\inv$ \\
				(2) & $A(BC) = (AB)C$ \\
				(3) & $I = BB\inv = \Sigma_p\Sigma_p\inv = (\Phi \Phi\transpose)(\Phi \Phi\transpose)\inv$ \\
				\bottomrule
			\end{tabular}
		\end{table}
	
		With $I$ the identity matrix. \textit{Please tex your solution.}
		
		\part[2] Your second task is to implement 2.11 to predict the mean and variance given some observations.
		We provide a function prototype that takes the observations, a function $\phi$, and an array of points, $[\vec x_{1}, \vec x_{2}, \dots] $, as arguments. Compute the mean $\mu$ and the variance $\sigma^2$ at all $\vec x$. For matrix inversion you can use \texttt{numpy.linalg.inv}. Use the data provided in your source folder to create a plot that shows the mean and the $2\sigma$ confidence interval around it\footnote{You can use \texttt{matplotlib.pyplot.fill\_between} to generate the confidence interval.} for a feature space of size $N=2$. Use  $\sigma_n = 1$ and $\Sigma_p = I$ and
		\begin{equation*}
		\phi(x) = (1,x)^T
		\label{eq:phi_of_x}
		\end{equation*}
		which corresponds to Bayesian \textit{linear} regression for one dimensional input.
		
		\part[2] Finally, implement a second function based on 2.12. Convince yourself, that both yield the same values, by checking the output for the given input\footnote{By the nature of numerical calculations, the results will not be identical, but the difference will be very small. Use \texttt{numpy.allclose} to test for equality.}. Compare the time it takes for both implementations to compute the output for the given data, and points $\vec x_{i}$, for a growing number of features. Again use $\sigma_n = 1$ and $\Sigma_p = I$ and
		\begin{equation}
		\phi_n(x) = (1,x,x^2,\dots,x^{n-1})^T
		\label{eq:phi_n_of_x}
		\end{equation}
		to plot the computing time for $n=2,4,8,\dots,2048, 4096$. 
	\end{parts}
		
		\titledquestion{Feedback}[Bonus: 0.5]
		For each question in this assignment, state:
		\begin{itemize}
			\item How long you worked on it.
			\item What you learned.
			\item Anything you would improve in this question if you were teaching the course.
		\end{itemize}
	\end{questions}
	
	\noindent
	\due Submit your solution for the tasks by uploading a PDF to your groups BitBucket repository. The PDF has to include the name of the submitter(s).
\end{document}