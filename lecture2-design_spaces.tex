\begin{frame}[c]{}

\centering
\huge
Lecture 2:\\
Design Spaces in Machine Learning
\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Where are we? The big picture}

\begin{itemize}
	\item Introduction
	\item[$\to$] Background
	\begin{itemize}
		\item[$\to$] Design spaces in ML
		\item Experimentation and visualization
	\end{itemize}
	\item Hyperparameter optimization (HPO)
	\begin{itemize}
	  \item Bayesian optimization
	  \item Other black-box techniques
	\end{itemize}
	\item Speeding up HPO with multi-fidelity optimization
	\item Pentecost (Holiday) -- no lecture
	\item Architecture search I + II
	\item Meta-Learning
	\item Learning to learn $\&$ optimize
	\item Beyond AutoML: algorithm configuration and control
	\item Project announcement and closing
\end{itemize}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning Goals}

After this lecture, you will be able to \ldots

\begin{itemize}
  \item identify design decisions of machine learning algorithms
  \item explain different types of design decisions and there relations
  \item create design spaces
  \item discuss the pros and cons of different design space approaches
  \item explain design spaces for neural architecture search
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Simple Design Decisions: Selection of Algorithm}

\includegraphics[width=1.0\textwidth]{images/sklearn-cheat}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Simple Design Decisions: Selection of Algorithm}

\begin{itemize}
  \item categorical design decison: $\{ \algo_1, \algo_2, \algo_3, \ldots, \algo_n\}$
  \begin{itemize}
	\item random forest (RF), support vector machine (SVM),\\ gradient boosting (GB), \ldots
    \item there is no ordering between algorithms
    \item set notation
  \end{itemize}
  \pause
  \item if we would run all of them and each takes (on average) $t$ seconds:\\
  $t \cdot n$ seconds
  \pause
  \smallskip
  \item in addition, choose pre-processing algorithm: $\{\algo_1^P, \algo_2^P, \algo_3^P, \ldots \algo_l^P \}$
  \begin{itemize}
    \item PCA, feature selection, random kitchen sinks, \ldots
  \end{itemize}
  \item if we only use one preprocessor and one ML algorithm,\\ brute force would require:
  $t \cdot n \cdot l$
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
\section{Design Space from Documentation}
%----------------------------------------------------------------------
\begin{frame}[c]{Design Space of Support Vector Machines}

\centering
\includegraphics[width=0.7\textwidth]{images/sklearn_svm_doc.png}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Design Space of Support Vector Machines}

\begin{block}{Hyperparameter Optimization (HPO; informal)}
Given
\begin{itemize}
  \item a dataset
  \item a set of hyperparameters of a machine learning algorithms
  \item a cost metric (e.g., predictive error)
\end{itemize}
we want to find the hyperparameter configuration\\ that minimizes the cost metric wrt the dataset. 
\end{block}

\begin{block}{Hyperparameter Types of SVM}
\begin{description}
  \item[C] float hyperparameter
  \item[Kernel] categorical hyperparameter
  \item[Degree] integer hyperparamerter
  \item[gamma] float hyperparamerter
  \item \ldots
\end{description}
\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Hyperparameter Types}

\begin{description}
	\item[categorical] set of values (not sorted, no distance)
	\begin{itemize}
	  \item \texttt{kernel \{linear, rbf, poly, sigmoid\}}
	\end{itemize}
	\pause
	\item[ordinal] list of ordered values with uniform distance
	\begin{itemize}
	  \item no example in SVM design space
	  \item \texttt{size [small, medium, large]}
	\end{itemize}
	\pause
	\item[integer] bounded range of integers
	\begin{itemize}
	  \item \texttt{degree [1, 5]}
	\end{itemize}
	\pause
	\item[float] bounded range of floats
	\begin{itemize}
	  \item \texttt{gamma\_value [0.0001, 8.0]}
	\end{itemize}
\end{description}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c,fragile]{Design Space and Configuration}

\begin{block}{Design/Configuration Space}
The combination of several hyperparameter ranges $\pcs_i$ for the $i$-th hyperparameter creates a design space:
$\pcs = \pcs_1 \times \pcs_2 \times \pcs_3 \ldots \times \pcs_n$ 

\pause
\bigskip

For example, the design space of a SVM would include:

\begin{verbatim}
kernel categorical {linear, rbf, poly, sigmoid}
degree integer [1, 5]
gamma_value float [0.0001, 8.0]
\end{verbatim}
\end{block}

\pause

\begin{block}{Configuration}
An element of the configuration space $\conf \in \pcs$ instantiates each hyperparameter $\conf_i$ with a value.
For example:
\begin{verbatim}
{kernel: rbf, gamma_value: 1, degree: 2}
\end{verbatim}

\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Concept of Defaults}

\begin{itemize}
  \item We assume that each algorithm has a default
  \begin{itemize}
    \item often a robust configuration if you don't want to change it
    \item defaults often provided by developer, e.g.,\\
   		  in its documentation or the corresponding paper
  \end{itemize}
  \pause
  \item We use the default to start our search for HPO
  \begin{itemize}
    \item if we know a reasonable configuration,\\ we should start with a random configuration?
    \item Goal: find something which is better than the default
    \pause
    \smallskip
    \item Pro: Can help us to start in good region of the design space
    \item Contra: We might start being trapped in local optimum.
  \end{itemize}
\end{itemize}

\pause
Example: the default \texttt{kernel} of the SVM could be the RBF-kernel\\
\texttt{kernel categorical \{linear, rbf, poly, sigmoid\}[rbf]}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Conditional Dependencies}

\begin{block}{SVM Example}
\begin{verbatim}
kernel categorical {linear, rbf, poly, sigmoid}[rbf]
degree integer [1, 5][3]
gamma_value float [0.0001, 8.0][2.0]
\end{verbatim}
\end{block}

\begin{itemize}
  \item Some parameters depend on each other
  \begin{itemize}
    \item \texttt{degree} is only active if \texttt{kernel} is set to \texttt{poly}
    \item \texttt{gamma\_value} is only active if \texttt{kernel} is set to \texttt{rbf}  
  \end{itemize}
  \bigskip
  \pause
  \item[$\leadsto$] model such conditional dependencies in configuration space:
\end{itemize}

\begin{verbatim}
degree | kernel in {poly}
gamma_value | kernel in {rbf}
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Forbidden Constraints}

\begin{itemize}
  \item sometimes combinations of settings are forbidden
  \item For example: $a \leq b$
  \smallskip
  \pause
  \item[$\leadsto$] Try to avoid such constraints\\ because sampling in constrained spaces gets much harder
  \smallskip
  \item Sometimes constraints can be rewritten:
\end{itemize}

\begin{verbatim}
    a float [0,1][0]
    b float [0,1][0]
    a <= b
\end{verbatim}

Rewrite:
\begin{verbatim}
    a float [0,1][0]
    c float [0,1][0]
\end{verbatim}

with $b = a + c$ $\leadsto$ \texttt{b} might be larger than 1! 


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Expert Knowledge: Transformations}

\begin{itemize}
	\item expert knowledge can help to guide hyperparameter optimization
	\item For example, some hyperparameters might not be sampled uniformly
\end{itemize}

\pause
\medskip
For example, regularization hyperparameter of SVM:

\begin{verbatim}
    C float [0.001, 1000.0][1.0]
\end{verbatim}

\begin{itemize}
	\item the distance between $999.9$ and $1000.0$ should not be the same as between $0.001$ and $0.101$
	\smallskip
	\pause
	\item[$\leadsto$] We might want to sample here from from a log-scale
\end{itemize}

\begin{verbatim}
    C float [0.001, 1000.0][1.0] log
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Design Space of Support Vector Machines}

\centering
\includegraphics[width=0.7\textwidth]{images/sklearn_svm_doc.png}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Configuration Space of SVM}

\begin{verbatim}
  Hyperparameters:
    C float [0.001, 1000.0][1.0] log
    coef0 float [0.0, 10.0][0.0]
    degree integer [1, 5][3]
    gamma categorical {auto, value}[auto]
    gamma_value float [0.0001, 8.0][1.0]
    kernel categorical {linear, rbf, poly, sigmoid}[poly]
    shrinking categorical {true, false}[true]
  Conditions:
    coef0 | kernel in {poly, sigmoid}
    degree | kernel in {poly}
    gamma | kernel in {rbf, poly, sigmoid}
    gamma_value | gamma in {value}
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------
\section{Design Space from Algorithm}
%----------------------------------------------------------------------
\begin{frame}[c]{Algorithm: Randomized Regression Tree}

\begin{algorithm}[H]
\Input{$D = \{(\vec{x}^{(i)}, y^{(i)})\}_{i\in \{1\ldots|D|\}}$, attributes A}
\BlankLine
\If{current tree depth larger than threshold $t_d$} {
	  \Return{Leaf with $\vec{y}$}
}
\If{samples size $|D|$ is smaller than threshold $t_n$} {
	  \Return{Leaf with $\vec{y}$}
}
\pause
$A'$ $\leftarrow$ subsample $k$ attributes from $A$;\\
let $v$ be the \emph{best split value} of attribute $a \in A'$ according to criterion $c$;\\ 
\pause
Create edges with constraint $\vec{x}^{(i)}.a \leq v$ and $\vec{x}^{(i)}.a > v$;\\
BuildTree($\{ (\vec{x}^{(i)}, y^{(i)}) \in D | \vec{x}^{(i)}.a \leq v\}$, $A$);\\
BuildTree($\{ (\vec{x}^{(i)}, y^{(i)}) \in D | \vec{x}^{(i)}.a > v\}$, $A$);
	
\Return{current node}
\caption{\texttt{BuildTree()}}
\end{algorithm}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Algorithm: Regression Random Forest}

\begin{algorithm}[H]
\Input{$D = \{(\vec{x}^{(i)}, y^{(i)})\}_{i\in \{1\ldots|D|\}}$, attributes A}
\BlankLine

\For{$i \in \{1 \ldots n\}$}{
	$D'$ $\leftarrow$ bootstrap $D$ with $d$ points;\\
	$T_i$ $\leftarrow$ BuildTree($D'$, $A$);\\
}
	
\caption{\texttt{BuildForest()}}
\end{algorithm}

\pause
\bigskip

\begin{algorithm}[H]
\Input{Data point $x$}
\BlankLine

\For{$i \in \{1 \ldots n\}$}{
        $y_i$ $\leftarrow$ $T_i$.predict($x$); \\
}

$y$ $\leftarrow$ $\frac{1}{n}\sum_{i}^n y_i$;\\
	
\caption{\texttt{Predict()}}
\end{algorithm}


\end{frame}
%----------------------------------------------------------------------
\begin{frame}{Configuration Space of Regression Random Forest}

Task: \hands [5min]
\begin{itemize}
  \item What are design decisions of a regression random forest?
  \item What could be a reasonable configuration space?  
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Levels of Programming by Optimization \litw{Hoos 2012}}

\begin{description}
\item[Level 0] Already exposed hyperparameters
\pause
\item[Level 1] Make hardwired design choices accessible
\pause
\item[Level 2] Design choices are already considered\\ during software development
\pause 
\item[Level 3] Seek design choices in software development
\end{description}

\pause
\bigskip

$\leadsto$ The field of search-based software engineering is\\ closely related to AutoML.

\end{frame}
%----------------------------------------------------------------------
\section{Hyperparameter Optimization and CASH}
%----------------------------------------------------------------------
\begin{frame}[c]{Hyperparameter Optimization}

\begin{block}{Hyperparameter Optimization (HPO)}
Given
\begin{itemize}
  \item a dataset $\dataset$
  \item a set of hyperparameters $\pcs$ of a machine learning algorithms $\algo$
  \item a cost metric or loss function $\loss$ (e.g., predictive error)
\end{itemize}
we want to find the hyperparameter configuration that minimizes $\loss$ wrt the dataset:

\begin{equation}
\conf^* \in \argmin_{\conf \in \pcs} \loss(\algo(\conf), \dataset) \nonumber
\end{equation}

\end{block}

\pause
Remarks: 

\begin{itemize}
  \item We use an simplified notation of the loss function $\loss$ to focus on the main point here;
  \pause
  \item $\argmin$ returns a set of optimal points of a given function. We are happy to find one element of this set and thus use $\in$ instead of $=$.
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Extend HPO}

AutoML includes

\begin{itemize}
  \item Hyperparameter Optimization (HPO)
  \item Algorithm selection 
  \item \ldots (and more)
\end{itemize}

\pause
\bigskip
$\leadsto$ How to select an algorithm?


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{CASH \litw{Thornton et al. 2013}}

\begin{block}{CASH: Combined Algorithm Selection and Hyperparameter Optimization}
Given
\begin{itemize}
  \item a dataset $\dataset$
  \item a set of algorithms $\mathbf{A} = \{\algo_1, \algo_2, \ldots, \algo_k\}$
  \item a set of hyperparameters $\pcs$ of each machine learning algorithms $\algo_i$
  \item a cost metric or loss function $\loss$ (e.g., predictive error)
\end{itemize}
we want to find the best combination of algorithm and its hyperparameter configuration minimizing $\loss$ wrt the dataset:

\begin{equation}
(\algo^*, \conf^*) \in \argmin_{\algo \in \mathbf{A}, \conf \in \pcs} \loss(\algo(\conf), \dataset) \nonumber
\end{equation}

\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Representation of CASH}

\begin{itemize}
  \item top-level hyperparameter to select algorithm
  \item conditional constraints for all algorithm-specific hyperparameters
\end{itemize}

\pause

\begin{verbatim}
algo categorical {SVM, RF, DNN}[RF]

n_tree integer [10,100][10]
n_tree | algo in {RF}

gamma float {0.0001,8}[1]
gamma | algo in {SVM]

...
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Representation of CASH}

\centering
\includegraphics[width=1.0\textwidth]{images/cash}

\hfill Source: \lit{Komer et al. 2019}

\end{frame}
%----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Select several Algorithms}

We can use set of algorithms:
\begin{itemize}
  \item for example, we want to apply a set of preprocessors
  \item Each algorithm gets a binary hyperparameter
  \begin{itemize}
  	\item $2^n$ possible combinations for $n$ preprocessors	
  \end{itemize}
\end{itemize}

\begin{verbatim}
normalize categorical {True, False}[False]
pca categorical {True, False}[False]
pca_dim integer [1,100][10]
pca_dim | pca in {True}
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------
\section{Unbounded Configuration Spaces}
%----------------------------------------------------------------------
\begin{frame}[c]{Bounded Configuration Spaces}

So far, we assumed that each hyperparameter has a pre-defined domain.

\medskip
Problems:
\begin{itemize}
  \item sometimes we don't know a good range
  \begin{itemize}
     \item too large range $\leadsto$ very hard optimization problem
     \item too small range $\leadsto$ we might miss high-performance areas
  \end{itemize}
  \pause
  \medskip
  \item for pipelines, we might don't know the size of the pipeline
  \begin{itemize}
    \item some components could be part of the pipeline multiple times
  \end{itemize}
\end{itemize}

\pause
\bigskip

$\leadsto$ Can we design configuration spaces without explicit bounds?

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{TPOT \litw{Olson et al. 2016}}

\centering
\includegraphics[width=0.8\textwidth]{images/tpot_tree}

\begin{itemize}
  \item TPOP searches in a space of tree-based pipelines
  \item pipeline can potentially grow in size
  \item Challenge: avoid illegal pipelines
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Distributions instead of Bounds (HyperOpt \litw{Bergstra et al. 2013})}

\begin{itemize}
  \item instead of a range, HyperOpt also allows for user-defined distributions
  \begin{itemize}
    \item sampling of gamma (of SVM) could be done according to\\ log-normal distribution (with given statistics) 
  \end{itemize}
  \item Advantage:\\ allows for flexible definition of expert knowledge
  \item Disadvantage:\\ hard to find a good distribution if expert knowledge is limited
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Increasing Bounds \litw{Shahriari et al. 2015}}

\begin{enumerate}
  \item start with peaked distributions \\ s.t. it is very unlikely to sample outside of fairly narrow bounds
  \item increase width of distribution over time\\ to search in larger areas over time
\end{enumerate}

\bigskip
\pause

Remark:
\begin{itemize}
  \item similar ideas can be used for safe optimization
  \begin{itemize}
    \item quite important if a failed configuration incurs great (monetary) costs, \\
          a robot is destroyed because of its configuration \lit{Sui et al. 2015}
  \end{itemize}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
\section{Design Spaces for Neural Networks}
%----------------------------------------------------------------------
\begin{frame}[c]{Design of Neural Networks}

To train a deep neural network, we have many crucial design decisions \hands
\pause

\begin{itemize}
  \item number of layers
  \item number of neurons in each layer
  \item activation functions
  \item skip connections
  \item global architecture (MLP, ResNet, DenseNet, \ldots)
  \item regularization
  \begin{itemize}
    \item batch norm, weight decay, dropout, mixup, cut-out, \ldots 
  \end{itemize}
  \item optimizer hyperparameters
  \begin{itemize}
    \item type of optimizer (SGD, Adam, \ldots)
    \item learning rate
    \item momentum
    \item learning rate schedule
  \end{itemize}
  \item \ldots
\end{itemize}

$\leadsto$ joint global optimization of hyperparameters and architecture!

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Neural Architecture Search (NAS)}

\begin{block}{Neural Architecture Search (NAS)}
Given
\begin{itemize}
  \item a dataset $\dataset$
  \item a set of hyperparameters $\pcs$\\ \textbf{defining the architecture of a deep neural network}
  \item a cost metric or loss function $\loss$ (e.g., predictive error)
\end{itemize}
we want to find the hyperparameter configuration minimizing $\loss$ wrt the dataset:

\begin{equation}
\conf^* \in \argmin_{\conf \in \pcs} \loss(\algo(\conf), \dataset) \nonumber
\end{equation}
\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Global NAS}

\centering
\includegraphics[width=0.7\textwidth]{images/nas_global.png}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Neural Architecture Search (NAS) -- Remarks}

\begin{itemize}
  \item yet another hyperparameter problem?
  \begin{itemize}
    \item[$\to$] we will see in later sessions how we exploit expert knowledge about neural networks to go beyond black-box HPO
  \end{itemize}
  \pause
  \bigskip
  \item Current practice: 
  \begin{itemize}
    \item hyperparameters (e.g., of the optimizer) are tuned manual or independently from the architecture
  \end{itemize}
  \pause
  \item Better practice:
  \begin{itemize}
    \item jointly optimize hyperparameters and architecture design\\ \lit{Zela et al. 2018}
  \end{itemize}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Shapes of Deep Neural Networks \litw{Kotila 2017}}

\begin{itemize}
  \item Many networks designed by humans follow a pattern
  \item Whether this is a good idea is not well studied
  \item Advantage: The number of hyperparameters is smaller
  \begin{itemize}
    \item E.g., instead of tuning the number of neurons in each layer\\ ($\leadsto$ one hyperparameter per layer),\\
          a few parameters to define shape
  \end{itemize} 
\end{itemize}

\medskip
\centering
\includegraphics[width=.6\textwidth]{images/nas_shapes.png}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Cell NAS}

\centering
\includegraphics[width=0.7\textwidth]{images/nas_cellsearch.png}

$\leadsto$ Search for cells and repeat these in the final architecture $n$ times.

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Flow of Tensors through Operators}


\begin{columns}
 \column{0.3\textwidth}

\centering
\includegraphics[width=0.6\textwidth]{images/nas_darts_space_idea.png}

 \column{0.7\textwidth}

\begin{itemize}
	\item each node is a tensor (i.e., a latent representation of the input data)
	\item between nodes operators change the data\\ (e.g., convolution or max pooling)
	\begin{itemize}  
		\item includes no-op operators to deactivate edges
	\end{itemize}
\end{itemize}


\begin{flushright}
	Source: \lit{Liu et al. 2019}
\end{flushright}

\end{columns}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning Goals}

Now, you should be able to \ldots

\begin{itemize}
  \item identify design decisions of machine learning algorithms
  \item explain different types of design decisions and there relations
  \item create design spaces
  \item discuss the pro and cons of different design space approaches
  \item explain design spaces for neural architecture search
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Literature [These are links]}

\begin{itemize}
	\item \lit{\href{https://www.cs.ubc.ca/~hoos/Publ/Hoos10.pdf}{Programming by Optimization. Hoos 2012}}
	\item \lit{\href{https://ml.informatik.uni-freiburg.de/papers/13-KDD2013-AutoWEKA.pdf}{AutoWEKA and CASH. Thornton et al. 2013}}
	\item \lit{\href{https://www.automl.org/wp-content/uploads/2018/12/tpot.pdf}{TPOT. Olson and Moore. 2019}}
	\item \lit{\href{https://www.automl.org/wp-content/uploads/2018/12/hyperopt-sklearn-1.pdf}{Hyperopt-Sklearn. Komer et al. 2019}}
	\item \lit{\href{http://proceedings.mlr.press/v51/shahriari16.html}{Unbounded Bayesian Optimization via Regularization. Shahriari et al. 2016}}
	\item \lit{\href{https://arxiv.org/abs/1806.09055}{DARTS: Differentiable Architecture Search. Liu et al. 2019}}			
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
