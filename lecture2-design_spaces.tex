\begin{frame}[c]{}

\centering
\huge
Lecture 2:\\
Design Spaces in Machine Learning
\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Where are we? The big picture}

\begin{itemize}
	\item Introduction
	\item[$\to$] Background
	\begin{itemize}
		\item[$\to$] Design spaces in ML
		\item Experimentation and visualization
	\end{itemize}
	\item Hyperparameter optimization (HPO)
	\begin{itemize}
	  \item Bayesian optimization
	  \item Other black-box techniques
	\end{itemize}
	\item Speeding up HPO with multi-fidelity optimization
	\item Pentecost (Holiday) -- no lecture
	\item Architecture search I + II
	\item Meta-Learning
	\item Learning to learn $\&$ optimize
	\item Beyond AutoML: algorithm configuration and control
	\item Project announcement and closing
\end{itemize}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Learning Goals}

After this lecture, you will be able to \ldots

\begin{itemize}
  \item identifiy design decisions of machine learning algorithms
  \item explain different types of design decisions and there relations
  \item create design spaces
  \item discuss the pro and cons of different design space approaches
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Simples Design Decisions: Selection of Algorithm}

\includegraphics[width=1.0\textwidth]{images/sklearn-cheat}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Simples Design Decisions: Selection of Algorithm}

\begin{itemize}
  \item categorical design decison: $\{ \algo_1, \algo_2, \algo_3, \ldots, \algo_n\}$
  \begin{itemize}
	\item random forest (RF), support vector machine (SVM),\\ gradient boosting (GB), \ldots
    \item there is no ordering between algorithms
    \item set notation
  \end{itemize}
  \pause
  \item if we would run all of them and each takes (on average) $t$ seconds:\\
  $t \cdot n$ seconds
  \pause
  \smallskip
  \item in addition, choose pre-processing algorithm: $\{\algo_1^P, \algo_2^P, \algo_3^P, \ldots \algo_l^P \}$
  \begin{itemize}
    \item PCA, feature selection, random kitchen sinks, \ldots
  \end{itemize}
  \item if we only use one preprocessor and one ML algorithm, brute force would need:\\
  $t \cdot n \cdot l$
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
\section{Design Space from Documentation}
%----------------------------------------------------------------------
\begin{frame}[c]{Design Space of Support Vector Machines}

\centering
\includegraphics[width=0.7\textwidth]{images/sklearn_svm_doc.png}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Design Space of Support Vector Machines}

\begin{block}{Hyperparameter Optimization (HPO; informal)}
Given
\begin{itemize}
  \item a dataset
  \item a set of hyperparameters of a machine learning algorithms
  \item a cost metric (e.g., predictive error)
\end{itemize}
we want to find the hyperparameter configuration minimizing the cost metric wrt the dataset. 
\end{block}

\begin{block}{Hyperparameter Types of SVM}
\begin{description}
  \item[C] float hyperparameter
  \item[Kernel] categorical hyperparameter
  \item[Degree] integer hyperparamerter
  \item[gamma] float hyperparamerter
  \item \ldots
\end{description}
\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Hyperparameter Types}

\begin{description}
	\item[categorical] set of values (not sorted, no distance)
	\begin{itemize}
	  \item \texttt{kernel \{linear, rbf, poly, sigmoid\}}
	\end{itemize}
	\pause
	\item[ordinal] list of values with uniform distance
	\begin{itemize}
	  \item no example in SVM design space
	  \item \texttt{size [small, medium, large]}
	\end{itemize}
	\pause
	\item[integer] bounded range of integers
	\begin{itemize}
	  \item \texttt{degree [1, 5]}
	\end{itemize}
	\pause
	\item[float] bounded range of floats
	\begin{itemize}
	  \item \texttt{gamma\_value [0.0001, 8.0]}
	\end{itemize}
\end{description}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c,fragile]{Design Space and Configuration}

\begin{block}{Design/Configuration Space}
The combination of several hyperparameter ranges $\pcs_i$ for the $it$-th hyperparameter creates a design space:
$\pcs = \pcs_1 \times \pcs_2 \times \pcs_3 \ldots \times \pcs_n$ 

\pause
\bigskip

For example the design space of a SVM, would include:

\begin{verbatim}
kernel categorical {linear, rbf, poly, sigmoid}
degree integer [1, 5]
gamma_value float [0.0001, 8.0]
\end{verbatim}
\end{block}

\pause

\begin{block}{Configuration}
An element of the configuration space $\conf \in \pcs$ instantiates each hyperparameter $\conf_i$ with a value.
For example:
\begin{verbatim}
{kernel: rbf, gamma_value: 1, degree: 2}
\end{verbatim}

\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Concept of Defaults}

\begin{itemize}
  \item We assume that each algorithm has a default
  \begin{itemize}
    \item often a robust configuration if you don't want to change it
    \item default often provided by developer, e.g.,\\
   		  in its documentation or the paper
  \end{itemize}
  \pause
  \item We use the default to start our search for hyperparameter optimization
  \begin{itemize}
    \item if we know a reasonable configuration,\\ we should start with a random configuration?
    \item Goal: find something which is better than the default
    \pause
    \smallskip
    \item Pro: Can help us to start in good region of the design space
    \item Contra: We might start being trapped in local optimum.
  \end{itemize}
\end{itemize}

\pause
Example: the default \texttt{kernel} is the RBF-kernel\\
\texttt{kernel categorical \{linear, rbf, poly, sigmoid\}[rbf]}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Conditional Dependencies}

\begin{block}{SVM Example}
\begin{verbatim}
kernel categorical {linear, rbf, poly, sigmoid}[rbf]
degree integer [1, 5][3]
gamma_value float [0.0001, 8.0][2.0]
\end{verbatim}
\end{block}

\pause

\begin{itemize}
  \item Some parameters depend on each other
  \begin{itemize}
    \item \texttt{degree} is only active if \texttt{kernel} is set to \texttt{poly}
    \item \texttt{gamma\_value} is only active if \texttt{kernel} is set to \texttt{rbf}  
  \end{itemize}
  \bigskip
  \pause
  \item[$\leadsto$] model such conditional dependencies in configuration space:
\end{itemize}

\begin{verbatim}
degree | kernel in {poly}
gamma_value | kernel in {rbf}
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Forbidden Constraints}

\begin{itemize}
  \item sometimes, combinations of settings are forbidden
  \item For example: $a \leq b$
  \smallskip
  \pause
  \item[$\leadsto$] Try to avoid such constraints\\ because sampling in constrainted spaces gets much harder
  \smallskip
  \item Sometimes constraints can be rewritten:
\end{itemize}

\begin{verbatim}
a float [0,1][0]
b float [0,1][0]
a < b
\end{verbatim}

Rewrite:
\begin{verbatim}
a float [0,1][0]
c float [0,1][0]
\end{verbatim}

with $b = a + c$ $\leadsto$ \texttt{b} might be larger than 1! 


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Design Space of Support Vector Machines}

\centering
\includegraphics[width=0.7\textwidth]{images/sklearn_svm_doc.png}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Configuration Space of SVM}

\begin{verbatim}
  Hyperparameters:
    C float [0.001, 1000.0][1.0]
    coef0 float [0.0, 10.0][0.0]
    degree integer [1, 5][3]
    gamma categorical {auto, value}[auto]
    gamma_value float [0.0001, 8.0][1.0]
    kernel categorical {linear, rbf, poly, sigmoid}[poly]
    shrinking categorical {true, false}[true]
  Conditions:
    coef0 | kernel in {poly, sigmoid}
    degree | kernel in {poly}
    gamma | kernel in {rbf, poly, sigmoid}
    gamma_value | gamma in {value}
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------
\section{Design Space from Algorithm}
%----------------------------------------------------------------------
\begin{frame}[c]{Algorithm: Randomized Regression Tree}

\begin{algorithm}[H]
\Input{$D = \{(\vec{x}^{(i)}, y^{(i)})\}_{i\in \{1\ldots|D|\}}$, attributes A}
\BlankLine
\If{current tree depth larger than threshold $t_d$} {
	  \Return{Leaf with average label $y$}
}
\If{samples size $|D|$ is smaller than threshold $t_n$} {
	  \Return{Leaf with average label $y$}
}
\pause
$A'$ $\leftarrow$ subsample $k$ attributes from $A$;\\
let $s$ be the \emph{best split value} of attribute $a \in A'$ according to criterion $c$;\\ 
\pause
Create edges with constraint $s \leq x.a$ and $s > x.a$;\\
BuildTree($\{ (\vec{x}^{(i)}, y^{(i)}) \in D | \vec{x}^{(i)}.a \leq v\}$, $A$);\\
BuildTree($\{ (\vec{x}^{(i)}, y^{(i)}) \in D | \vec{x}^{(i)}.a > v\}$, $A$);
	
\Return{current node}
\caption{\texttt{BuildTree()}}
\end{algorithm}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Algorithm: Regression Random Forest}

\begin{algorithm}[H]
\Input{$D = \{(\vec{x}^{(i)}, y^{(i)})\}_{i\in \{1\ldots|D|\}}$, attributes A}
\BlankLine

\For{$i \in \{1 \ldots n\}$}{
	$D'$ $\leftarrow$ bootstrap $D$ with $d$ points;\\
	$T_i$ $\leftarrow$ BuildTree($D'$, $A$);\\
}
	
\caption{\texttt{BuildForest()}}
\end{algorithm}

\pause
\bigskip

\begin{algorithm}[H]
\Input{Data point $x$}
\BlankLine

\For{$i \in \{1 \ldots n\}$}{
        $y_i$ $\leftarrow$ $T_i$.predict($x$); \\
}

$y$ $\leftarrow$ $\frac{1}{n}\sum_{i}^n y_i$;\\
	
\caption{\texttt{Predict()}}
\end{algorithm}


\end{frame}
%----------------------------------------------------------------------
\begin{frame}{Configuration Space of Regression Random Forest}

Task: \hands
\begin{itemize}
  \item What are design decisions of a regression random forest?
  \item What could be a reasonable configuration space?  
\end{itemize}



\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Levels of Programming by Optimization \refw{Hoos 2012}}

\begin{description}
\item[Level 0] Already exposed hyperparameters
\pause
\item[Level 1] Make hardwired design choices accessible
\pause
\item[Level 2] Design choices are already considered during software development
\pause 
\item[Level 3] Seek design choices in software development
\end{description}

\pause

$\leadsto$ The field of search-based software engineering is closely related to AutoML.

\end{frame}
%----------------------------------------------------------------------
\section{Hyperparameter Optimization and CASH}
%----------------------------------------------------------------------
\begin{frame}[c]{Hyperparameter Optimization}

\begin{block}{Hyperparameter Optimization (HPO)}
Given
\begin{itemize}
  \item a dataset $\dataset$
  \item a set of hyperparameters $\pcs$ of a machine learning algorithms $\algo$
  \item a cost metric or loss function $\loss$ (e.g., predictive error)
\end{itemize}
we want to find the hyperparameter configuration minimizing the cost metric wrt the dataset:

\begin{equation}
\conf^* \in \argmin_{\conf \in \pcs} \loss(\algo(\conf), \dataset) \nonumber
\end{equation}

\end{block}

\pause
Remarks: 

\begin{itemize}
  \item We use an simplified notation of the loss function $\loss$ to focus on the main point here;
  \pause
  \item $\argmin$ returns a set of optimal points of a given function. We are happy to find one element of this set and thus use $\in$ instead of $=$.
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Extend HPO}

AutoML includes

\begin{itemize}
  \item Hyperparameter Optimization (HPO)
  \item Algorithm selection 
  \item \ldots (and more)
\end{itemize}

\pause
$\leadsto$ How to select an algorithm?


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{CASH: Combined Algorithm Selection and Hyperparameter Optimization}

\begin{block}{Hyperparameter Optimization (HPO)}
Given
\begin{itemize}
  \item a dataset $\dataset$
  \item a set of algorithms $A = \{\algo_1, \algo_2, \ldots, \algo_k\}$
  \item a set of hyperparameters $\pcs$ of each machine learning algorithms $\algo_i$
  \item a cost metric or loss function $\loss$ (e.g., predictive error)
\end{itemize}
we want to find the best combination of algorithm and its hyperparameter configuration minimizing the cost metric wrt the dataset:

\begin{equation}
(\algo^*, \conf^*) \in \argmin_{\algo \in A, \conf \in \pcs} \loss(\algo(\conf), \dataset) \nonumber
\end{equation}

\end{block}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Representation of CASH}

\begin{itemize}
  \item top-level hyperparameter to select algorithm
  \item conditional constraints for all algorihm-specific hyperparameters
\end{itemize}

\pause

\begin{verbatim}
algo categorical {SVM, RF, DNN}[RF]

n_tree integer [10,100][10]
n_tree | algo in {RF}

gamma float {0.0001,8}[1]
gamma | algo in {SVM]

...
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Representation of CASH}

\centering
\includegraphics[width=1.0\textwidth]{images/cash}

\hfill Source: Komer et al. 2019

\end{frame}
%----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{frame}[c, fragile]{Select several Algorithms}

We can use set of algorithms:
\begin{itemize}
  \item for example, we want to apply a set of preprocessors
  \item Each algorithm gets a binary hyperparameter
\end{itemize}

\begin{verbatim}
normalize categorical {True, False}[False]
pca categorical {True, False}[False]
pca_dim integer [1,100][10]
pca_dim | pca in {True}
\end{verbatim}

\end{frame}
%----------------------------------------------------------------------
\section{Unbounded Configuration Spaces}
%----------------------------------------------------------------------
\begin{frame}[c]{Bounded Configuration Spaces}

So far, we assumed that each hyperparameter has a pre-defined domain.

Problems:
\begin{itemize}
  \item sometime we don't know a good range
  \begin{itemize}
     \item too large range $\leadsto$ very hard optimization problem
     \item too small range $\leadsto$ we might miss the true optimum
  \end{itemize}
  \pause
  \medskip
  \item for pipelines, we might don't know the size of the pipeline
  \begin{itemize}
    \item some compoenents could be part of the pipeline multiple times
  \end{itemize}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{TPOT {Olson et al. 2016}}

\centering
\includegraphics[width=0.8\textwidth]{images/tpot_tree}

\begin{itemize}
  \item TPOP searches in a space of tree-based pipelines
  \item pipeline can potentially grow in size
  \item challenge: avoid illegal pipelines
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{HyperOpt}

\begin{itemize}
  \item instead of a bound range, HyperOpt also allows for (unbounded) distributions
  \begin{itemize}
    \item sampling of gamma (of SVM) could be done according to log-normal distribution (with given statistics) 
  \end{itemize}
  \item Advantage: allows for flexible definition of expert knowledge
  \item Disadvantage: hard to find a good distribution if expert knowledge is limited
\end{itemize}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Increasing Bounds {???}}


\end{frame}
%----------------------------------------------------------------------
\section{Design Spaces for Neural Networks}
%----------------------------------------------------------------------
\begin{frame}[c]{Slide}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Slide}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Slide}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Slide}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Slide}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Slide}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Slide}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Slide}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Slide}


\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Slide}


\end{frame}
%----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{frame}[c]{Learning Goals}

Now, you should be able to \ldots

\begin{itemize}
  \item identifiy design decisions of machine learning algorithms
  \item explain different types of design decisions and there relations
  \item create design spaces
  \item discuss the pro and cons of different design space approaches
\end{itemize}

\end{frame}
%----------------------------------------------------------------------

